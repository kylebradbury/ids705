% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother

\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Assignment 3},
  pdfauthor={Kyle Bradbury},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Assignment 3}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Supervised Learning - model training and evaluation}
\author{Kyle Bradbury}
\date{2025-02-05}

\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{1}
\tableofcontents
}

\subsection{Instructions}\label{instructions}

\emph{Instructions for all assignments can be found
\href{https://kylebradbury.github.io/ids705/notebooks/assignment_instructions.html}{here}.
Note: this assignment falls under collaboration Mode 2: Individual
Assignment -- Collaboration Permitted. Please refer to the syllabus for
additional information. Please be sure to list the names of any students
that you worked with on this assignment. Total points in the assignment
add up to 90; an additional 10 points are allocated to professionalism
and presentation quality.}

\subsection{Learning Objectives}\label{learning-objectives}

This assignment will provide structured practice to help enable you
to\ldots{}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Understand the primary workflow in machine learning: (1) identifying a
  hypothesis function set of models, (2) determining a
  loss/cost/error/objective function to minimize, and (3) minimizing
  that function through gradient descent
\item
  Understand the inner workings of logistic regression and how linear
  models for classification can be developed.
\item
  Gain practice in implementing machine learning algorithms from the
  most basic building blocks to understand the math and programming
  behind them to achieve practical proficiency with the techniques
\item
  Implement batch gradient descent and become familiar with how that
  technique is used and its dependence on the choice of learning rate
\item
  Evaluate supervised learning algorithm performance through ROC curves
  and using cross validation
\item
  Apply regularization to linear models to improve model generalization
  performance
\end{enumerate}

\subsection{Exercise 1 - Classification using logistic regression: build
it from the ground
up}\label{exercise-1---classification-using-logistic-regression-build-it-from-the-ground-up}

\textbf{{[}60 points{]}}

This exercise will walk you through the full life-cycle of a supervised
machine learning classification problem. Classification problem consists
of two features/predictors (e.g.~petal width and petal length) and your
goal is to predict one of two possible classes (class 0 or class 1). You
will build, train, and evaluate the performance of a logistic regression
classifier on the data provided. Before you begin any modeling, you'll
load and explore your data in Part I to familiarize yourself with it -
and check for any missing or erroneous data. Then, in Part II, we will
review an appropriate hypothesis set of functions to fit to the data: in
this case, logistic regression. In Part III, we will derive an
appropriate cost function for the data (spoiler alert: it's
cross-entropy) as well as the gradient descent update equation that will
allow you to optimize that cost function to identify the parameters that
minimize the cost for the training data. In Part IV, all the pieces come
together and you will implement your logistic regression model class
including methods for fitting the data using gradient descent. Using
that model you'll test it out and plot learning curves to verify the
model learns as you train it and to identify and appropriate learning
rate hyperparameter. Lastly, in Part V you will apply the model you
designed, implemented, and verified to your actual data and evaluate and
visualize its generalization performance as compared to a KNN algorithm.
\textbf{When complete, you will have accomplished learning objectives
1-5 above!}

\subsubsection{A. Load, prepare, and plot your
data}\label{a.-load-prepare-and-plot-your-data}

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, coltitle=black, rightrule=.15mm, opacityback=0, bottomtitle=1mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, titlerule=0mm, colframe=quarto-callout-note-color-frame, colback=white, left=2mm, opacitybacktitle=0.6, arc=.35mm, toprule=.15mm, toptitle=1mm, leftrule=.75mm]

\href{https://github.com/kylebradbury/ids705/tree/main/notebooks/data/a3}{Data
for this exercise can be downloaded here}

\end{tcolorbox}

You are given some data for which you are tasked with constructing a
classifier. The first step when facing any machine learning project:
look at your data!

\textbf{1.1} Load the data.

\begin{itemize}
\tightlist
\item
  In the data folder in the same directory of this notebook, you'll find
  the data in \texttt{A3\_Q1\_data.csv}. This file contains the binary
  class labels, \(y\), and the features \(x_1\) and \(x_2\).
\item
  Divide your data into a training and testing set where the test set
  accounts for 30 percent of the data and the training set the remaining
  70 percent.\\
\item
  Plot the training data by class.
\item
  Comment on the data: do the data appear separable? May logistic
  regression be a good choice for these data? Why or why not?
\end{itemize}

\textbf{1.2} Do the data require any preprocessing due to missing
values, scale differences (e.g.~different ranges of values), etc.? If
so, how did you handle these issues?

Next, we walk through our key steps for model fitting: choose a
hypothesis set of models to train (in this case, logistic regression);
identify a cost function to measure the model fit to our training data;
optimize model parameters to minimize cost (in this case using gradient
descent). Once we've completed model fitting, we will evaluate the
performance of our model and compare performance to another approach (a
KNN classifier).

\subsubsection{B. Stating the hypothesis set of models to evaluate
(we'll use logistic
regression)}\label{b.-stating-the-hypothesis-set-of-models-to-evaluate-well-use-logistic-regression}

Given that our data consists of two features, our logistic regression
problem will be applied to a two-dimensional feature space. Recall that
our logistic regression model is:

\[f(\mathbf{x}_i,\mathbf{w})=\sigma(\mathbf{w}^{\top} \mathbf{x}_i)\]

where the sigmoid function is defined as
\(\sigma(x) = \dfrac{e^x}{1+e^{x}}= \dfrac{1}{1+e^{-x}}\). Also, since
this is a two-dimensional problem, we define
\(\mathbf{w}^{\top} \mathbf{x}_i = w_0 x_{i,0} + w_1 x_{i,1} + w_2 x_{i,2}\)
and here, \(\mathbf{x}_i=[x_{i,0}, x_{i,1}, x_{i,2}]^{\top}\), and
\(x_{i,0} \triangleq 1\)

Remember from class that we interpret our logistic regression classifier
output (or confidence score) as the conditional probability that the
target variable for a given sample \(y_i\) is from class ``1'', given
the observed features, \(\mathbf{x}_i\). For one sample,
\((y_i, \mathbf{x}_i)\), this is given as:

\[P(Y=1|X=\mathbf{x}_i) = f(\mathbf{x}_i,\mathbf{w})=\sigma(\mathbf{w}^{\top} \mathbf{x}_i)\]

In the context of maximizing the likelihood of our parameters given the
data, we define this to be the likelihood function
\(L(\mathbf{w}|y_i,\mathbf{x}_i)\), corresponding to one sample
observation from the training dataset.

\emph{Aside: the careful reader will recognize this expression looks
different from when we talk about the likelihood of our data given the
true class label, typically expressed as \(P(x|y)\), or the posterior
probability of a class label given our data, typically expressed as
\(P(y|x)\). In the context of training a logistic regression model, the
likelihood we are interested in is the likelihood function of our
logistic regression \textbf{parameters}, \(\mathbf{w}\). It's our goal
to use this to choose the parameters to maximize the likelihood
function.}

\textbf{No output is required for this section - just read and use this
information in the later sections.}

\subsubsection{\texorpdfstring{C. Find the cost function that we can use
to choose the model parameters, \(\mathbf{w}\), that best fit the
training
data.}{C. Find the cost function that we can use to choose the model parameters, \textbackslash mathbf\{w\}, that best fit the training data.}}\label{c.-find-the-cost-function-that-we-can-use-to-choose-the-model-parameters-mathbfw-that-best-fit-the-training-data.}

\textbf{1.3} What is the likelihood function that corresponds to all the
\(N\) samples in our training dataset that we will wish to maximize?
Unlike the likelihood function written above which gives the likelihood
function for a \emph{single training data pair} \((y_i, \mathbf{x}_i)\),
this question asks for the likelihood function for the \emph{entire
training dataset}
\(\{(y_1, \mathbf{x}_1), (y_2, \mathbf{x}_2), ..., (y_N, \mathbf{x}_N)\}\).

\textbf{1.4} Since a logarithm is a monotonic function, maximizing the
\(f(x)\) is equivalent to maximizing \(\ln [f(x)]\). Express the
likelihood from the last question as a cost function of the model
parameters, \(C(\mathbf{w})\); that is the negative of the logarithm of
the likelihood. Express this cost as an average cost per sample
(i.e.~divide your final value by \(N\)), and use this quantity going
forward as the cost function to optimize.

\textbf{1.5} Calculate the gradient of the cost function with respect to
the model parameters \(\nabla_{\mathbf{w}}C(\mathbf{w})\). Express this
in terms of the partial derivatives of the cost function with respect to
each of the parameters,
e.g.~\(\nabla_{\mathbf{w}}C(\mathbf{w}) = \left[\dfrac{\partial C}{\partial w_0}, \dfrac{\partial C}{\partial w_1}, \dfrac{\partial C}{\partial w_2}\right]\).

To simplify notation, please use \(\mathbf{w}^{\top}\mathbf{x}\) instead
of writing out \(w_0 x_{i,0} + w_1 x_{i,1} + w_2 x_{i,2}\) when it
appears each time (where \(x_{i,0} = 1\) for all \(i\)). You are also
welcome to use \(\sigma()\) to represent the sigmoid function. Lastly,
this will be a function the features, \(x_{i,j}\) (with the first index
in the subscript representing the observation and the second the
feature; targets, \(y_i\); and the logistic regression model parameters,
\(w_j\).

\textbf{1.6} Write out the gradient descent update equation. This should
clearly express how to update each weight from one step in gradient
descent \(w_j^{(k)}\) to the next \(w_j^{(k+1)}\). There should be one
equation for each model logistic regression model parameter (or you can
represent it in vectorized form). Assume that \(\eta\) represents the
learning rate.

\subsubsection{D. Implement gradient descent and your logistic
regression
algorithm}\label{d.-implement-gradient-descent-and-your-logistic-regression-algorithm}

\textbf{1.7} Implement your logistic regression model.

\begin{itemize}
\item
  You are provided with a template, below, for a class with key methods
  to help with your model development. It is modeled on the Scikit-Learn
  convention. For this, you only need to create a version of logistic
  regression for the case of two feature variables (i.e.~two
  predictors).
\item
  Create a method called \texttt{sigmoid} that calculates the sigmoid
  function
\item
  Create a method called \texttt{cost} that computes the cost function
  \(C(\mathbf{w})\) for a given dataset and corresponding class labels.
  This should be the \textbf{average cost} (make sure your total cost is
  divided by your number of samples in the dataset).
\item
  Create a method called \texttt{gradient\_descent} to run \textbf{one
  step} of gradient descent on your training data. We'll refer to this
  as ``batch'' gradient descent since it takes into account the gradient
  based on all our data at each iteration of the algorithm.
\item
  Create a method called \texttt{fit} that fits the model to the data
  (i.e.~sets the model parameters to minimize cost) using your
  \texttt{gradient\_descent} method. In doing this we'll need to make
  some assumptions about the following:

  \begin{itemize}
  \tightlist
  \item
    \emph{Weight initialization}. What should you initialize the model
    parameters to? For this, randomly initialize the weights to a
    different values between 0 and 1.
  \item
    \emph{Learning rate}. How slow/fast should the algorithm step
    towards the minimum? This you will vary in a later part of this
    problem.
  \item
    \emph{Stopping criteria}. When should the algorithm be finished
    searching for the optimum? There are two stopping criteria: small
    changes in the gradient descent step size and a maximum number of
    iterations. The first is whether there was a sufficiently small
    change in the gradient; this is evaluated as whether the magnitude
    of the step that the gradient descent algorithm takes changes by
    less than \(10^{-6}\) between iterations. Since we have a weight
    vector, we can compute the change in the weight by evaluating the
    \(L_2\) norm (Euclidean norm) of the change in the vector between
    iterations. From our gradient descent update equation we know that
    mathematically this is
    \(||-\eta\nabla_{\mathbf{w}}C(\mathbf{w})||\). The second criterion
    is met if a maximum number of iterations has been reach (5,000 in
    this case, to prevent infinite loops from poor choices of learning
    rates).
  \item
    Design your approach so that at each step in the gradient descent
    algorithm you evaluate the cost function for both the training and
    the test data for each new value for the model weights. You should
    be able to plot cost vs gradient descent iteration for both the
    training and the test data. This will allow you to plot ``learning
    curves'' that can be informative for how the model training process
    is proceeding.
  \end{itemize}
\item
  Create a method called \texttt{predict\_proba} that predicts
  confidence scores (that can be thresholded into the predictions of the
  \texttt{predict} method.
\item
  Create a method called \texttt{predict} that makes predictions based
  on the trained model, selecting the most probable class, given the
  data, as the prediction, that is class that yields the larger
  \(P(y|\mathbf{x})\).
\item
  (Optional, but recommended) Create a method called
  \texttt{learning\_curve} that produces the cost function values that
  correspond to each step from a previously run gradient descent
  operation.
\item
  (Optional, but recommended) Create a method called \texttt{prepare\_x}
  which appends a column of ones as the first feature of the dataset
  \(\mathbf{X}\) to account for the bias term (\(x_{i,1}=1\)).
\end{itemize}

This structure is strongly encouraged; however, you're welcome to adjust
this to your needs (adding helper methods, modifying parameters, etc.).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Logistic regression class}
\KeywordTok{class}\NormalTok{ Logistic\_regression:}
    \CommentTok{\# Class constructor}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{):}
        \VariableTok{self}\NormalTok{.w }\OperatorTok{=} \VariableTok{None}     \CommentTok{\# logistic regression weights}
        \VariableTok{self}\NormalTok{.saved\_w }\OperatorTok{=}\NormalTok{ [] }\CommentTok{\# Since this is a small problem, we can save the weights}
                          \CommentTok{\#  at each iteration of gradient descent to build our }
                          \CommentTok{\#  learning curves}
        \CommentTok{\# returns nothing}
        \ControlFlowTok{pass}
    
    \CommentTok{\# Method for calculating the sigmoid function of w\^{}T X for an input set of weights}
    \KeywordTok{def}\NormalTok{ sigmoid(}\VariableTok{self}\NormalTok{, X, w):}
        \CommentTok{\# returns the value of the sigmoid}
        \ControlFlowTok{pass}
    
    \CommentTok{\# Cost function for an input set of weights}
    \KeywordTok{def}\NormalTok{ cost(}\VariableTok{self}\NormalTok{, X, y, w):}
        \CommentTok{\# returns the average cross entropy cost}
        \ControlFlowTok{pass}
    
    \CommentTok{\# Update the weights in an iteration of gradient descent}
    \KeywordTok{def}\NormalTok{ gradient\_descent(}\VariableTok{self}\NormalTok{, X, y, lr):}
        \CommentTok{\# returns a scalar of the magnitude of the Euclidean norm }
        \CommentTok{\#  of the change in the weights during one gradient descent step}
        \ControlFlowTok{pass}
    
    \CommentTok{\# Fit the logistic regression model to the data through gradient descent}
    \KeywordTok{def}\NormalTok{ fit(}\VariableTok{self}\NormalTok{, X, y, w\_init, lr, delta\_thresh}\OperatorTok{=}\FloatTok{1e{-}6}\NormalTok{, max\_iter}\OperatorTok{=}\DecValTok{5000}\NormalTok{, verbose}\OperatorTok{=}\VariableTok{False}\NormalTok{):}
        \CommentTok{\# Note the verbose flag enables you to print out the weights at each iteration }
        \CommentTok{\#  (optional {-} but may help with one of the questions)}
        
        \CommentTok{\# returns nothing}
        \ControlFlowTok{pass}
    
    \CommentTok{\# Use the trained model to predict the confidence scores (prob of positive class in this case)}
    \KeywordTok{def}\NormalTok{ predict\_proba(}\VariableTok{self}\NormalTok{, X):}
        \CommentTok{\# returns the confidence score for the each sample}
        \ControlFlowTok{pass}
    
    \CommentTok{\# Use the trained model to make binary predictions}
    \KeywordTok{def}\NormalTok{ predict(}\VariableTok{self}\NormalTok{, X, thresh}\OperatorTok{=}\FloatTok{0.5}\NormalTok{):}
        \CommentTok{\# returns a binary prediction for each sample}
        \ControlFlowTok{pass}
    
    \CommentTok{\# Stores the learning curves from saved weights from gradient descent}
    \KeywordTok{def}\NormalTok{ learning\_curve(}\VariableTok{self}\NormalTok{, X, y):}
        \CommentTok{\# returns the value of the cost function from each step in gradient descent}
        \CommentTok{\#  from the last model fitting process}
        \ControlFlowTok{pass}
    
    \CommentTok{\# Appends a column of ones as the first feature to account for the bias term}
    \KeywordTok{def}\NormalTok{ prepare\_x(}\VariableTok{self}\NormalTok{, X):}
        \CommentTok{\# returns the X with a new feature of all ones (a column that is the new column 0)}
        \ControlFlowTok{pass}
\end{Highlighting}
\end{Shaded}

\textbf{1.8} Choose a learning rate and fit your model. Learning curves
are a plot of metrics of model performance evaluated through the process
of model training to provide insights about how model training is
proceeding. Show the learning curves for the gradient descent process
for learning rates of \(\{10^{-0}, 10^{-2}, 10^{-4}\}\). For each
learning rate plot the learning curves by plotting \textbf{both the
training and test data average cost} as a function of each iteration of
gradient descent. You should run the model fitting process until it
completes (up to 5,000 iterations of gradient descent). All of the 6
resulting curves (train and test average cost for each learning rate)
should be plotted on the \textbf{same set of axes} to enable direct
comparison. \emph{Note: make sure you're using average cost per sample,
not the total cost}.

\begin{itemize}
\tightlist
\item
  Try running this process for a really big learning rate for this
  problem: \(10^2\). Look at the weights that the fitting process
  generates over the first 50 iterations and how they change. Either
  print these first 50 iterations as console output or plot them. What
  happens? How does the output compare to that corresponding to a
  learning rate of \(10^0\) and why?
\item
  What is the impact that the different values of learning have on the
  speed of the process and the results?
\item
  Of the options explored, what learning rate do you prefer and why?
\item
  Use your chosen learning rate for the remainder of this problem.
\end{itemize}

\subsubsection{E. Evaluate your model performance through cross
validation}\label{e.-evaluate-your-model-performance-through-cross-validation}

\textbf{1.9} Test the performance of your trained classifier using
K-folds cross validation resampling technique. The scikit-learn package
\href{http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\#sklearn.model_selection.StratifiedKFold}{StratifiedKFolds}
may be helpful.

\begin{itemize}
\item
  Train your logistic regression model and a K-Nearest Neighbor
  classification model with \(k=7\) nearest neighbors.
\item
  Using the trained models, make four plots: two for logistic regression
  and two for KNN. For each model have one plot showing the training
  data used for fitting the model, and the other showing the test data.
  On each plot, include the decision boundary resulting from your
  trained classifier.
\item
  Produce a Receiver Operating Characteristic curve (ROC curve) that
  represents the performance from cross validated performance evaluation
  for each classifier (your logistic regression model and the KNN model,
  with \(k=7\) nearest neighbors). For the cross validation, use
  \(k=10\) folds.

  \begin{itemize}
  \tightlist
  \item
    Plot these curves on the same set of axes to compare them. You
    should not plot one curve for each fold of k-folds; instead, you
    should plot one ROC curve for Logistic Regression and one for KNN
    (each should incorporate all 10 folds of validation). Also, don't
    forget to plot the ``chance'' line.
  \item
    On the ROC curve plot, also include the chance diagonal for
    reference (this represents the performance of the worst possible
    classifier). This is represented as a line from \((0,0)\) to
    \((1,1)\).
  \item
    Calculate the Area Under the Curve for each model and include this
    measure in the legend of the ROC plot.
  \end{itemize}
\item
  Comment on the following:

  \begin{itemize}
  \tightlist
  \item
    What is the purpose of using cross validation for this problem?
  \item
    How do the models compare in terms of performance (both ROC curves
    and decision boundaries) and which model (logistic regression or
    KNN) would you select to use on previously unseen data for this
    problem and why?
  \end{itemize}
\end{itemize}

\subsection{Exercise 2 - Digits
classification}\label{exercise-2---digits-classification}

\emph{An exploration of regularization, imbalanced classes, ROC and PR
curves}

\textbf{{[}30 points{]}}

The goal of this exercise is to apply your supervised learning skills on
a very different dataset: in this case, image data; MNIST: a collection
of images of handwritten digits. Your goal is to train a classifier that
is able to distinguish the number ``3'' from all possible numbers and to
do so as accurately as possible. You will first explore your data (this
should always be your starting point to gain domain knowledge about the
problem.). Since the feature space in this problem is 784-dimensional,
overfitting is possible. To avoid overfitting you will investigate the
impact of regularization on generalization performance (test accuracy)
and compare regularized and unregularized logistic regression model test
error against other classification techniques such as naive Bayes and
random forests and draw conclusions about the best-performing model.

Start by loading your dataset from the
\href{http://yann.lecun.com/exdb/mnist/}{MNIST dataset} of handwritten
digits, using the code provided below. MNIST has a training set of
60,000 examples, and a test set of 10,000 examples. The digits have been
size-normalized and centered in a fixed-size image.

Your goal is to classify whether or not an example digit is a 3. Your
binary classifier should predict \(y=1\) if the digit is a 3, and
\(y=0\) otherwise. Create your dataset by transforming your labels into
a binary format (3's are class 1, and all other digits are class 0).

\textbf{2.1} Plot 10 examples of each class (i.e.~class \(y=0\), which
are not 3's and class \(y=1\) which are 3's), from the training dataset.

\emph{Note that the data are composed of samples of length 784. These
represent 28 x 28 images, but have been reshaped for storage
convenience. To plot digit examples, you'll need to reshape the data to
be 28 x 28 (which can be done with numpy \texttt{reshape}).}

\textbf{2.2} How many examples are present in each class? Show a plot of
samples by class (bar plot). What fraction of samples are positive? What
issues might this cause?

\textbf{2.3} Identify the value of the regularization parameter that
optimizes model performance on out-of-sample data. Using a logistic
regression classifier, apply lasso regularization and retrain the model
and evaluate its performance on the test set over a range of values on
the regularization coefficient. You can implement this using the
\href{http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html}{LogisticRegression}
module and activating the `l1' penalty; the parameter \(C\) is the
inverse of the regularization strength. Vary the value of C
logarithmically from \(10^{-4}\) to \(10^4\) (and make your x-axes
logarithmic in scale) and evaluate it at least 20 different values of C.
As you vary the regularization coefficient, Plot the following four
quantities (this should result in 4 separate plots)\ldots{}

\begin{itemize}
\tightlist
\item
  The number of model parameters that are estimated to be nonzero (in
  the logistic regression model, one attribute is \texttt{coef\_}, which
  gives you access to the model parameters for a trained model)
\item
  The cross entropy loss (which can be evaluated with the Scikit Learn
  \texttt{log\_loss} function)
\item
  Area under the ROC curve (AUC)
\item
  The \(F_1\)-score (assuming a threshold of 0.5 on the predicted
  confidence scores, that is, scores above 0.5 are predicted as Class 1,
  otherwise Class 0). Scikit Learn also has a \texttt{f1\_score}
  function which may be useful. -Which value of C seems best for this
  problem? Please select the closest power of 10. You will use this in
  the next part of this exercise.
\end{itemize}

\textbf{2.4} Train and test a (1) logistic regression classifier with
minimal regularization (using the Scikit Learn package, set
penalty=`l1', C=1e100 to approximate this), (2) a logistic regression
classifier with the best value of the regularization parameter from the
last section, (3) a Gradient Boosting classifier, and (4) a Random
Forest (RF) classifier (using default parameters for the RF classifier).

\begin{itemize}
\tightlist
\item
  Compare your classifiers' performance using ROC and Precision Recall
  (PR) curves. For the ROC curves, all your curves should be plotted on
  the same set of axes so that you can directly compare them. Please do
  the same wih the PR curves.
\item
  Plot the line that represents randomly guessing the class (50\% of the
  time a ``3'', 50\% not a ``3''). You SHOULD NOT actually create random
  guesses. Instead, you should think through the theory behind how ROC
  and PR curves work and plot the appropriate lines. It's a good
  practice to include these in ROC and PR curve plots as a reference
  point.
\item
  For PR curves, an excellent resource on how to correctly plot them can
  be found
  \href{https://classeval.wordpress.com/introduction/introduction-to-the-precision-recall-plot/}{here}
  (ignore the section on ``non-linear interpolation between two
  points''). This describes how a random classifier is represented in PR
  curves and demonstrates that it should provide a lower bound on
  performance.
\item
  When training your logistic regression model, it's recommended that
  you use solver=``liblinear''; otherwise, your results may not
  converge.
\item
  Describe the performance of the classifiers you compared. Did the
  regularization of the logistic regression model make much difference
  here? Which classifier you would select for application to unseen
  data.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the MNIST Data}
\ImportTok{from}\NormalTok{ sklearn.datasets }\ImportTok{import}\NormalTok{ fetch\_openml}
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ train\_test\_split}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ pickle}

\CommentTok{\# Set this to True to download the data for the first time and False after the first time }
\CommentTok{\#   so that you just load the data locally instead}
\NormalTok{download\_data }\OperatorTok{=} \VariableTok{True}

\ControlFlowTok{if}\NormalTok{ download\_data:}
    \CommentTok{\# Load data from https://www.openml.org/d/554}
\NormalTok{    X, y }\OperatorTok{=}\NormalTok{ fetch\_openml(}\StringTok{\textquotesingle{}mnist\_784\textquotesingle{}}\NormalTok{, return\_X\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{, as\_frame}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
    
    \CommentTok{\# Adjust the labels to be \textquotesingle{}1\textquotesingle{} if y==3, and \textquotesingle{}0\textquotesingle{} otherwise}
\NormalTok{    y[y}\OperatorTok{!=}\StringTok{\textquotesingle{}3\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \DecValTok{0}
\NormalTok{    y[y}\OperatorTok{==}\StringTok{\textquotesingle{}3\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \DecValTok{1}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ y.astype(}\StringTok{\textquotesingle{}int\textquotesingle{}}\NormalTok{)}
    
    \CommentTok{\# Divide the data into a training and test split}
\NormalTok{    X\_train, X\_test, y\_train, y\_test }\OperatorTok{=}\NormalTok{ train\_test\_split(X, y, test\_size}\OperatorTok{=}\DecValTok{1}\OperatorTok{/}\DecValTok{7}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{88}\NormalTok{)}
    
    \BuiltInTok{file} \OperatorTok{=} \BuiltInTok{open}\NormalTok{(}\StringTok{\textquotesingle{}tmpdata\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}wb\textquotesingle{}}\NormalTok{)}
\NormalTok{    pickle.dump((X\_train, X\_test, y\_train, y\_test), }\BuiltInTok{file}\NormalTok{)}
    \BuiltInTok{file}\NormalTok{.close()}
\ControlFlowTok{else}\NormalTok{:}
    \BuiltInTok{file} \OperatorTok{=} \BuiltInTok{open}\NormalTok{(}\StringTok{\textquotesingle{}tmpdata\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}rb\textquotesingle{}}\NormalTok{)}
\NormalTok{    X\_train, X\_test, y\_train, y\_test }\OperatorTok{=}\NormalTok{ pickle.load(}\BuiltInTok{file}\NormalTok{)}
    \BuiltInTok{file}\NormalTok{.close()}
\end{Highlighting}
\end{Shaded}





\end{document}
