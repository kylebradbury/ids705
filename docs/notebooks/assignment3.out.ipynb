{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assignment 3\n",
        "\n",
        "Supervised Learning - model training and evaluation\n",
        "\n",
        "Kyle Bradbury  \n",
        "2025-02-05\n",
        "\n",
        "## Instructions\n",
        "\n",
        "*Instructions for all assignments can be found\n",
        "[here](https://kylebradbury.github.io/ids705/notebooks/assignment_instructions.html).\n",
        "Note: this assignment falls under collaboration Mode 2: Individual\n",
        "Assignment – Collaboration Permitted. Please refer to the syllabus for\n",
        "additional information. Please be sure to list the names of any students\n",
        "that you worked with on this assignment. Total points in the assignment\n",
        "add up to 90; an additional 10 points are allocated to professionalism\n",
        "and presentation quality.*\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "This assignment will provide structured practice to help enable you to…\n",
        "\n",
        "1.  Understand the primary workflow in machine learning: (1) identifying\n",
        "    a hypothesis function set of models, (2) determining a\n",
        "    loss/cost/error/objective function to minimize, and (3) minimizing\n",
        "    that function through gradient descent\n",
        "2.  Understand the inner workings of logistic regression and how linear\n",
        "    models for classification can be developed.\n",
        "3.  Gain practice in implementing machine learning algorithms from the\n",
        "    most basic building blocks to understand the math and programming\n",
        "    behind them to achieve practical proficiency with the techniques\n",
        "4.  Implement batch gradient descent and become familiar with how that\n",
        "    technique is used and its dependence on the choice of learning rate\n",
        "5.  Evaluate supervised learning algorithm performance through ROC\n",
        "    curves and using cross validation\n",
        "6.  Apply regularization to linear models to improve model\n",
        "    generalization performance\n",
        "\n",
        "## Exercise 1 - Classification using logistic regression: build it from the ground up\n",
        "\n",
        "**\\[60 points\\]**\n",
        "\n",
        "This exercise will walk you through the full life-cycle of a supervised\n",
        "machine learning classification problem. Classification problem consists\n",
        "of two features/predictors (e.g. petal width and petal length) and your\n",
        "goal is to predict one of two possible classes (class 0 or class 1). You\n",
        "will build, train, and evaluate the performance of a logistic regression\n",
        "classifier on the data provided. Before you begin any modeling, you’ll\n",
        "load and explore your data in Part I to familiarize yourself with it -\n",
        "and check for any missing or erroneous data. Then, in Part II, we will\n",
        "review an appropriate hypothesis set of functions to fit to the data: in\n",
        "this case, logistic regression. In Part III, we will derive an\n",
        "appropriate cost function for the data (spoiler alert: it’s\n",
        "cross-entropy) as well as the gradient descent update equation that will\n",
        "allow you to optimize that cost function to identify the parameters that\n",
        "minimize the cost for the training data. In Part IV, all the pieces come\n",
        "together and you will implement your logistic regression model class\n",
        "including methods for fitting the data using gradient descent. Using\n",
        "that model you’ll test it out and plot learning curves to verify the\n",
        "model learns as you train it and to identify and appropriate learning\n",
        "rate hyperparameter. Lastly, in Part V you will apply the model you\n",
        "designed, implemented, and verified to your actual data and evaluate and\n",
        "visualize its generalization performance as compared to a KNN algorithm.\n",
        "**When complete, you will have accomplished learning objectives 1-5\n",
        "above!**\n",
        "\n",
        "### A. Load, prepare, and plot your data\n",
        "\n",
        "> **Note**\n",
        ">\n",
        "> [Data for this exercise can be downloaded\n",
        "> here](https://github.com/kylebradbury/ids705/tree/main/notebooks/data/a3)\n",
        "\n",
        "You are given some data for which you are tasked with constructing a\n",
        "classifier. The first step when facing any machine learning project:\n",
        "look at your data!\n",
        "\n",
        "**1.1** Load the data.\n",
        "\n",
        "-   In the data folder in the same directory of this notebook, you’ll\n",
        "    find the data in `A3_Q1_data.csv`. This file contains the binary\n",
        "    class labels, $y$, and the features $x_1$ and $x_2$.\n",
        "-   Divide your data into a training and testing set where the test set\n",
        "    accounts for 30 percent of the data and the training set the\n",
        "    remaining 70 percent.  \n",
        "-   Plot the training data by class.\n",
        "-   Comment on the data: do the data appear separable? May logistic\n",
        "    regression be a good choice for these data? Why or why not?\n",
        "\n",
        "**1.2** Do the data require any preprocessing due to missing values,\n",
        "scale differences (e.g. different ranges of values), etc.? If so, how\n",
        "did you handle these issues?\n",
        "\n",
        "Next, we walk through our key steps for model fitting: choose a\n",
        "hypothesis set of models to train (in this case, logistic regression);\n",
        "identify a cost function to measure the model fit to our training data;\n",
        "optimize model parameters to minimize cost (in this case using gradient\n",
        "descent). Once we’ve completed model fitting, we will evaluate the\n",
        "performance of our model and compare performance to another approach (a\n",
        "KNN classifier).\n",
        "\n",
        "### B. Stating the hypothesis set of models to evaluate (we’ll use logistic regression)\n",
        "\n",
        "Given that our data consists of two features, our logistic regression\n",
        "problem will be applied to a two-dimensional feature space. Recall that\n",
        "our logistic regression model is:\n",
        "\n",
        "$$f(\\mathbf{x}_i,\\mathbf{w})=\\sigma(\\mathbf{w}^{\\top} \\mathbf{x}_i)$$\n",
        "\n",
        "where the sigmoid function is defined as\n",
        "$\\sigma(x) = \\dfrac{e^x}{1+e^{x}}= \\dfrac{1}{1+e^{-x}}$. Also, since\n",
        "this is a two-dimensional problem, we define\n",
        "$\\mathbf{w}^{\\top} \\mathbf{x}_i = w_0 x_{i,0} + w_1 x_{i,1} + w_2 x_{i,2}$\n",
        "and here, $\\mathbf{x}_i=[x_{i,0}, x_{i,1}, x_{i,2}]^{\\top}$, and\n",
        "$x_{i,0} \\triangleq 1$\n",
        "\n",
        "Remember from class that we interpret our logistic regression classifier\n",
        "output (or confidence score) as the conditional probability that the\n",
        "target variable for a given sample $y_i$ is from class “1”, given the\n",
        "observed features, $\\mathbf{x}_i$. For one sample,\n",
        "$(y_i, \\mathbf{x}_i)$, this is given as:\n",
        "\n",
        "$$P(Y=1|X=\\mathbf{x}_i) = f(\\mathbf{x}_i,\\mathbf{w})=\\sigma(\\mathbf{w}^{\\top} \\mathbf{x}_i)$$\n",
        "\n",
        "In the context of maximizing the likelihood of our parameters given the\n",
        "data, we define this to be the likelihood function\n",
        "$L(\\mathbf{w}|y_i,\\mathbf{x}_i)$, corresponding to one sample\n",
        "observation from the training dataset.\n",
        "\n",
        "*Aside: the careful reader will recognize this expression looks\n",
        "different from when we talk about the likelihood of our data given the\n",
        "true class label, typically expressed as $P(x|y)$, or the posterior\n",
        "probability of a class label given our data, typically expressed as\n",
        "$P(y|x)$. In the context of training a logistic regression model, the\n",
        "likelihood we are interested in is the likelihood function of our\n",
        "logistic regression **parameters**, $\\mathbf{w}$. It’s our goal to use\n",
        "this to choose the parameters to maximize the likelihood function.*\n",
        "\n",
        "**No output is required for this section - just read and use this\n",
        "information in the later sections.**\n",
        "\n",
        "### C. Find the cost function that we can use to choose the model parameters, $\\mathbf{w}$, that best fit the training data.\n",
        "\n",
        "**1.3** What is the likelihood function that corresponds to all the $N$\n",
        "samples in our training dataset that we will wish to maximize? Unlike\n",
        "the likelihood function written above which gives the likelihood\n",
        "function for a *single training data pair* $(y_i, \\mathbf{x}_i)$, this\n",
        "question asks for the likelihood function for the *entire training\n",
        "dataset*\n",
        "$\\{(y_1, \\mathbf{x}_1), (y_2, \\mathbf{x}_2), ..., (y_N, \\mathbf{x}_N)\\}$.\n",
        "\n",
        "**1.4** Since a logarithm is a monotonic function, maximizing the $f(x)$\n",
        "is equivalent to maximizing $\\ln [f(x)]$. Express the likelihood from\n",
        "the last question as a cost function of the model parameters,\n",
        "$C(\\mathbf{w})$; that is the negative of the logarithm of the\n",
        "likelihood. Express this cost as an average cost per sample (i.e. divide\n",
        "your final value by $N$), and use this quantity going forward as the\n",
        "cost function to optimize.\n",
        "\n",
        "**1.5** Calculate the gradient of the cost function with respect to the\n",
        "model parameters $\\nabla_{\\mathbf{w}}C(\\mathbf{w})$. Express this in\n",
        "terms of the partial derivatives of the cost function with respect to\n",
        "each of the parameters,\n",
        "e.g. $\\nabla_{\\mathbf{w}}C(\\mathbf{w}) = \\left[\\dfrac{\\partial C}{\\partial w_0}, \\dfrac{\\partial C}{\\partial w_1}, \\dfrac{\\partial C}{\\partial w_2}\\right]$.\n",
        "\n",
        "To simplify notation, please use $\\mathbf{w}^{\\top}\\mathbf{x}$ instead\n",
        "of writing out $w_0 x_{i,0} + w_1 x_{i,1} + w_2 x_{i,2}$ when it appears\n",
        "each time (where $x_{i,0} = 1$ for all $i$). You are also welcome to use\n",
        "$\\sigma()$ to represent the sigmoid function. Lastly, this will be a\n",
        "function the features, $x_{i,j}$ (with the first index in the subscript\n",
        "representing the observation and the second the feature; targets, $y_i$;\n",
        "and the logistic regression model parameters, $w_j$.\n",
        "\n",
        "**1.6** Write out the gradient descent update equation. This should\n",
        "clearly express how to update each weight from one step in gradient\n",
        "descent $w_j^{(k)}$ to the next $w_j^{(k+1)}$. There should be one\n",
        "equation for each model logistic regression model parameter (or you can\n",
        "represent it in vectorized form). Assume that $\\eta$ represents the\n",
        "learning rate.\n",
        "\n",
        "### D. Implement gradient descent and your logistic regression algorithm\n",
        "\n",
        "**1.7** Implement your logistic regression model.\n",
        "\n",
        "-   You are provided with a template, below, for a class with key\n",
        "    methods to help with your model development. It is modeled on the\n",
        "    Scikit-Learn convention. For this, you only need to create a version\n",
        "    of logistic regression for the case of two feature variables\n",
        "    (i.e. two predictors).\n",
        "\n",
        "-   Create a method called `sigmoid` that calculates the sigmoid\n",
        "    function\n",
        "\n",
        "-   Create a method called `cost` that computes the cost function\n",
        "    $C(\\mathbf{w})$ for a given dataset and corresponding class labels.\n",
        "    This should be the **average cost** (make sure your total cost is\n",
        "    divided by your number of samples in the dataset).\n",
        "\n",
        "-   Create a method called `gradient_descent` to run **one step** of\n",
        "    gradient descent on your training data. We’ll refer to this as\n",
        "    “batch” gradient descent since it takes into account the gradient\n",
        "    based on all our data at each iteration of the algorithm.\n",
        "\n",
        "-   Create a method called `fit` that fits the model to the data\n",
        "    (i.e. sets the model parameters to minimize cost) using your\n",
        "    `gradient_descent` method. In doing this we’ll need to make some\n",
        "    assumptions about the following:\n",
        "\n",
        "    -   *Weight initialization*. What should you initialize the model\n",
        "        parameters to? For this, randomly initialize the weights to a\n",
        "        different values between 0 and 1.\n",
        "    -   *Learning rate*. How slow/fast should the algorithm step towards\n",
        "        the minimum? This you will vary in a later part of this problem.\n",
        "    -   *Stopping criteria*. When should the algorithm be finished\n",
        "        searching for the optimum? There are two stopping criteria:\n",
        "        small changes in the gradient descent step size and a maximum\n",
        "        number of iterations. The first is whether there was a\n",
        "        sufficiently small change in the gradient; this is evaluated as\n",
        "        whether the magnitude of the step that the gradient descent\n",
        "        algorithm takes changes by less than $10^{-6}$ between\n",
        "        iterations. Since we have a weight vector, we can compute the\n",
        "        change in the weight by evaluating the $L_2$ norm (Euclidean\n",
        "        norm) of the change in the vector between iterations. From our\n",
        "        gradient descent update equation we know that mathematically\n",
        "        this is $||-\\eta\\nabla_{\\mathbf{w}}C(\\mathbf{w})||$. The second\n",
        "        criterion is met if a maximum number of iterations has been\n",
        "        reach (5,000 in this case, to prevent infinite loops from poor\n",
        "        choices of learning rates).\n",
        "    -   Design your approach so that at each step in the gradient\n",
        "        descent algorithm you evaluate the cost function for both the\n",
        "        training and the test data for each new value for the model\n",
        "        weights. You should be able to plot cost vs gradient descent\n",
        "        iteration for both the training and the test data. This will\n",
        "        allow you to plot “learning curves” that can be informative for\n",
        "        how the model training process is proceeding.\n",
        "\n",
        "-   Create a method called `predict_proba` that predicts confidence\n",
        "    scores (that can be thresholded into the predictions of the\n",
        "    `predict` method.\n",
        "\n",
        "-   Create a method called `predict` that makes predictions based on the\n",
        "    trained model, selecting the most probable class, given the data, as\n",
        "    the prediction, that is class that yields the larger\n",
        "    $P(y|\\mathbf{x})$.\n",
        "\n",
        "-   (Optional, but recommended) Create a method called `learning_curve`\n",
        "    that produces the cost function values that correspond to each step\n",
        "    from a previously run gradient descent operation.\n",
        "\n",
        "-   (Optional, but recommended) Create a method called `prepare_x` which\n",
        "    appends a column of ones as the first feature of the dataset\n",
        "    $\\mathbf{X}$ to account for the bias term ($x_{i,1}=1$).\n",
        "\n",
        "This structure is strongly encouraged; however, you’re welcome to adjust\n",
        "this to your needs (adding helper methods, modifying parameters, etc.)."
      ],
      "id": "9e5ffb54-1b20-4f57-a6fd-91703d525639"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Logistic regression class\n",
        "class Logistic_regression:\n",
        "    # Class constructor\n",
        "    def __init__(self):\n",
        "        self.w = None     # logistic regression weights\n",
        "        self.saved_w = [] # Since this is a small problem, we can save the weights\n",
        "                          #  at each iteration of gradient descent to build our \n",
        "                          #  learning curves\n",
        "        # returns nothing\n",
        "        pass\n",
        "    \n",
        "    # Method for calculating the sigmoid function of w^T X for an input set of weights\n",
        "    def sigmoid(self, X, w):\n",
        "        # returns the value of the sigmoid\n",
        "        pass\n",
        "    \n",
        "    # Cost function for an input set of weights\n",
        "    def cost(self, X, y, w):\n",
        "        # returns the average cross entropy cost\n",
        "        pass\n",
        "    \n",
        "    # Update the weights in an iteration of gradient descent\n",
        "    def gradient_descent(self, X, y, lr):\n",
        "        # returns a scalar of the magnitude of the Euclidean norm \n",
        "        #  of the change in the weights during one gradient descent step\n",
        "        pass\n",
        "    \n",
        "    # Fit the logistic regression model to the data through gradient descent\n",
        "    def fit(self, X, y, w_init, lr, delta_thresh=1e-6, max_iter=5000, verbose=False):\n",
        "        # Note the verbose flag enables you to print out the weights at each iteration \n",
        "        #  (optional - but may help with one of the questions)\n",
        "        \n",
        "        # returns nothing\n",
        "        pass\n",
        "    \n",
        "    # Use the trained model to predict the confidence scores (prob of positive class in this case)\n",
        "    def predict_proba(self, X):\n",
        "        # returns the confidence score for the each sample\n",
        "        pass\n",
        "    \n",
        "    # Use the trained model to make binary predictions\n",
        "    def predict(self, X, thresh=0.5):\n",
        "        # returns a binary prediction for each sample\n",
        "        pass\n",
        "    \n",
        "    # Stores the learning curves from saved weights from gradient descent\n",
        "    def learning_curve(self, X, y):\n",
        "        # returns the value of the cost function from each step in gradient descent\n",
        "        #  from the last model fitting process\n",
        "        pass\n",
        "    \n",
        "    # Appends a column of ones as the first feature to account for the bias term\n",
        "    def prepare_x(self, X):\n",
        "        # returns the X with a new feature of all ones (a column that is the new column 0)\n",
        "        pass"
      ],
      "id": "cell-7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1.8** Choose a learning rate and fit your model. Learning curves are a\n",
        "plot of metrics of model performance evaluated through the process of\n",
        "model training to provide insights about how model training is\n",
        "proceeding. Show the learning curves for the gradient descent process\n",
        "for learning rates of $\\{10^{-0}, 10^{-2}, 10^{-4}\\}$. For each learning\n",
        "rate plot the learning curves by plotting **both the training and test\n",
        "data average cost** as a function of each iteration of gradient descent.\n",
        "You should run the model fitting process until it completes (up to 5,000\n",
        "iterations of gradient descent). All of the 6 resulting curves (train\n",
        "and test average cost for each learning rate) should be plotted on the\n",
        "**same set of axes** to enable direct comparison. *Note: make sure\n",
        "you’re using average cost per sample, not the total cost*.\n",
        "\n",
        "-   Try running this process for a really big learning rate for this\n",
        "    problem: $10^2$. Look at the weights that the fitting process\n",
        "    generates over the first 50 iterations and how they change. Either\n",
        "    print these first 50 iterations as console output or plot them. What\n",
        "    happens? How does the output compare to that corresponding to a\n",
        "    learning rate of $10^0$ and why?\n",
        "-   What is the impact that the different values of learning have on the\n",
        "    speed of the process and the results?\n",
        "-   Of the options explored, what learning rate do you prefer and why?\n",
        "-   Use your chosen learning rate for the remainder of this problem.\n",
        "\n",
        "### E. Evaluate your model performance through cross validation\n",
        "\n",
        "**1.9** Test the performance of your trained classifier using K-folds\n",
        "cross validation resampling technique. The scikit-learn package\n",
        "[StratifiedKFolds](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold)\n",
        "may be helpful.\n",
        "\n",
        "-   Train your logistic regression model and a K-Nearest Neighbor\n",
        "    classification model with $k=7$ nearest neighbors.\n",
        "\n",
        "-   Using the trained models, make four plots: two for logistic\n",
        "    regression and two for KNN. For each model have one plot showing the\n",
        "    training data used for fitting the model, and the other showing the\n",
        "    test data. On each plot, include the decision boundary resulting\n",
        "    from your trained classifier.\n",
        "\n",
        "-   Produce a Receiver Operating Characteristic curve (ROC curve) that\n",
        "    represents the performance from cross validated performance\n",
        "    evaluation for each classifier (your logistic regression model and\n",
        "    the KNN model, with $k=7$ nearest neighbors). For the cross\n",
        "    validation, use $k=10$ folds.\n",
        "\n",
        "    -   Plot these curves on the same set of axes to compare them. You\n",
        "        should not plot one curve for each fold of k-folds; instead, you\n",
        "        should plot one ROC curve for Logistic Regression and one for\n",
        "        KNN (each should incorporate all 10 folds of validation). Also,\n",
        "        don’t forget to plot the “chance” line.\n",
        "    -   On the ROC curve plot, also include the chance diagonal for\n",
        "        reference (this represents the performance of the worst possible\n",
        "        classifier). This is represented as a line from $(0,0)$ to\n",
        "        $(1,1)$.\n",
        "    -   Calculate the Area Under the Curve for each model and include\n",
        "        this measure in the legend of the ROC plot.\n",
        "\n",
        "-   Comment on the following:\n",
        "\n",
        "    -   What is the purpose of using cross validation for this problem?\n",
        "    -   How do the models compare in terms of performance (both ROC\n",
        "        curves and decision boundaries) and which model (logistic\n",
        "        regression or KNN) would you select to use on previously unseen\n",
        "        data for this problem and why?\n",
        "\n",
        "## Exercise 2 - Digits classification\n",
        "\n",
        "*An exploration of regularization, imbalanced classes, ROC and PR\n",
        "curves*\n",
        "\n",
        "**\\[30 points\\]**\n",
        "\n",
        "The goal of this exercise is to apply your supervised learning skills on\n",
        "a very different dataset: in this case, image data; MNIST: a collection\n",
        "of images of handwritten digits. Your goal is to train a classifier that\n",
        "is able to distinguish the number “3” from all possible numbers and to\n",
        "do so as accurately as possible. You will first explore your data (this\n",
        "should always be your starting point to gain domain knowledge about the\n",
        "problem.). Since the feature space in this problem is 784-dimensional,\n",
        "overfitting is possible. To avoid overfitting you will investigate the\n",
        "impact of regularization on generalization performance (test accuracy)\n",
        "and compare regularized and unregularized logistic regression model test\n",
        "error against other classification techniques such as naive Bayes and\n",
        "random forests and draw conclusions about the best-performing model.\n",
        "\n",
        "Start by loading your dataset from the [MNIST\n",
        "dataset](http://yann.lecun.com/exdb/mnist/) of handwritten digits, using\n",
        "the code provided below. MNIST has a training set of 60,000 examples,\n",
        "and a test set of 10,000 examples. The digits have been size-normalized\n",
        "and centered in a fixed-size image.\n",
        "\n",
        "Your goal is to classify whether or not an example digit is a 3. Your\n",
        "binary classifier should predict $y=1$ if the digit is a 3, and $y=0$\n",
        "otherwise. Create your dataset by transforming your labels into a binary\n",
        "format (3’s are class 1, and all other digits are class 0).\n",
        "\n",
        "**2.1** Plot 10 examples of each class (i.e. class $y=0$, which are not\n",
        "3’s and class $y=1$ which are 3’s), from the training dataset.\n",
        "\n",
        "*Note that the data are composed of samples of length 784. These\n",
        "represent 28 x 28 images, but have been reshaped for storage\n",
        "convenience. To plot digit examples, you’ll need to reshape the data to\n",
        "be 28 x 28 (which can be done with numpy `reshape`).*\n",
        "\n",
        "**2.2** How many examples are present in each class? Show a plot of\n",
        "samples by class (bar plot). What fraction of samples are positive? What\n",
        "issues might this cause?\n",
        "\n",
        "**2.3** Identify the value of the regularization parameter that\n",
        "optimizes model performance on out-of-sample data. Using a logistic\n",
        "regression classifier, apply lasso regularization and retrain the model\n",
        "and evaluate its performance on the test set over a range of values on\n",
        "the regularization coefficient. You can implement this using the\n",
        "[LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
        "module and activating the ‘l1’ penalty; the parameter $C$ is the inverse\n",
        "of the regularization strength. Vary the value of C logarithmically from\n",
        "$10^{-4}$ to $10^4$ (and make your x-axes logarithmic in scale) and\n",
        "evaluate it at least 20 different values of C. As you vary the\n",
        "regularization coefficient, Plot the following four quantities (this\n",
        "should result in 4 separate plots)…\n",
        "\n",
        "-   The number of model parameters that are estimated to be nonzero (in\n",
        "    the logistic regression model, one attribute is `coef_`, which gives\n",
        "    you access to the model parameters for a trained model)\n",
        "-   The cross entropy loss (which can be evaluated with the Scikit Learn\n",
        "    `log_loss` function)\n",
        "-   Area under the ROC curve (AUC)\n",
        "-   The $F_1$-score (assuming a threshold of 0.5 on the predicted\n",
        "    confidence scores, that is, scores above 0.5 are predicted as Class\n",
        "    1, otherwise Class 0). Scikit Learn also has a `f1_score` function\n",
        "    which may be useful. -Which value of C seems best for this problem?\n",
        "    Please select the closest power of 10. You will use this in the next\n",
        "    part of this exercise.\n",
        "\n",
        "**2.4** Train and test a (1) logistic regression classifier with minimal\n",
        "regularization (using the Scikit Learn package, set penalty=‘l1’,\n",
        "C=1e100 to approximate this), (2) a logistic regression classifier with\n",
        "the best value of the regularization parameter from the last section,\n",
        "(3) a Gradient Boosting classifier, and (4) a Random Forest (RF)\n",
        "classifier (using default parameters for the RF classifier).\n",
        "\n",
        "-   Compare your classifiers’ performance using ROC and Precision Recall\n",
        "    (PR) curves. For the ROC curves, all your curves should be plotted\n",
        "    on the same set of axes so that you can directly compare them.\n",
        "    Please do the same wih the PR curves.\n",
        "-   Plot the line that represents randomly guessing the class (50% of\n",
        "    the time a “3”, 50% not a “3”). You SHOULD NOT actually create\n",
        "    random guesses. Instead, you should think through the theory behind\n",
        "    how ROC and PR curves work and plot the appropriate lines. It’s a\n",
        "    good practice to include these in ROC and PR curve plots as a\n",
        "    reference point.\n",
        "-   For PR curves, an excellent resource on how to correctly plot them\n",
        "    can be found\n",
        "    [here](https://classeval.wordpress.com/introduction/introduction-to-the-precision-recall-plot/)\n",
        "    (ignore the section on “non-linear interpolation between two\n",
        "    points”). This describes how a random classifier is represented in\n",
        "    PR curves and demonstrates that it should provide a lower bound on\n",
        "    performance.\n",
        "-   When training your logistic regression model, it’s recommended that\n",
        "    you use solver=“liblinear”; otherwise, your results may not\n",
        "    converge.\n",
        "-   Describe the performance of the classifiers you compared. Did the\n",
        "    regularization of the logistic regression model make much difference\n",
        "    here? Which classifier you would select for application to unseen\n",
        "    data."
      ],
      "id": "37aec36c-dddf-49fc-9821-e5c2ca577188"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the MNIST Data\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "# Set this to True to download the data for the first time and False after the first time \n",
        "#   so that you just load the data locally instead\n",
        "download_data = True\n",
        "\n",
        "if download_data:\n",
        "    # Load data from https://www.openml.org/d/554\n",
        "    X, y = fetch_openml('mnist_784', return_X_y=True, as_frame=False)\n",
        "    \n",
        "    # Adjust the labels to be '1' if y==3, and '0' otherwise\n",
        "    y[y!='3'] = 0\n",
        "    y[y=='3'] = 1\n",
        "    y = y.astype('int')\n",
        "    \n",
        "    # Divide the data into a training and test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/7, random_state=88)\n",
        "    \n",
        "    file = open('tmpdata', 'wb')\n",
        "    pickle.dump((X_train, X_test, y_train, y_test), file)\n",
        "    file.close()\n",
        "else:\n",
        "    file = open('tmpdata', 'rb')\n",
        "    X_train, X_test, y_train, y_test = pickle.load(file)\n",
        "    file.close()"
      ],
      "id": "cell-12"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "varInspector": {
      "cols": {
        "lenName": "16",
        "lenType": "16",
        "lenVar": "40"
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ")",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list())"
        }
      },
      "position": {
        "height": "722px",
        "left": "1550px",
        "right": "20px",
        "top": "121px",
        "width": "353px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  }
}