[
  {
    "objectID": "shared/assignment_template.html",
    "href": "shared/assignment_template.html",
    "title": "Assignment 0 - Assignment Template",
    "section": "",
    "text": "Name: Jane Doe\nNetid: jd123\nCollaborators: Jessika Schneider, Athos Zetticci"
  },
  {
    "objectID": "shared/assignment_template.html#exercise-1---derivatives",
    "href": "shared/assignment_template.html#exercise-1---derivatives",
    "title": "Assignment 0 - Assignment Template",
    "section": "Exercise 1 - Derivatives",
    "text": "Exercise 1 - Derivatives\n1.1\nThe first derivative is: \\[ \\frac{df}{dt} = 3t^2\\]\n1.2\nThe second derivative is: \\[ \\frac{d^2f}{dt^2} = 6t\\]\n1.3\nWe calculate the second derivative at time \\(t=3\\) as follows:\n\n# (c) Numerically evaluate the second derivative for f(x)\n\n# Initialize variables for the analysis\nt = 3\n\n# Compute the derivative\ndf2 = 6 * t\n\nprint('The second derivative at t = 3 is {}'.format(df2))\n\nThe second derivative at t = 3 is 18\n\n\n1.4\nThis derivative, which is the second derivative of distance with respect to time represents the acceleration of the ball in \\(m^2\\) per second. This shows how quickly the ball is increasing its speed over time."
  },
  {
    "objectID": "shared/assignment_template.html#exercise-2---plotting",
    "href": "shared/assignment_template.html#exercise-2---plotting",
    "title": "Assignment 0 - Assignment Template",
    "section": "Exercise 2 - Plotting",
    "text": "Exercise 2 - Plotting\n2.1\nPlotting the first derivative of \\(f(t)\\) for the values \\(-2 \\leq t \\leq 1\\)\n\n%config InlineBackend.figure_format = 'retina' # Make clear on high-res screens\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create the data to plot\nt = np.linspace(-2,2,1000)\nf_derivative = 3 * t**2\n\n# Plot the data\nfig, ax = plt.subplots(figsize=(5,5))\nax.plot(t,f_derivative, color = 'red')\n\n# Always use X and Y labels. Note that in Python r'string' allows LaTeX to \n# interpret the content of the text string\nax.set_xlabel(r'$t$ (seconds)') \nax.set_ylabel(r'Velocity, $\\frac{df}{dt}$, (m/s)')\nax.set_xlim([-2,2]) \nax.grid('on')\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1. The velocity (first derivative of distance) in meters per second.\n2.2\nThe first derivative of \\(f(t)\\), which represents distance in meters as a function of time in seconds is velocity, in units of meters per second, as plotted in Figure 1. In this case, that means that at time \\(t=2\\) the velocity is 12 meters per second."
  },
  {
    "objectID": "notebooks/assignment_instructions.html",
    "href": "notebooks/assignment_instructions.html",
    "title": "Assignment Instructions",
    "section": "",
    "text": "Assignment grading will consist of two components: content (90%) and presentation (10%). The content grade will be based on the accuracy and completeness (including the depth) of your answers to each question. If a question asks you to explain, hypothesize, or otherwise think critically, be sure to demonstrate your critical engagement with the question. Presentation and collaboration are critical skills for success as a data scientist. To support you in refining those skills, we include the presentation component of the grade. The presentation score will be based on how well you communicate in writing, figure creation, and coding. Clear writing (with appropriate grammar and spelling), organized answers, well-organized/commented code, and properly formatted figures will ensure your success on the presentation component of the assignment.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment Instructions"
    ]
  },
  {
    "objectID": "notebooks/assignment_instructions.html#assignment-grading",
    "href": "notebooks/assignment_instructions.html#assignment-grading",
    "title": "Assignment Instructions",
    "section": "",
    "text": "Assignment grading will consist of two components: content (90%) and presentation (10%). The content grade will be based on the accuracy and completeness (including the depth) of your answers to each question. If a question asks you to explain, hypothesize, or otherwise think critically, be sure to demonstrate your critical engagement with the question. Presentation and collaboration are critical skills for success as a data scientist. To support you in refining those skills, we include the presentation component of the grade. The presentation score will be based on how well you communicate in writing, figure creation, and coding. Clear writing (with appropriate grammar and spelling), organized answers, well-organized/commented code, and properly formatted figures will ensure your success on the presentation component of the assignment.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment Instructions"
    ]
  },
  {
    "objectID": "notebooks/assignment_instructions.html#how-to-submit-assignments",
    "href": "notebooks/assignment_instructions.html#how-to-submit-assignments",
    "title": "Assignment Instructions",
    "section": "How to submit assignments",
    "text": "How to submit assignments\nAssignments will be submitted via Gradescope as a PDF and you will need to indicate the location of your work for each question and subquestion. Below is a step-by-step guide to submitting your assignments.\n\nHeader. At the top of the document, include the assignment number, your name, your netid, and a list of anyone you collaborated with on the assignment.\nEach answer is prefaced in bold by the question/subquestion number. Each question is followed by one or more sections typically labeled with section numbers (e.g. 2.3). Make sure you have a cell with that section number prior to your response. You may use multiple cells for each answer that include both markdown and code. For full credit, submitted assignments should be easily navigable with answers for subsections clearly indicated.\nEnsure that all cells have been run. Cells that have not been run will be treated as unanswered questions and assigned zero points.\nCreate a PDF document of your notebook. There are a few ways to do this. Please see the guide below.\nYour content is legible prior to submission. Look over your PDF before you submit it. If we cannot read it, or parts are missing, we cannot grade it, and no credit will be given for anything we cannot read or is missing.\n\nCode is valid (able to run and producing the correct answer), neat, understandable, and well commented.\nMath is either clearly written and inserted into the proper part of the document or typeset using LateX equations.\nAll text that is not code should be formatted using markdown. Two references to help include: ref1 and ref2.\n\nSubmit your assignment by the deadline on gradescope. Any assignments received after the deadline will be treated as late. Please see this video for how to submit your assignment on gradescope. The submission link is here: https://www.gradescope.com/\nAssign the respective pages to each question. When you submit your assignment on Gradescope, you will be asked to assign pages of your PDF to each question accordingly. Be sure to leave time to complete this before submission. Please remember, if we cannot find your answer, we cannot grade it.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment Instructions"
    ]
  },
  {
    "objectID": "notebooks/assignment_instructions.html#example-question-and-template",
    "href": "notebooks/assignment_instructions.html#example-question-and-template",
    "title": "Assignment Instructions",
    "section": "Example Question and Template",
    "text": "Example Question and Template\nTo demonstrate how to organize your responses using a Jupyter notebook please review the following:\n\nSample assignment questions\nSample assignment template and example for submission\n\n\nEntering mathematical equations\nYou may either write out equations by hand or using markdown and LaTeX (LaTeX is recommended). Either way, I recommend that you complete the work on paper before typing up the final version if you choose to use LaTeX. If you hand-write your math, please digitize them (scan them in or take a picture) and place them in the proper order for of the document for your final PDF. Either way, show your math including any intermediate steps necessary to understand the logic of your solution. If we are not able to interpret your meaning (or understand your writing), no credit will be given.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment Instructions"
    ]
  },
  {
    "objectID": "notebooks/assignment_instructions.html#rendering-a-pdf-file",
    "href": "notebooks/assignment_instructions.html#rendering-a-pdf-file",
    "title": "Assignment Instructions",
    "section": "Rendering a PDF file",
    "text": "Rendering a PDF file\n\nOption 1: Render from VS Code (without additional extensions)\nWe recommend using VS Code for this course and for use in developing your Jupyter notebooks. Rendering pdfs from Jupyter Notebooks in VS Code has a couple of extra steps, though. To render a pdf without any additional extensions, you do the following:\n\nOpen your notebook in VS code and hit the ellipsis (the three horizontal dots) just to the right of the tools for your notebook:\n\n\n\nClick “Export”:\n\n\n\nSelect “HTML” (exporting to pdf won’t work without the installation of additional tools):\n\n\n\nOpen the HTML file you just saved in Chrome:\n\n\n\nClick the options button (three vertical dots) and click “print”:\n\n\n\nSelect the destination as “Save as PDF” and click “Save”:\n\n\n\nVoila! You should have a pdf\n\n\n\nOption 2: Render from VS Code with Quarto and LaTeX\nYou can save a lot of the above steps if you’re willing to install 2 things: 1. Install Quarto, which is an open-source scientific and technical publishing system (great for making websites and blogs for your professional portfolio). 2. Install the Quarto VS Code extension 3. Install a version of tex for your operating system. 4. Once you do this, you can directly export your documents to pdf:\n\n\n\nOption 3: Render from Jupyter Notebooks\nOpen your notebook in a Jupyter Notebook in Google Chrome. Go to File-&gt;Print Preview, then after verifying the document looks correct, click “print” and for your printer choose “Save as PDF.”\nWhichever method you choose - always check your pdf before submitting to make sure everything is rendered and nothing is cut off!",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment Instructions"
    ]
  },
  {
    "objectID": "notebooks/assignment_instructions.html#figure-guidelines",
    "href": "notebooks/assignment_instructions.html#figure-guidelines",
    "title": "Assignment Instructions",
    "section": "Figure Guidelines",
    "text": "Figure Guidelines\nHere is an example of a well-prepared figure that checks all of the requirements for good figures. Please check out this Coursera course for skill development and Python plotting best practices.\n\nFigure checklist\n\nAll plots should have a purpose - either being directly requested or should make a clear point.\nAll plots should have axes labels and legible fonts (large enough to read).\nLegends are used when there are multiple series plotted on a single plot.\nMarkers on plots are properly sized (e.g. not be too big nor too small) and/or have the appropriate level of transparency to be able to clearly read the data without obscuring other data.\nIf there are multiple colors used on a plot, EVERY color can be distinguished from the rest.\nFigures are crisp and clear - there are no blurry figures.\nPlots should be explained clearly in text or have a caption explaining them\n\nTo demonstrate we’ll start by loading some data to plot (the example here is from the lecture on the bias-variance tradeoff):\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\n\n# Load the data for plotting\ndatafilename = './plot_example_data/data.pkl'\ninfile = open(datafilename,'rb')\nloaded_data = pickle.load(infile)\ninfile.close()\n\n# Store the loaded data in convenient variable names\nkvalues = loaded_data['kvalues']\nerror_training = loaded_data['error_training']\nerror_testing = loaded_data['error_testing']\nerror_bayesclf = loaded_data['error_bayesclf']\n\n# Choose colors that will be distinguishable from one another\ncolor0 = '#121619' # Dark grey\ncolor1 = '#00B050' # Green\ncolor2 = '#7c7c7c' # Light grey\n\nThen, we create the plot:\n\n%config InlineBackend.figure_format = 'retina' # Make clear on high-res screens\n\n# Create the plot\nfig, ax = plt.subplots(figsize=(7,5), dpi= 100) # Adjust the figure size and dots per inch to make it legible and clear \nax.semilogx(kvalues,error_training,\n            color=color0,\n            label='Training (in-sample)')\nax.semilogx(kvalues,error_testing,\n            color=color1,\n            label='Test (out-of-sample)')\nax.semilogx(kvalues,error_bayesclf,'--',\n            color=color2,\n            label='Bayes (optimal)')\nax.legend()\nax.grid('on')\nax.set_xlabel('k nearest neighbors') # Always use X and Y labels\nax.set_ylabel('Binary Classification Error Rate')\nax.set_xlim([1,200]) # Ensure the axis is the right size for the plot data\nax.set_ylim([0,0.5])\nfig.tight_layout() # Use this to maximize the use of space in the figure\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1. Ideally, each figure should have a caption that explains the figure. Here, the test data (green) approximates the generalization error rate, the lower-bound of which is the Bayes error rate (grey dotted line). The value of \\(k\\) represents the flexibility of the model with lower values of \\(k\\) representing higher model flexibility and higher values of \\(k\\), lower flexibility. The training error (black) is not considered out of sample, so in cases of high model flexiblity (and in this case high overfit), the training error rate can reach zero.\nNote the clear x and y axis labels and legends - there are no acronyms or shorthands used, just the full description of what is being plotted. Also note how easy it is to compare the color of lines and identify the baseline (Bayes) comparison line. Make your plots easy to read and understand. A figure caption is generally helpful as well to indicate what we are seeing in each plot.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment Instructions"
    ]
  },
  {
    "objectID": "notebooks/assignment_instructions.html#sources",
    "href": "notebooks/assignment_instructions.html#sources",
    "title": "Assignment Instructions",
    "section": "Sources",
    "text": "Sources\nSome questions on the assignments are adapted from sources including:\n1. James et al., An Introduction to Statistical Learning\n2. Abu-Mostafa, Yaser, Learning from Data\n3. Weinberger, Kilian, Machine Learning CS4780, Cornell University",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment Instructions"
    ]
  },
  {
    "objectID": "notebooks/assignment4.html",
    "href": "notebooks/assignment4.html",
    "title": "Assignment 4",
    "section": "",
    "text": "Instructions for all assignments can be found here. Note: this assignment falls under collaboration Mode 2: Individual Assignment – Collaboration Permitted. Please refer to the syllabus for additional information. Please be sure to list the names of any students that you worked with on this assignment. Total points in the assignment add up to 90; an additional 10 points are allocated to professionalism and presentation quality.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 4"
    ]
  },
  {
    "objectID": "notebooks/assignment4.html#instructions",
    "href": "notebooks/assignment4.html#instructions",
    "title": "Assignment 4",
    "section": "",
    "text": "Instructions for all assignments can be found here. Note: this assignment falls under collaboration Mode 2: Individual Assignment – Collaboration Permitted. Please refer to the syllabus for additional information. Please be sure to list the names of any students that you worked with on this assignment. Total points in the assignment add up to 90; an additional 10 points are allocated to professionalism and presentation quality.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 4"
    ]
  },
  {
    "objectID": "notebooks/assignment4.html#learning-objectives",
    "href": "notebooks/assignment4.html#learning-objectives",
    "title": "Assignment 4",
    "section": "Learning objectives",
    "text": "Learning objectives\nThrough completing this assignment you will be able to…\n\nIdentify key hyperparameters in neural networks and how they can impact model training and fit\nBuild, tune the parameters of, and apply feed-forward neural networks to data\nImplement and explain each and every part of a standard fully-connected neural network and its operation including feed-forward propagation, backpropagation, and gradient descent.\nApply a standard neural network implementation and search the hyperparameter space to select optimized values.\nDevelop a detailed understanding of the math and practical implementation considerations of neural networks, one of the most widely used machine learning tools, so that it can be leveraged for learning about other neural networks of different model architectures.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 4"
    ]
  },
  {
    "objectID": "notebooks/assignment4.html#exercise-1---exploring-and-optimizing-neural-network-hyperparameters",
    "href": "notebooks/assignment4.html#exercise-1---exploring-and-optimizing-neural-network-hyperparameters",
    "title": "Assignment 4",
    "section": "Exercise 1 - Exploring and optimizing neural network hyperparameters",
    "text": "Exercise 1 - Exploring and optimizing neural network hyperparameters\n[60 points]\nNeural networks have become ubiquitous in the machine learning community, demonstrating exceptional performance over a wide range of supervised learning tasks. The benefits of these techniques come at a price of increased computational complexity and model designs with increased numbers of hyperparameters that need to be correctly set to make these techniques work. It is common that poor hyperparameter choices in neural networks result in significant decreases in model generalization performance. The goal of this exercise is to better understand some of the key hyperparameters you will encounter in practice using neural networks so that you can be better prepared to tune your model for a given application. Through this exercise, you will explore two common approaches to hyperparameter tuning a manual approach where we greedily select the best individual hyperparameter (often people will pick potentially sensible options, try them, and hope it works) as well as a random search of the hyperparameter space which as been shown to be an efficient way to achieve good hyperparameter values.\nTo explore this, we’ll be using the example data created below throughout this exercise and the various training, validation, test splits. We will select each set of hyperparameters for our greedy/manual approach and the random search using a training/validation split, then retrain on the combined training and validation data before finally evaluating our generalization performance for both our final models on the test data.\n\n# Optional for clear plotting on Macs\n# %config InlineBackend.figure_format='retina'\n\n# Some of the network training leads to warnings. When we know and are OK with \n#  what's causing the warning and simply don't want to see it, we can use the \n#  following code. Run this block\n#  to disable warnings\nimport sys\nimport os\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n    os.environ[\"PYTHONWARNINGS\"] = 'ignore'\n\n\nimport numpy as np\nfrom sklearn.model_selection import PredefinedSplit\n\n#-----------------------------------------------------------------------------\n# Create the data\n#-----------------------------------------------------------------------------\n# Data generation function to create a checkerboard-patterned dataset\ndef make_data_normal_checkerboard(n, noise=0):\n    n_samples = int(n/4)\n    shift = 0.5\n    c1a = np.random.randn(n_samples,2)*noise + [-shift, shift]\n    c1b = np.random.randn(n_samples,2)*noise + [shift, -shift]\n    c0a = np.random.randn(n_samples,2)*noise + [shift, shift]\n    c0b = np.random.randn(n_samples,2)*noise + [-shift, -shift]\n    X = np.concatenate((c1a,c1b,c0a,c0b),axis=0)\n    y = np.concatenate((np.ones(2*n_samples), np.zeros(2*n_samples)))\n    \n    # Set a cutoff to the data and fill in with random uniform data:\n    cutoff = 1.25\n    indices_to_replace = np.abs(X)&gt;cutoff\n    for index,value in enumerate(indices_to_replace.ravel()):\n        if value:\n            X.flat[index] = np.random.rand()*2.5-1.25\n    return (X,y)\n\n# Training datasets\nnp.random.seed(42)\nnoise = 0.45\nX_train,y_train = make_data_normal_checkerboard(500, noise=noise)\n    \n# Validation and test data\nX_val,y_val = make_data_normal_checkerboard(500, noise=noise)\nX_test,y_test = make_data_normal_checkerboard(500, noise=noise)\n\n# For RandomSeachCV, we will need to combine training and validation sets then\n#  specify which portion is training and which is validation\n# Also, for the final performance evaluation, train on all of the training AND \n#  validation data\nX_train_plus_val = np.concatenate((X_train, X_val), axis=0)\ny_train_plus_val = np.concatenate((y_train, y_val), axis=0)\n\n# Create a predefined train/test split for RandomSearchCV (to be used later)\nvalidation_fold = np.concatenate((-1*np.ones(len(y_train)), np.zeros(len(y_val))))\ntrain_val_split = PredefinedSplit(validation_fold)\n\nTo help get you started we should always begin by visualizing our training data, here’s some code that does that:\n\nimport matplotlib.pyplot as plt\n\n# Code to plot the sample data\ndef plot_data(ax,X,y,title, limits):\n    # Select the colors to use in the plots\n    color0 = '#121619' # Dark grey\n    color1 = '#00B050' # Green\n    color_boundary='#858585'\n    \n    # Separate samples by class\n    samples0 = X[y==0]\n    samples1 = X[y==1]\n\n    ax.plot(samples0[:,0],samples0[:,1],\n        marker='o',\n        markersize=5,\n        linestyle=\"None\",\n        color=color0,\n        markeredgecolor='w',\n        markeredgewidth=0.5,\n        label='Class 0')\n    ax.plot(samples1[:,0],samples1[:,1],\n        marker='o',\n        markersize=5,\n        linestyle=\"None\",\n        color=color1,\n        markeredgecolor='w',\n        markeredgewidth=0.5,\n        label='Class 1')\n    ax.set_title(title)\n    ax.set_xlabel('$x_1$')\n    ax.set_ylabel('$x_2$')\n    ax.legend(loc='upper left')\n    ax.set_aspect('equal')\n\nfig, ax = plt.subplots(constrained_layout=True, figsize=(5,5))\nlimits = [-1.25, 1.25, -1.25, 1.25]\nplot_data(ax, X_train, y_train, 'Training Data', limits)\n\n\n\n\n\n\n\n\nThe hyperparameters we want to explore control the architecture of our model and how our model is fit to our data. These hyperparameters include the (a) learning rate, (b) batch size, and the (c) regularization coefficient, as well as the (d) model architecture hyperparameters (the number of layers and the number of nodes per layer). We’ll explore each of these and determine an optimized configuration of the network for this problem through this exercise. For all of the settings we’ll explore and just, we’ll assume the following default hyperparameters for the model (we’ll use scikit learn’s MLPClassifier as our neural network model):\n\nlearning_rate_init = 0.03\nhidden_layer_sizes = (30,30) (two hidden layers, each with 30 nodes)\nalpha = 0 (regularization penalty)\nsolver = ‘sgd’ (stochastic gradient descent optimizer)\ntol = 1e-5 (this sets the convergence tolerance)\nearly_stopping = False (this prevents early stopping)\nactivation = ‘relu’ (rectified linear unit)\nn_iter_no_change = 1000 (this prevents early stopping)\nbatch_size = 50 (size of the minibatch for stochastic gradient descent)\nmax_iter = 500 (maximum number of epochs, which is how many times each data point will be used, not the number of gradient steps)\n\nThis default setting is our initial guess of what good values may be. Notice there are many model hyperparameters in this list: any of these could potentially be options to search over. We constrain the search to those hyperparameters that are known to have a significant impact on model performance.\n1.1. Visualize the impact of different hyperparameter choices on classifier decision boundaries. Visualize the impact of different hyperparameter settings. Starting with the default settings above make the following changes (only change one hyperparameter at a time). For each hyperparameter value, plot the decision boundary on the training data (you will need to train the model once for each parameter value):\n\nVary the architecture (hidden_layer_sizes) by changing the number of nodes per layer while keeping the number of layers constant at 2: (2,2), (5,5), (30,30). Here (X,X) means a 2-layer network with X nodes in each layer.\nVary the learning rate: 0.0001, 0.01, 1\nVary the regularization: 0, 1, 10\nVary the batch size: 5, 50, 500\n\nThis should produce 12 plots, altogether. For easier comparison, please plot nodes & layers combinations, learning rates, regularization strengths, and batch sizes in four separate rows (with three columns each representing a different value for each of those hyperparameters).\nAs you’re exploring these settings, visit this website, the Neural Network Playground, which will give you the chance to interactively explore the impact of each of these parameters on a similar dataset to the one we use in this exercise. The tool also allows you to adjust the learning rate, batch size, regularization coefficient, and the architecture and to see the resulting decision boundary and learning curves. You can also visualize the model’s hidden node output and its weights, and it allows you to add in transformed features as well. Experiment by adding or removing hidden layers and neurons per layer and vary the hyperparameters.\n1.2. Manual (greedy) hyperparameter tuning I: manually optimize hyperparameters that govern the learning process, one hyperparameter at a time. Now with some insight into which settings may work better than others, let’s more fully explore the performance of these different settings in the context of our validation dataset through a manual optimization process. Holding all else constant (with the default settings mentioned above), vary each of the following parameters as specified below. Train your algorithm on the training data, and evaluate the performance of your trained algorithm on the validation dataset. Here, overall accuracy is a reasonable performance metric since the classes are balanced and we don’t weight one type of error as more important than the other; therefore, use the score method of the MLPClassifier for this. Create plots of accuracy vs each parameter you vary (this will result in three plots).\n\nVary learning rate logarithmically from \\(10^{-5}\\) to \\(10^{0}\\) with 20 steps\nVary the regularization parameter logarithmically from \\(10^{-8}\\) to \\(10^2\\) with 20 steps\nVary the batch size over the following values: \\([1,3,5,10,20,50,100,250,500]\\)\n\nFor each of these cases:\n\nBased on the results, report your optimal choices for each of these hyperparameters and why you selected them.\nSince neural networks can be sensitive to initialization values, you may notice these plots may be a bit noisy. Consider this when selecting the optimal values of the hyperparameters. If the noise seems significant, run the fit and score procedure multiple times (without fixing a random seed) and report the average. Rerunning the algorithm will change the initialization and therefore the output (assuming you do not set a random seed for that algorithm).\nUse the chosen hyperparameter values as the new default settings for 1.3 and 1.4.\n\n1.3. Manual (greedy) hyperparameter tuning II: manually optimize hyperparameters that impact the model architecture. Next, we want to explore the impact of the model architecture on performance and optimize its selection. This means varying two parameters at a time instead of one as above. To do this, evaluate the validation accuracy resulting from training the model using each pair of possible numbers of nodes per layer and number of layers from the lists below. We will assume that for any given configuration the number of nodes in each layer is the same (e.g. (2,2,2), which would be a 3-layer network with 2 hidden node in each layer and (25,25) are valid, but (2,5,3) is not because the number of hidden nodes varies in each layer). Use the manually optimized values for learning rate, regularization, and batch size selected from 1.2.\n\nNumber of nodes per layer: \\([1,2,3,4,5,10,15,25,30]\\)\nNumber of layers = \\([1,2,3,4]\\) Report the accuracy of your model on the validation data. For plotting these results, use heatmaps to plot the data in two dimensions. To make the heatmaps, you can use [this code for creating heatmaps] https://matplotlib.org/stable/gallery/images_contours_and_fields/image_annotated_heatmap.html). Be sure to include the numerical values of accuracy in each grid square as shown in the linked example and label your x, y, and color axes as always. For these numerical values, round them to 2 decimal places (due to some randomness in the training process, any further precision is not typically meaningful).\nWhen you select your optimized parameters, be sure to keep in mind that these values may be sensitive to the data and may offer the potential to have high variance for larger models. Therefore, select the model with the highest accuracy but lowest number of total model weights (all else equal, the simpler model is preferred).\nWhat do the results show? Which parameters did you select and why?\n\n1.4. Manual (greedy) model selection and retraining. Based the optimal choice of hyperparameters, train your model with your optimized hyperparameters on all the training data AND the validation data (this is provided as X_train_plus_val and y_train_plus_val).\n\nApply the trained model to the test data and report the accuracy of your final model on the test data.\nPlot an ROC curve of your performance (plot this with the curve in 1.5 on the same set of axes you use for that question).\n\n1.5. Automated hyperparameter search through random search. The manual (greedy) approach (setting one or two parameters at a time holding the rest constant), provides good insights into how the neural network hyperparameters impacts model fitting for this particular training process. However, it is limited in one very problematic way: it depends heavily on a good “default” setting of the hyperparameters. Those were provided for you in this exercise, but are not generally know. Our manual optimization was somewhat greedy because we picked the hyperparameters one at a time rather than looking at different combinations of hyperparameters. Adopting such a pseudo-greedy approach to that manual optimization also limits our ability to more deeply search the hyperparameter space since we don’t look at simultaneous changes to multiple parameters. Now we’ll use a popular hyperparameter optimization tool to accomplish that: random search.\nRandom search is an excellent example of a hyperparameter optimization search strategy that has been shown to be more efficient (requiring fewer training runs) than another common approach: grid search. Grid search evaluates all possible combinations of hyperparameters from lists of possible hyperparameter settings - a very computationally expensive process. Yet another attractive alternative is Bayesian Optimization, which is an excellent hyperparameter optimization strategy but we will leave that to the interested reader.\nOur particular random search tool will be Scikit-Learn’s RandomizedSearchCV. This performs random search employing cross validation for performance evaluation (we will adjust this to ve a train/validation split).\nUsing RandomizedSearchCV, train on the training data while validating on the validation data (see instructions below on how to setup the train/validation split automatically). This tool will randomly pick combinations of parameter values and test them out, returning the best combination it finds as measured by performance on the validation set. You can use this example as a template for how to do this.\n\nTo make this comparable to the training/validation setup used for the greedy optimization, we need to setup a training and validation split rather than use cross validation. To do this for RandomSearchCV we input the COMBINED training and validation dataset (X_train_plus_val, and y_train_plus_val) and we set the cv parameter to be the train_val_split variable we provided along with the dataset. This will setup the algorithm to make its assessments training just on the training data and evaluation on the validation data. Once RandomSearchCV completes its search, it will fit the model one more time to the combined training and validation data using the optimized parameters as we would want it to. Note: The object returned by running fit (the random search) is NOT the best estimator. You can access the best estimator through the attribute .best_estimator_, assuming that you did not pass refit=False.\nSet the number of iterations to at least 200 (you’ll look at 200 random pairings of possible hyperparameters). You can go as high as you want, but it will take longer the larger the value.\nIf you run this on Colab or any system with multiple cores, set the parameter n_jobs to -1 to use all available cores for more efficient training through parallelization\nYou’ll need to set the range or distribution of the parameters you want to sample from. Search over the same ranges as in previous problems. To tell the algorithm the ranges to search, use lists of values for candidate batch_size, since those need to be integers rather than a range; the loguniform scipy function for setting the range of the learning rate and regularization parameter, and a list of tuples for the hidden_layer_sizes parameter, as you used in the greedy optimization.\nOnce the model is fit, use the best_params_ property of the fit classifier attribute to extract the optimized values of the hyperparameters and report those and compare them to what was selected through the manual, greedy optimization.\n\nFor the final generalization performance assessment:\n\nState the accuracy of the optimized models on the test dataset\nPlot the ROC curve corresponding to your best model on the test dataset through greedy hyperparameter section vs the model identified through random search (these curves should be on the same set of axes for comparison). In the legend of the plot, report the AUC for each curve. This should be one single graph with 3 curves (one for greedy search, one for random search, and one representing random chance). Please also provide AUC score for greedy research and random search.\nPlot the final decision boundary for the greedy and random search-based classifiers along with the test dataset to demonstrate the shape of the final boundary\nHow did the generalization performance compare between the hyperparameters selected through the manual (greedy) search and the random search?",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 4"
    ]
  },
  {
    "objectID": "notebooks/assignment4.html#exercise-2---build-and-test-your-own-neural-network-for-classification",
    "href": "notebooks/assignment4.html#exercise-2---build-and-test-your-own-neural-network-for-classification",
    "title": "Assignment 4",
    "section": "Exercise 2 - Build and test your own Neural Network for classification",
    "text": "Exercise 2 - Build and test your own Neural Network for classification\n[30 points]\nThere is no better way to understand how one of the core techniques of modern machine learning works than to build a simple version of it yourself. In this exercise you will construct and apply your own neural network classifier. You may use numpy if you wish but no other libraries.\n2.1 [10 points] Create a neural network class that follows the scikit-learn classifier convention by implementing fit, predict, and predict_proba methods. Your fit method should run backpropagation on your training data using stochastic gradient descent. Assume the activation function is a sigmoid. Choose your model architecture to have two input nodes, two hidden layers with five nodes each, and one output node.\nTo guide you in the right direction with this problem, please find a skeleton of a neural network class below. You absolutely MAY use additional methods beyond those suggested in this template, but the methods listed below are the minimum required to implement the model cleanly.\nStrategies for debugging. One of the greatest challenges of this implementations is that there are many parts and a bug could be present in any of them. Here are some recommended tips:\n\nDevelopment environment. Consider using an Integrated Development Environment (IDE). I strongly recommend the use of VS Code and the Python debugging tools in that development environment.\nUnit tests. You are strongly encouraged to create unit tests for most modules. Without doing this will make your code extremely difficult to bug. You can create simple examples to feed through the network to validate it is correctly computing activations and node values. Also, if you manually set the weights of the model, you can even calculate backpropagation by hand for some simple examples (admittedly, that unit test would be challenging and is optional, but a unit test is possible).\nCompare against a similar architecture. You can also verify the performance of your overall neural network by comparing it against the scikit-learn implementation and using the same architecture and parameters as your model (your model outputs will certainly not be identical, but they should be somewhat similar for similar parameter settings).\n\n\n\n\n\n\n\nImportant Note\n\n\n\nBuilding a neural net is a valuable learning opportunity, but a time intensive process. Due to the depth of effort this question requires, some students may choose not to complete this section. It’s only worth 10 points, which is not proportional to the time it takes to get it working, and that’s by design. If you choose not to build your own neural network, or if your neural network is not functional prior to submission, then use the scikit-learn implementation instead in the questions below; where it asks to compare to scikit-learn, compare against a random forest classifier instead.\nSimply write “OMITTED” in your response to this question to indicate that you did not write your own neural network.\n\n\n\n# neural network class skeleton code\n\nclass myNeuralNetwork(object):\n    \n    def __init__(self, n_in, n_layer1, n_layer2, n_out, learning_rate=):\n        '''__init__\n        Class constructor: Initialize the parameters of the network including\n        the learning rate, layer sizes, and each of the parameters\n        of the model (weights, placeholders for activations, inputs, \n        deltas for gradients, and weight gradients). This method\n        should also initialize the weights of your model randomly\n            Input:\n                n_in:          number of inputs\n                n_layer1:      number of nodes in layer 1\n                n_layer2:      number of nodes in layer 2\n                n_out:         number of output nodes\n                learning_rate: learning rate for gradient descent\n            Output:\n                none\n        '''\n            \n    def forward_propagation(self, x):\n        '''forward_propagation\n        Takes a vector of your input data (one sample) and feeds\n        it forward through the neural network, calculating activations and\n        layer node values along the way.\n            Input:\n                x: a vector of data representing 1 sample [n_in x 1]\n            Output:\n                y_hat: a vector (or scaler of predictions) [n_out x 1]\n                (typically n_out will be 1 for binary classification)\n        '''\n    \n    def compute_loss(self, X, y):\n        '''compute_loss\n        Computes the current loss/cost function of the neural network\n        based on the weights and the data input into this function.\n        To do so, it runs the X data through the network to generate\n        predictions, then compares it to the target variable y using\n        the cost/loss function\n            Input:\n                X: A matrix of N samples of data [N x n_in]\n                y: Target variable [N x 1]\n            Output:\n                loss: a scalar measure of loss/cost\n        '''\n    \n    def backpropagate(self, x, y):\n        '''backpropagate\n        Backpropagate the error from one sample determining the gradients\n        with respect to each of the weights in the network. The steps for\n        this algorithm are:\n            1. Run a forward pass of the model to get the activations \n               Corresponding to x and get the loss functionof the model \n               predictions compared to the target variable y\n            2. Compute the deltas (see lecture notes) and values of the\n               gradient with respect to each weight in each layer moving\n               backwards through the network\n    \n            Input:\n                x: A vector of 1 samples of data [n_in x 1]\n                y: Target variable [scalar]\n            Output:\n                loss: a scalar measure of th loss/cost associated with x,y\n                      and the current model weights\n        '''\n        \n    def stochastic_gradient_descent_step(self):\n        '''stochastic_gradient_descent_step [OPTIONAL - you may also do this\n        directly in backpropagate]\n        Using the gradient values computed by backpropagate, update each\n        weight value of the model according to the familiar stochastic\n        gradient descent update equation.\n        \n        Input: none\n        Output: none\n        '''\n    \n    def fit(self, X, y, max_epochs=, learning_rate=, get_validation_loss=):\n        '''fit\n            Input:\n                X: A matrix of N samples of data [N x n_in]\n                y: Target variable [N x 1]\n            Output:\n                training_loss:   Vector of training loss values for each epoch\n                validation_loss: Vector of validation loss values for each epoch\n                                 [optional output if get_validation_loss==True]\n        '''\n            \n    def predict_proba(self, X):\n        '''predict_proba\n        Compute the output of the neural network for each sample in X, with the \n        last layer's sigmoid activation providing an estimate of the target \n        output between 0 and 1\n            Input:\n                X: A matrix of N samples of data [N x n_in]\n            Output:\n                y_hat: A vector of class predictions between 0 and 1 [N x 1]\n        '''\n    \n    def predict(self, X, decision_thresh=):\n        '''predict\n        Compute the output of the neural network prediction for \n        each sample in X, with the last layer's sigmoid activation \n        providing an estimate of the target output between 0 and 1, \n        then thresholding that prediction based on decision_thresh\n        to produce a binary class prediction\n            Input:\n                X: A matrix of N samples of data [N x n_in]\n                decision_threshold: threshold for the class confidence score\n                                    of predict_proba for binarizing the output\n            Output:\n                y_hat: A vector of class predictions of either 0 or 1 [N x 1]\n        '''\n    \n    def sigmoid(self, X):\n        '''sigmoid\n        Compute the sigmoid function for each value in matrix X\n            Input:\n                X: A matrix of any size [m x n]\n            Output:\n                X_sigmoid: A matrix [m x n] where each entry corresponds to the\n                           entry of X after applying the sigmoid function\n        '''\n    \n    def sigmoid_derivative(self, X):\n        '''sigmoid_derivative\n        Compute the sigmoid derivative function for each value in matrix X\n            Input:\n                X: A matrix of any size [m x n]\n            Output:\n                X_sigmoid: A matrix [m x n] where each entry corresponds to the\n                           entry of X after applying the sigmoid derivative \n                           function\n        '''\n\n2.2. Apply your neural network.\n\nCreate training, validation, and test datasets using sklearn.datasets.make_moons(N, noise=0.20) data, where \\(N_{train} = 500\\) and \\(N_{test} = 100\\). The validation dataset should be a portion of your training dataset that you hold out for hyperparameter tuning.\nCost function plots. Train and validate your model on this dataset plotting your training and validation cost learning curves on the same set of axes. This is the training and validation error for each epoch of stochastic gradient descent, where an epoch represents having trained on each of the training samples one time.\nTune the learning rate and number of training epochs for your model to improve performance as needed. You’re free to use any methods you deem fit to tune your hyperparameters like grid search, random search, Bayesian optimization etc.\nDecision boundary plots. In two subplots, plot the training data on one subplot and the validation data on the other subplot. On each plot, also plot the decision boundary from your neural network trained on the training data.\nROC Curve plots. Report your performance on the test data with an ROC curve and the corresponding AUC score. Compare against the scikit-learn MLPClassifier trained with the same parameters on the same set of axes and include the chance diagonal. Note: if you chose not to build your own neural network in part (a) above, or if your neural network is not functional prior to submission, then use the scikit-learn MLPClassifier class instead for the neural network and compare it against a random forest classifier instead. Be sure to set the hidden layer sizes, epochs, and learning rate for that model, if so.\nRemember to retrain your model. After selecting your hyperparameters using the validation data set, when evaluating the final performance on the ROC curve, it’s good practice to retrain your model with the selected hyperparameters on the train + validation dataset, before evaluating on the test data.\n\nNote if you opted not to build your own neural network: in this case, for hyperparameter tuning, we recommend using the partial_fit method to train your model for every epoch. Partial fit allows you to incrementally fit on one sample at a time.\n2.3. Suggest two ways in which you neural network implementation could be improved: are there any options we discussed in class that were not included in your implementation that could improve performance?",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 4"
    ]
  },
  {
    "objectID": "notebooks/assignment2.html",
    "href": "notebooks/assignment2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Instructions for all assignments can be found here. Note: this assignment falls under collaboration Mode 2: Individual Assignment – Collaboration Permitted. Please refer to the syllabus for additional information. Please be sure to list the names of any students that you worked with on this assignment. Total points in the assignment add up to 90; an additional 10 points are allocated to professionalism and presentation quality.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "notebooks/assignment2.html#instructions",
    "href": "notebooks/assignment2.html#instructions",
    "title": "Assignment 2",
    "section": "",
    "text": "Instructions for all assignments can be found here. Note: this assignment falls under collaboration Mode 2: Individual Assignment – Collaboration Permitted. Please refer to the syllabus for additional information. Please be sure to list the names of any students that you worked with on this assignment. Total points in the assignment add up to 90; an additional 10 points are allocated to professionalism and presentation quality.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "notebooks/assignment2.html#learning-objectives",
    "href": "notebooks/assignment2.html#learning-objectives",
    "title": "Assignment 2",
    "section": "Learning Objectives:",
    "text": "Learning Objectives:\nBy successfully completing this assignment you will be able to… - Explain the bias-variance tradeoff of supervised machine learning and the impact of model flexibility on algorithm performance - Perform supervised machine learning training and performance evaluation - Implement a k-nearest neighbors machine learning algorithm from scratch in a style similar to that of popular machine learning tools like scikit-learn - Describe how KNN classification works, the method’s reliance on distance measurements, and the impact of higher dimensionality on computational speed - Apply regression (linear regression) and classification (KNN) supervised learning techniques to data and evaluate the performance of those methods - Construct simple feature transformations for improving model fit in linear models - Fit a scikit-learn supervised learning technique to training data and make predictions using it",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "notebooks/assignment2.html#exercise-1---conceptual-questions-on-supervised-learning-i",
    "href": "notebooks/assignment2.html#exercise-1---conceptual-questions-on-supervised-learning-i",
    "title": "Assignment 2",
    "section": "Exercise 1 - Conceptual Questions on Supervised Learning I",
    "text": "Exercise 1 - Conceptual Questions on Supervised Learning I\n[4 points]\nFor each part below, indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.\n1.1. The sample size \\(n\\) is extremely large, and the number of predictors \\(p\\) is small.\n1.2. The number of predictors \\(p\\) is extremely large, and the number of observations \\(n\\) is small.\n1.3. The relationship between the predictors and response is highly non-linear.\n1.4. The variance of the error terms, i.e. \\(\\sigma^2 = Var(\\epsilon)\\), is extremely high.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "notebooks/assignment2.html#exercise-2---conceptual-questions-on-supervised-learning-ii",
    "href": "notebooks/assignment2.html#exercise-2---conceptual-questions-on-supervised-learning-ii",
    "title": "Assignment 2",
    "section": "Exercise 2 - Conceptual Questions on Supervised Learning II",
    "text": "Exercise 2 - Conceptual Questions on Supervised Learning II\n[6 points]\nFor each of the following, (i) explain if each scenario is a classification or regression problem AND why, (ii) indicate whether we are most interested in inference or prediction for that problem AND why, and (iii) provide the sample size \\(n\\) and number of predictors \\(p\\) indicated for each scenario.\n2.1. We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.\n2.2. We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.\n2.3. We are interested in predicting the % change in the US dollar in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the dollar, the % change in the US market, the % change in the British market, and the % change in the German market.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "notebooks/assignment2.html#exercise-3---classification-using-knn",
    "href": "notebooks/assignment2.html#exercise-3---classification-using-knn",
    "title": "Assignment 2",
    "section": "Exercise 3 - Classification using KNN",
    "text": "Exercise 3 - Classification using KNN\n[6 points]\nThe table below provides a training dataset containing six observations (a.k.a. samples) (\\(n=6\\)) each with three predictors (a.k.a. features) (\\(p=3\\)), and one qualitative response variable (a.k.a. target).\nTable 1. Training dataset with \\(n=6\\) observations in \\(p=3\\) dimensions with a categorical response, \\(y\\)\n\n\n\nObs.\n\\(x_1\\)\n\\(x_2\\)\n\\(x_3\\)\n\\(y\\)\n\n\n\n\n1\n0\n3\n0\nRed\n\n\n2\n2\n0\n0\nRed\n\n\n3\n0\n1\n3\nRed\n\n\n4\n0\n1\n2\nBlue\n\n\n5\n-1\n0\n1\nBlue\n\n\n6\n1\n1\n1\nRed\n\n\n\nWe want to use the above training dataset to make a prediction, \\(\\hat{y}\\), for an unlabeled test data observation where \\(x_1=x_2=x_3=0\\) using \\(K\\)-nearest neighbors. You are given some code below to get you started. Note: coding is only required for part (a), for (b)-(d) please provide your reasoning based on your answer to part (a).\n3.1. Compute the Euclidean distance between each observation and the test point, \\(x_1=x_2=x_3=0\\). Present your answer in a table similar in style to Table 1 with observations 1-6 as the row headers.\n3.2. What is our prediction, \\(\\hat{y}\\), when \\(K=1\\) for the test point? Why?\n3.3. What is our prediction, \\(\\hat{y}\\), when \\(K=3\\) for the test point? Why?\n3.4. If the Bayes decision boundary (the optimal decision boundary) in this problem is highly nonlinear, then would we expect the best value of \\(K\\) to be large or small? Why?\n\nimport numpy as np\n\nX = np.array([[ 0, 3, 0],\n              [ 2, 0, 0],\n              [ 0, 1, 3],\n              [ 0, 1, 2],\n              [-1, 0, 1],\n              [ 1, 1, 1]])\ny = np.array(['r','r','r','b','b','r'])",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "notebooks/assignment2.html#exercise-4---build-your-own-classification-algorithm",
    "href": "notebooks/assignment2.html#exercise-4---build-your-own-classification-algorithm",
    "title": "Assignment 2",
    "section": "Exercise 4 - Build your own classification algorithm",
    "text": "Exercise 4 - Build your own classification algorithm\n[18 points]\n\n\n\n\n\n\nNote\n\n\n\nData for this exercise can be downloaded here\n\n\n4.1. Build a working version of a binary KNN classifier using the skeleton code below. We’ll use the sklearn convention that a supervised learning algorithm has the methods fit which trains your algorithm (for KNN that means storing the data) and predict which identifies the K nearest neighbors and determines the most common class among those K neighbors. Note: Most classification algorithms typically also have a method predict_proba which outputs the confidence score of each prediction, but we will explore that in a later assignment. Please use NumPy to implement euclidean distance function.\n4.2. Load the datasets to be evaluated here. Each includes training features (\\(\\mathbf{X}\\)), and test features (\\(\\mathbf{y}\\)) for both a low dimensional dataset (\\(p = 2\\) features/predictors) and a higher dimensional dataset (\\(p = 100\\) features/predictors). For each of these datasets there are \\(n=1000\\) observations of each. They can be found in the data subfolder on github (see link above). Each file is labeled similar to A2_Q4_X_train_low.csv, which lets you know whether the dataset is of features, \\(X\\), targets, \\(y\\); training or testing; and low or high dimensions.\n4.3. Train your classifier on first the low dimensional dataset and then the high dimensional dataset with \\(k=5\\). Evaluate the classification performance on the corresponding test data for each of those trained models. Calculate the time it takes each model to make the predictions and the overall accuracy of those predictions for each corresponding set of test data - state each.\n4.4. Compare your implementation’s accuracy and computation time to the scikit learn KNeighborsClassifier class. How do the results and speed compare to your implementation? Hint: your results should be identical to that of the scikit-learn implementation.\n4.5. Some supervised learning algorithms are more computationally intensive during training than testing. What are the drawbacks of the prediction process being slow? In what cases in practice might slow testing (inference) be more problematic than slow training?\n\n# Skeleton code for part (a) to write your own kNN classifier\n\nclass Knn:\n# k-Nearest Neighbor class object for classification training and testing\n    def __init__(self):\n        \n    def fit(self, x, y):\n        # Save the training data to properties of this class\n        \n    def predict(self, x, k):\n        y_hat = [] # Variable to store the estimated class label for \n        # Calculate the distance from each vector in x to the training data\n        \n        # Return the estimated targets\n        return y_hat\n\n# Metric of overall classification accuracy\n#  (a more general function, sklearn.metrics.accuracy_score, is also available)\ndef accuracy(y,y_hat):\n    nvalues = len(y)\n    accuracy = sum(y == y_hat) / nvalues\n    return accuracy",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "notebooks/assignment2.html#exercise-5---bias-variance-tradeoff-exploring-the-tradeoff-with-a-knn-classifier",
    "href": "notebooks/assignment2.html#exercise-5---bias-variance-tradeoff-exploring-the-tradeoff-with-a-knn-classifier",
    "title": "Assignment 2",
    "section": "Exercise 5 - Bias-variance tradeoff: exploring the tradeoff with a KNN classifier",
    "text": "Exercise 5 - Bias-variance tradeoff: exploring the tradeoff with a KNN classifier\n[20 points]\nThis exercise will illustrate the impact of the bias-variance tradeoff on classifier performance by investigating how model flexibility impacts classifier decision boundaries. For this problem, please us Scikit-learn’s KNN implementation rather than your own implementation, as you did at the end of the last question.\n5.1. Create a synthetic dataset (with both features and targets). Use the make_moons module with the parameter noise=0.35 to generate 1000 random samples.\n5.2. Visualize your data: scatterplot your random samples with each class in a different color.\n5.3. Create 3 different data subsets by selecting 100 of the 1000 data points at random three times (with replacement). For each of these 100-sample datasets, fit three separate k-Nearest Neighbor classifiers with: \\(k = \\{1, 25, 50\\}\\). This will result in 9 combinations (3 datasets, each with 3 trained classifiers).\n5.4. For each combination of dataset and trained classifier plot the decision boundary (similar in style to Figure 2.15 from Introduction to Statistical Learning). This should form a 3-by-3 grid. Each column should represent a different value of \\(k\\) and each row should represent a different dataset.\n5.5. What do you notice about the difference between the decision boundaries in the rows and the columns in your figure? Which decision boundaries appear to best separate the two classes of data with respect to the training data? Which decision boundaries vary the most as the training data change? Which decision boundaries do you anticipate will generalize best to unseen data and why?\n5.6. Explain the bias-variance tradeoff using the example of the plots you made in this exercise and its implications for training supervised machine learning algorithms.\nNotes and tips for plotting decision boundaries (as in part 5.4): - Resource for plotting decision boundaries with meshgrid and contour: https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html - If you would like to change the colors of the background, and do not like any of the existing cmap available in matplotlib, you can make your own cmap using the 2 sets of rgb values. Sample code (replace r, g, b with respective rgb values):\n\nfrom matplotlib.colors import LinearSegmentedColormap\nnewcmp = LinearSegmentedColormap.from_list(\"new\", [(r/255, g/255, b/255), (r/255, g/255, b/255)], N=2)",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "notebooks/assignment2.html#exercise-6---bias-variance-trade-off-ii-quantifying-the-tradeoff",
    "href": "notebooks/assignment2.html#exercise-6---bias-variance-trade-off-ii-quantifying-the-tradeoff",
    "title": "Assignment 2",
    "section": "Exercise 6 - Bias-variance trade-off II: Quantifying the tradeoff",
    "text": "Exercise 6 - Bias-variance trade-off II: Quantifying the tradeoff\n[18 points]\nThis exercise explores the impact of the bias-variance tradeoff on classifier performance by looking at the performance on both training and test data.\nHere, the value of \\(k\\) determines how flexible our model is.\n6.1. Using the function created earlier to generate random samples (using the make_moons function setting the noise parameter to 0.35), create a new set of 1000 random samples, and call this dataset your test set and the previously created dataset your training set.\n6.2. Train a kNN classifier on your training set for \\(k = 1,2,...500\\). Apply each of these trained classifiers to both your training dataset and your test dataset and plot the classification error (fraction of incorrect predictions).\n6.3. What trend do you see in the results?\n6.4. What values of \\(k\\) represent high bias and which represent high variance?\n6.5. What is the optimal value of \\(k\\) and why?\n6.6. In KNN classifiers, the value of k controls the flexibility of the model - what controls the flexibility of other models?",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "notebooks/assignment2.html#exercise-7---linear-regression-and-nonlinear-transformations",
    "href": "notebooks/assignment2.html#exercise-7---linear-regression-and-nonlinear-transformations",
    "title": "Assignment 2",
    "section": "Exercise 7 - Linear regression and nonlinear transformations",
    "text": "Exercise 7 - Linear regression and nonlinear transformations\n[18 points]\n\n\n\n\n\n\nNote\n\n\n\nData for this exercise can be downloaded here\n\n\nLinear regression can be used to model nonlinear relationships when feature variables are properly transformed to represent the nonlinearities in the data folder. In this exercise, you’re given training and test data contained in files “A2_Q7_train.csv” and “A2_Q7_test.csv” in the data. Your goal is to develop a regression algorithm from the training data that performs well on the test data.\nHint: Use the scikit learn LinearRegression module.\n7.1. Create a scatter plot of your training data.\n7.2. Estimate a linear regression model (\\(y = a_0 + a_1 x\\)) for the training data and calculate both the \\(R^2\\) value and mean square error for the fit of that model for the training data. Also provide the equation representing the estimated model (e.g. \\(y = a_0 + a_1 x\\), but with the estimated coefficients inserted. Consider this your baseline model against which you will compare other model options. Evaluating performance on the training data is not a measure of how well this model would generalize to unseen data. We will evaluate performance on the test data once we see our models fit the training data decently well.\n7.3. If features can be nonlinearly transformed, a linear model may incorporate those non-linear feature transformation relationships in the training process. From looking at the scatter plot of the training data, choose a transformation of the predictor variable, \\(x\\) that may make sense for these data. This will be a multiple regression model of the form \\(y = a_0 + a_1 z_1 + a_2 z_2 + \\ldots + a_n z_n\\). Here \\(z_i\\) could be any transformations of x - perhaps it’s \\(\\frac{1}{x}\\), \\(log(x)\\), \\(sin(x)\\), \\(x^k\\) (where \\(k\\) is any power of your choosing). Provide the estimated equation for this multiple regression model (e.g. if you chose your predictors to be \\(z_1 = x\\) and \\(z_2 = log(x)\\), your model would be of the form \\(y = a_0 + a_1 x + a_2 log(x)\\). Also provide the \\(R^2\\) and mean square error of the fit for the training data.\n7.4. Visualize the model fit to the training data. Using both of the models you created in parts (b) and (c), plot the original data (as a scatter plot) AND the curves representing your models (each as a separate curve) from (b) and (c).\n7.5. Now its time to compare your models and evaluate the generalization performance on held out test data. Using the models above from (b) an (c), apply them to the test data and estimate the \\(R^2\\) and mean square error of the test dataset.\n7.6. Which models perform better on the training data, and which on the test data? Why?\n7.7. Imagine that the test data were significantly different from the training dataset. How might this affect the predictive capability of your model? How would the accuracy of generalization performance be impacted? Why?\nTo help get you started - here’s some code to help you load in the data for this exercise (you’ll just need to update the path):\n\nimport numpy as np\nimport pandas as pd\n\npath = './data/a2'\ntrain = pd.read_csv(path + 'A2_Q7_train.csv')\ntest = pd.read_csv(path + 'A2_Q7_test.csv')\n\nx_train = train.x.values\ny_train = train.y.values\n\nx_test = test.x.values\ny_test = test.y.values",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "tools.html",
    "href": "tools.html",
    "title": "Software and Hardware Tools",
    "section": "",
    "text": "We will use Python 3.x. The Anaconda distribution is recommended and comes with the most common packages. Python continues to be an one of the top programming languages and the rich packages in the language make it an excellent choice for machine learning. In particular the Python ecosystem of packages makes it a natural choice for ML including core numerical programming and plotting libraries like numpy, scipy, matplotlib, and pandas as well as excellent packages for machine learning algorithm development and statistical modeling including Scikit-Learn, Keras, and Pytorch.",
    "crumbs": [
      "Schedule",
      "Resources",
      "Software and Hardware Tools"
    ]
  },
  {
    "objectID": "tools.html#programming-language-python",
    "href": "tools.html#programming-language-python",
    "title": "Software and Hardware Tools",
    "section": "",
    "text": "We will use Python 3.x. The Anaconda distribution is recommended and comes with the most common packages. Python continues to be an one of the top programming languages and the rich packages in the language make it an excellent choice for machine learning. In particular the Python ecosystem of packages makes it a natural choice for ML including core numerical programming and plotting libraries like numpy, scipy, matplotlib, and pandas as well as excellent packages for machine learning algorithm development and statistical modeling including Scikit-Learn, Keras, and Pytorch.",
    "crumbs": [
      "Schedule",
      "Resources",
      "Software and Hardware Tools"
    ]
  },
  {
    "objectID": "tools.html#development-environments-vs-code-and-jupyter-notebooks",
    "href": "tools.html#development-environments-vs-code-and-jupyter-notebooks",
    "title": "Software and Hardware Tools",
    "section": "Development environments: VS Code and Jupyter Notebooks",
    "text": "Development environments: VS Code and Jupyter Notebooks\nJupyter lab or Jupyter notebook will be appropriate for most class assignments. We highly encourage you to use Visual Studio Code, in particular due to the debugging capabilities. There are many configurations that may work for you, but I would recommend begin by gathering ideas in Jupyter Notebooks. Once you have the basic structure of your code worked out, consider moving it to a .py file to make it easier and cleaner to run and build on.\nIf you could use help getting started or a refresher on Jupyter notebooks, check out this video for more on basic Jupyter functionality. Using Jupyter notebooks allows you to practice applying machine learning concepts while building programming and writing skills, strengthening your ability to both create creative solutions to machine learning challenges while simultaneously enhancing your ability to communicate the meaning behind your findings and why others should give credence to your results. You’re encouraged to use VS Code to interact with Jupyter Notebooks.",
    "crumbs": [
      "Schedule",
      "Resources",
      "Software and Hardware Tools"
    ]
  },
  {
    "objectID": "tools.html#graphics-processing-units-gpus",
    "href": "tools.html#graphics-processing-units-gpus",
    "title": "Software and Hardware Tools",
    "section": "Graphics processing units (GPUs)",
    "text": "Graphics processing units (GPUs)\nGPUs are the workhorses of many modern machine learning algorithms, especially any that involve neural network-based architectures. There will be a small number of assignments that will require additional computation that would benefit from GPUs. For these there are several options:\n\nDuke Compute Cluster. This server provides on-demand access to GPUs for computation through a centralized Duke server. You have access to this resource for the semester. Further instructions are available here.\nGoogle Colab, which is a free notebook environment that enables access to cloud resources including GPUs. For longer sessions before timeouts, greater RAM, and better GPUs you can optionally upgrade to Colab Pro.\nWe will also be making a limited number of Azure cloud credits available to students later in the semester by request if neither of the above resources meets your needs.",
    "crumbs": [
      "Schedule",
      "Resources",
      "Software and Hardware Tools"
    ]
  },
  {
    "objectID": "tools.html#version-control-via-git",
    "href": "tools.html#version-control-via-git",
    "title": "Software and Hardware Tools",
    "section": "Version Control via Git",
    "text": "Version Control via Git\nGit is efficient for collaboration, and expectation in industry, and one of the best ways to share results in academia. You can even use some Git repositories (e.g. Github) as hosts for website, such as with the course website. As a data scientist with experience in machine learning, Git is expected. We will interact with Git repositories (a.k.a. repos) throughout this course, and your project will require the use of git repos for collaboration.\nComplete the Atlassian Git tutorial, specifically the following listed sections. Try each concept that’s presented. For this tutorial, instead of using BitBucket as your remote repository host, you may use your preferred platform such as Github or Duke’s Gitlab. 1. What is version control 2. What is Git 3. Install Git 4. Setting up a repository 5. Saving changes 6. Inspecting a repository 7. Undoing changes 8. Rewriting history 9. Syncing 10. Making a pull request 11. Using branches 12. Comparing workflows\nI also have created two videos on the topic to help you understand some of these concepts: Git basics and a step-by-step tutorial.\nAs an additional resource, Microsoft now offers a git tutorial on this topic as well.\nFor your answer, affirm that you either completed the tutorials above OR have previous experience with ALL of the concepts above. Confirm this by typing your name below and selecting the situation that applies from the two options in brackets.",
    "crumbs": [
      "Schedule",
      "Resources",
      "Software and Hardware Tools"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The schedule below is a guide to what we will be covering throughout the semester and is subject to change to meet the learning goals of the class. Check this website regularly for the latest schedule and for course materials that will be posted here through links on the syllabus.\n\n\n\n\n\n\nKey to books used below\n\n\n\n\nISL = An Introduction to Statistical Learning with Python, by James, Witten, Hastie, and Tibshirani\nUDL = Understanding Deep learning by Simon Prince\nDM = Introduction to Data Mining, by Tan, Steinbach, Karpatne, and Kumar\nPRML = Pattern Recognition and Machine Learning, by Bishop\nDL = Deep Learning, by Goodfellow, Bengio, and Courville\nRL = Reinforcement Learning: An Introduction: An Introduction, by Sutton and Barto\n\n\n\nva\n    H:\n  \n    \n      Event Type\n      Date\n      Description\n      Readings\n      Course Materials\n    \n  \n\n  \n    Lecture 1\n    Thursday Jan 9\n    \n      What is machine learning? \n      Course overview and an orientation to the major branches of machine learning: supervised, unsupervised, and reinforcement learning \n    \n    None\n    \n      \n      [slides]\n    \n  \n\n  \n    \n    \n    Module 1: Supervised Learning\n    \n    \n  \n\n  \n    \n    Tuesday Jan 14\n    \n      NO CLASS \n      Make up on Friday 1/17\n    \n    \n    \n  \n\n  \n    Lecture 2\n    Thursday  Jan 16\n    \n      An end-to-end machine learning example \n      An introduction to formulating a supervised machine learning problem. Stating the problem, creating the model, evaluating performance, and operationalizing the solution. \n    ISL Ch. 1 + 2.1Watch this lecture \n    \n      \n      [slides]\n      [sample code]\n    \n  \n\n  \n    Lecture 3\n    Friday  Jan 17\n    \n      How flexible should my algorithms be? The bias-variance tradeoff  \n      The bias-variance tradeoff explained using K-nearest neighbors classification\n    \n    ISL 2.2\n    \n      \n      [slides]\n    \n  \n\n  \n    \n    Monday  Jan 20\n    Martin Luther King Jr. Day\n    \n    \n  \n\n  \n    Lecture 4\n    Tuesday  Jan 21\n    \n      Linear Models I \n      Simple linear regression, multiple linear regression, measuring error, model fitting and least squares, comparing linear regression and classification\n    \n    ISL Intro of 3, 3.1, and 3.2\n    \n      \n      [slides]\n    \n  \n\n  Deliverable\n     Wednesday  Jan 22\n    Assignment #1 Due (at 9pm) Probability, Linear Algebra, & Computational Programming\n    \n    \n      [assignment]\n      \n      [submit]\n    \n  \n\n  \n    Lecture 5\n    Thursday  Jan 23\n    \n      Linear Models II \n      Nonlinear transformations of predictors; linear models for classification including the perceptron and logistic regression; cost/loss functions for classification (cross entropy loss); introduction to gradient descent.\n    \n    ISL 3.3 and 3.5\n    \n      \n      [slides]\n    \n  \n  \n  \n    Lecture 6\n    Tuesday  Jan 28\n    \n      Performance evaluation: metrics for regression/classification \n      Choosing the right model: accuracy vs speed vs interpretability; metrics for supervised learning performance evaluation: types of errors, receiver operating characteristics curves, and confusion matrices\n    \n    ISL 4.1, 4.2, and 4.3\n    \n      \n      [slides]\n    \n  \n\n  \n    Lecture 7\n    Thursday  January 30\n    \n      Experimental designs for evaluating generalization performance and model comparison \n      How to construct effective experimental designs for evaluating and comparing models; using model performance metrics to measure generalization performance; resampling techniques: training, testing, and validation datasets and cross validation; common pitfalls around biased sampling and data snooping/leakage\n    \n    ISL 5.1 and 5.2\n    \n      \n      [slides]\n    \n  \n\n  \n    Lecture 8\n    Tuesday  Feb 4\n    \n      Decision theory \n      A risk-based framework for determining to operate supervised learning algorithms in practice; choosing ROC operating points through risk-minimization and how application-specific costs associated with different types of errors can be used to determine optimal operating points for classifiers\n    \n    \n      \n      Link to reading\n    \n    \n      \n      [slides]\n    \n  \n\n  \n    Deliverable\n     WednesdayFeb 5\n    Assignment #2 Due (at 9pm)Supervised Machine Learning Fundamentals\n    \n    \n      \n      [assignment]\n      \n      [submit]\n    \n  \n\n    \n    Lecture 9\n    Thursday  Feb 6\n    \n      Reducing overfit \n      Feature selection; Occam’s razor; Subset selection; L1 (ridge), L2 (LASSO), and elastic net regularization; early stopping.\n    \n    ISL 6.1 and 6.2\n    \n      \n      [slides]\n    \n  \n\n  \n    Lecture 10\n    Tuesday  Feb 11\n    \n      Generative models for classification \n      Generative vs discriminative models; naïve Bayes\n    \n    ISL 4.4 and 4.5\n    \n      \n      [slides]\n    \n  \n\n  \n    Lecture 11\n    Thursday  Feb 13\n    \n      Tree-based models and ensembles \n      From decision trees to random forests: bagging, bootstrapping, and boosting\n    \n    ISL 8.1 and 8.2\n    \n      \n      [slides]\n    \n  \n\n    \n    Lecture 12\n    Tuesday  Feb 18\n    \n      Neural networks I \n      Introduction to neural networks and representation learning; forward propagation, network architecture, and how to adapt to regression or classification problems\n    \n    UDL Ch 3: 3.1, 3.2; PRML Ch 5: 5.1 \n    \n      \n      [slides]\n    \n  \n\n  \n    Lecture 13\n    Thursday  Feb 20\n    \n      Neural networks II \n      Fitting a neural network to training data through gradient descent and backpropagation; how backpropagation is used to compute gradients in neural networks; hyperparameters and architecture choices in neural networks and practices for training neural networks successfully\n    \n    UDL Ch 6: 6.1-6.2.2; PRML Ch 5: 5.3.1, and Calculus on Computational Graphs\n    \n      \n      [slides]\n    \n  \n\n  \n    Deliverable\n     Friday  Feb 19 Feb 21\n    Assignment #3 Due (at 9pm)Supervised learning model training and evaluation\n    \n    \n      \n      [assignment]\n      \n      [submit]\n    \n  \n\n  \n    Lecture 14\n    Tuesday  Feb 25\n    \n      Introduction to Deep learning I \n      Common architectures of deep learning models and the tools used to implement them. Introduction to convolutional neural networks (CNNs) and neural networks for gridded data (e.g. imagery).\n    \n    UDL Ch 10: Convolutional Neural Networks;DL Ch 11: Practical Methodology\n    \n      \n      [slides]\n    \n  \n\n  \n    Lecture 15\n    Thursday  Feb 27\n    \n      Introduction to Deep learning II \n      Common architectures of deep learning models and the tools used to implement them.\n    \n    \n    DL Ch 11: Practical Methodology;UDL Ch 12.1-12.5: Transformers;[Video: Transformers];[Video: Self-attention]\n    \n      \n      [slides]\n    \n  \n\n  \n    \n    \n    Module 2: Unsupervised Learning\n    \n    \n  \n\n  \n    Lecture 16\n    TuesdayMar 4\n    \n      Dimensionality reduction \n      The Curse of Dimensionality and intro to principal components analysis (PCA)\n    \n    ISL 6.3, 6.4, 12.1, and 12.2 \n    \n      \n      [slides]\n    \n  \n\n  \n    Lecture 17\n    ThursdayMar 6\n    \n      Principal components analysis (PCA) \n      Explaining how PCA works and how we calculate the principal components.\n    \n    ISL 12.4NO QUIZ\n    \n      \n      [slides]\n    \n  \n\n\n    Deliverable\n    Friday  Mar 6 Mar 7\n    Assignment #4 Due (at 9pm)Neural Networks\n    \n    \n      \n      \n      [assignment]\n      [submit]\n      [NN Math Guide]\n    \n  \n\n  \n    \n    Mar 8-16\n    Spring Break Week\n    \n    \n  \n\n  \n    Lecture 18\n    Tuesday  Mar 18\n    \n      Density Estimation and Clustering \n      Approaches for density estimation from histograms to Gaussian mixture models (for density estimation and clustering)\n    \n    DM Ch 7 (link): Intro, 7.1 and 7.2NO QUIZ\n    \n      \n      [slides]\n    \n  \n\n  \n    Lecture 19\n    Thursday  Mar 20\n    \n      Clustering \n      Hierarchical clustering, DBSCAN, and spectral clustering\n    \n   DM Ch 7 (link): 7.3 and 7.4\n   \n     \n      [slides]\n    \n  \n\n  \n    Deliverable\n     Friday  Mar 21 \n    Project Proposal Due (at 9pm)\n    \n    \n      [project]\n      [submit]\n    \n  \n\n  \n    \n    \n    Module 3: Reinforcement Learning\n    \n    \n  \n\n  \n    Lecture 20\n    Tuesday  Mar 25\n    \n      Reinforcement Learning I \n      Formulating the reinforcement learning problem\n    \n    RL Ch 1: 1.1-1.6; Ch 2: 2.1-2.5\n    \n      [slides]\n      \n    \n  \n\n  \n    Deliverable\n    Wednesday  Mar 26\n    Assignment #5 Due (at 9pm)  Kaggle Competition and Unsupervised LearningKaggle Competition Ends 9pm on Tuesday Mar 25\n    \n    \n      [assignment] \n      \n      [submit]\n      \n    \n  \n\n  \n    Lecture 21\n    Thursday  Mar 27\n    \n      Reinforcement Learning II \n      Policy and value functions, rewards, and introduction to Markov processes \n    \n    RL Ch 3\n    \n      [slides]\n      \n    \n  \n\n  \n    Lecture 22\n    Tuesday Apr 1\n    \n      Reinforcement Learning III \n      From Markov Chains to Markov Decision Processes (MDPs)\n    \n    RL Ch 4\n    \n      [slides]\n      \n    \n  \n\n  \n    Lecture 23\n    Thursday  Apr 3\n    \n      Reinforcement Learning IV \n      Finding optimal policies through policy iteration, value iteration, and Monte Carlo methods\n    \n    RL Ch 5: 5.1-5.3\n    \n      [slides]\n      \n    \n  \n\n  \n    \n    \n    Module 4: Practical Considerations and Advanced Topics\n    \n    \n  \n\n  \n    Lecture 24\n    Tuesday  Apr 8\n    \n      Practical Considerations and Advanced Topics I \n      A survey of practical considerations and advanced topics\n    \n    None\n    \n      [slides]\n      \n    \n  \n\n  \n    Lecture 25\n    Thursday  Apr 10\n    \n      Practical Considerations and Advanced Topics II \n      A survey of practical considerations and advanced topics\n    \n    None\n    \n      [slides]\n      \n    \n  \n\n  \n    Lecture 26\n    Tuesday  Apr 15\n    \n      Practical Considerations and Advanced Topics III \n      A survey of practical considerations and advanced topics\n    \n    None\n    \n      [slides]\n      \n    \n  \n\n  \n    Deliverable\n    Wednesday  Apr 16 \n    Draft Final Project Report Due (at 9pm)\n    \n    \n      [project] \n      [submit]\n      \n      \n    \n  \n\n  \n    Deliverable\n    Wednesday  Apr 16\n    (Optional) Assignment #6 Due (at 9pm)Reinforcement learning\n    \n    \n      [assignment]\n      \n      \n    \n  \n\n  \n    Deliverable\n    Wednesday  Apr 30  9am-noon\n    \n      Final project showcase \n      Meets during the final exam period Due on this day:\n      \n        Final Project Report (due by 9am on GradeScope)\n        Final Project Presentation (due by 9am; submit link to presentation here)\n        Project Github Repository (due by 9am on GradeScope)\n        Final Project Peer Evaluation (due by 9pm; submit via TEAMMATES, see email)\n      \n    \n    \n    \n      [project]",
    "crumbs": [
      "Schedule",
      "Syllabus",
      "Schedule"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources for further learning",
    "section": "",
    "text": "The following online courses on Coursera are a 5-part series on Python programming by Nick Eubank, Kyle Bradbury, Andrew Hilton, and Genevieve Lipp:\n\nPython Programming Fundamentals\nData Science with NumPy, Sets, and Dictionaries\nPandas for Data Science\nDesigning Larger Python Programs for Data Science\nData Visualization and Modeling in Python\n\nThere is also a textbook version of much of this material by Nick Eubank and Kyle Bradbury",
    "crumbs": [
      "Schedule",
      "Resources",
      "Resources for further learning"
    ]
  },
  {
    "objectID": "resources.html#python-programming",
    "href": "resources.html#python-programming",
    "title": "Resources for further learning",
    "section": "",
    "text": "The following online courses on Coursera are a 5-part series on Python programming by Nick Eubank, Kyle Bradbury, Andrew Hilton, and Genevieve Lipp:\n\nPython Programming Fundamentals\nData Science with NumPy, Sets, and Dictionaries\nPandas for Data Science\nDesigning Larger Python Programs for Data Science\nData Visualization and Modeling in Python\n\nThere is also a textbook version of much of this material by Nick Eubank and Kyle Bradbury",
    "crumbs": [
      "Schedule",
      "Resources",
      "Resources for further learning"
    ]
  },
  {
    "objectID": "resources.html#math-for-machine-learning-calculus-and-linear-algebra",
    "href": "resources.html#math-for-machine-learning-calculus-and-linear-algebra",
    "title": "Resources for further learning",
    "section": "Math for machine learning (calculus and linear algebra)",
    "text": "Math for machine learning (calculus and linear algebra)\n\nMathematics for Machine Learning by Deisenroth, Faisal, and Ong\nDeep Learning; Part I: Applied Math and Machine Learning Basics by Goodfellow, Bengio, and Courville\nThe Matrix Calculus You Need For Deep Learning by Parr and Howard\nDive Into Deep Learning; Appendix: Mathematics for Deep Learning by Weness, Hu, et al.",
    "crumbs": [
      "Schedule",
      "Resources",
      "Resources for further learning"
    ]
  },
  {
    "objectID": "resources.html#introductory-books-for-machine-learning",
    "href": "resources.html#introductory-books-for-machine-learning",
    "title": "Resources for further learning",
    "section": "Introductory books for machine learning",
    "text": "Introductory books for machine learning\nEach of these are used in one or more course reading assignment in this course:\n\nAn Introduction to Statistical Learning with Python, by James, Witten, Hastie, and Tibshirani\nUnderstanding Deep learning by Simon Prince\nIntroduction to Data Mining, by Tan, Steinbach, Karpatne, and Kumar\nPattern Recognition and Machine Learning, by Bishop\nDeep Learning, by Goodfellow, Bengio, and Courville\nReinforcement Learning: An Introduction: An Introduction, by Sutton and Barto",
    "crumbs": [
      "Schedule",
      "Resources",
      "Resources for further learning"
    ]
  },
  {
    "objectID": "pedagogy.html",
    "href": "pedagogy.html",
    "title": "Pedagogy",
    "section": "",
    "text": "Tenet #1: Good learning is active learning\nEveryone who was good at something was once bad at it. Learning comes from practice. No amount of reading or video/lecture watching alone will help you to become good without actively engaging with the material through practice. That is why this entire course is focused on supporting you to actively apply machine learning techniques through the assignments, quizzes, and project.\n\n\nTenet #2: Desirable difficulty leads to meaningful learning\nLearning is most effective when there’s a degree of struggle with the material. “Requiring students to organize new information and to work harder in the initial learning period can lead to greater and deeper learning. Although this struggle, dubbed a desirable difficulty…may at first be frustrating to learner and teacher alike, ultimately it improves long-term retention” (Excerpt from A Concise Guide to Improving Student Learning: Six Evidence-Based Principles and How to Apply Them). Desirable difficulties help you build connections between concepts and learn representations of knowledge (meta-cognition) that, like an index of a book, will increase your ability to creatively connect concepts and think more deeply about the topic.\n\n\nTenet #3: Read, reflect, recall is a pattern for effective learning\nSpaced retrieval and reflection is a key to effective learning. When we learn something, if we don’t use it, the knowledge fades. However, if we return to the material, apply it, create with it, we’re increasing the probability of long-term learning. This is why you will interact with each concept typically 4 times: lectures, readings, quizzes, and assignments, and at least one more time for those concepts involved in the final project. An added benefit of the frequent reflection through quizzes is that it tests your knowledge regularly, helping us to avoid the illusion of knowledge (thinking we know something, when we actually do not).\nReference\nBrown, P.C., Roediger III, H.L. and McDaniel, M.A., 2014. Make it stick: The science of successful learning. Harvard University Press.\nPersellin, D.C. and Daniels, M.B., 2023. A concise guide to improving student learning: Six evidence-based principles and how to apply them. Taylor & Francis.",
    "crumbs": [
      "Schedule",
      "Syllabus",
      "Pedagogy"
    ]
  },
  {
    "objectID": "contacts.html",
    "href": "contacts.html",
    "title": "Instructional Team",
    "section": "",
    "text": "Contact Us & Office Hours\n\n\n\nReach out to us through Ed Discussions for any questions. Office hours are also posted on Ed.\n\n\n\nInstructor\n\n\n\nKyle Bradbury (, )\n\n\nDr. Kyle Bradbury develops and applies machine learning techniques to better understand and manage energy and climate resources. His work focuses on how to advance machine learning methodologies and apply them to solve energy and climate system challenges through the development of open source, widely-applicable computational tools. His current research interests include developing scalable computer vision techniques for assessing energy resources, infrastructure, and access globally through the use of publicly available remote sensing data. Methodologically, he investigates how to overcome the challenge of distribution shift in computer vision through novel training paradigms that require less labeled data.\n\n\nTeaching Assistants\n\n\n\n\n\n\n\n\n\nKian Bagherlee (, )\n\n\n\n\n\n\n\nAarya Desai (, )\n\n\n\n\n\n\n\nDhaval Potdar (, )\n\n\n\n\n\n\n\nRakeen Rouf (, )",
    "crumbs": [
      "Schedule",
      "Syllabus",
      "Instructional Team"
    ]
  },
  {
    "objectID": "ai.html",
    "href": "ai.html",
    "title": "Effective Use of AI",
    "section": "",
    "text": "This class is all about artificial intelligence, however, the primary intelligence that we are working to expand is your own. AI tools, like ChatGPT, can be useful aids for learning, but without considering your relationship with these tools, they can present several major challenges to your learning:\n\nThe Illusion of Knowledge. Without careful use, you can easily succomb to the cognitive bias known as the illusion of knowledge. The illusion of knowledge, also known as the Dunning-Kruger effect, occurs when you believe you know something to a greater degree than you actually do. It’s better to evaluate yourself accurately in your knowledge of a subject so that you can seek out resources for filling the gaps in your knowledge. This can lead to poor decision making and overconfidence in those poor decisions. It also prevents further learning.\nPreventing actual learning. Using an AI to avoid the cognitive struggle of thinking through the material literally prevents you from learning since those cognitive struggles are part of the learning process. If your goal is to learn in this course, over-reliance on AI is a clear path to\nReduced potential for creativity and innovation. Getting too used to the habit of relying on AI weakens your critical thinking abilities and creativity. When you depend on AI to generate ideas or solve problems, you miss out on the opportunity to develop your own innovative solutions and original thoughts. This can stifle your creative growth and limit your ability to think independently.\nGetting it wrong. AI is fallible. As we see in this course, even the best trained systems aren’t 100% correct, so without the proper knowledge to evaluate AI output, over-reliance on AI sets you up for getting it wrong.",
    "crumbs": [
      "Schedule",
      "Syllabus",
      "Effective Use of AI"
    ]
  },
  {
    "objectID": "ai.html#the-challenges-of-ai-use",
    "href": "ai.html#the-challenges-of-ai-use",
    "title": "Effective Use of AI",
    "section": "",
    "text": "This class is all about artificial intelligence, however, the primary intelligence that we are working to expand is your own. AI tools, like ChatGPT, can be useful aids for learning, but without considering your relationship with these tools, they can present several major challenges to your learning:\n\nThe Illusion of Knowledge. Without careful use, you can easily succomb to the cognitive bias known as the illusion of knowledge. The illusion of knowledge, also known as the Dunning-Kruger effect, occurs when you believe you know something to a greater degree than you actually do. It’s better to evaluate yourself accurately in your knowledge of a subject so that you can seek out resources for filling the gaps in your knowledge. This can lead to poor decision making and overconfidence in those poor decisions. It also prevents further learning.\nPreventing actual learning. Using an AI to avoid the cognitive struggle of thinking through the material literally prevents you from learning since those cognitive struggles are part of the learning process. If your goal is to learn in this course, over-reliance on AI is a clear path to\nReduced potential for creativity and innovation. Getting too used to the habit of relying on AI weakens your critical thinking abilities and creativity. When you depend on AI to generate ideas or solve problems, you miss out on the opportunity to develop your own innovative solutions and original thoughts. This can stifle your creative growth and limit your ability to think independently.\nGetting it wrong. AI is fallible. As we see in this course, even the best trained systems aren’t 100% correct, so without the proper knowledge to evaluate AI output, over-reliance on AI sets you up for getting it wrong.",
    "crumbs": [
      "Schedule",
      "Syllabus",
      "Effective Use of AI"
    ]
  },
  {
    "objectID": "ai.html#responsible-use-of-ai-tools",
    "href": "ai.html#responsible-use-of-ai-tools",
    "title": "Effective Use of AI",
    "section": "Responsible use of AI tools",
    "text": "Responsible use of AI tools\nWith those challenges outlined, there are several ways in which AI can be helpful:\n\nA learning aid. If you’re not familiar with a concept or need help understanding something and you don’t have easy access to another source of guidance, an AI tool (especially those that provide relevant citations) can be helpful for accelerating the learning process. Don’t let the AI tool do a task for your, but allow the tool to help you connect with the knowledge and resources you need to complete the task and develop new skills (always search for high-quality resources before relying on AI directly).\nA coding aid. For coding, consider AI to be a pair programmer who may have some suggestions to enhance your coding or may be able to provide some templates to work from. DO NOT directly copy and paste code from AI tools. You should fully understand and be capable of reproducing any code segment that you use - if not, you have not learned the material and that is considered plagiarism.\nA brainstorming aid. AI can help you to think through and generate ideas. However, always begin creating ideas on your own first, and use AI after you’ve done some critical thinking on your own to prevent your own creativity and problem solving skills from atrophying.\nA tool for detecting spelling and grammar issues (e.g., Grammarly)\n\nYou should NEVER directly submit work that was created by AI. In this class, the use of AI tools is acceptable WITH CITATION. An appropriate citation would indicate the following information:\n\nWhat AI tool you used (e.g. ChatGPT)\nHow you used it\n\nAt the end of the day you are responsible for the correctness of anything you submit, you are responsible for having a complete understanding and the ability to reproduce from scratch anything that you submit.",
    "crumbs": [
      "Schedule",
      "Syllabus",
      "Effective Use of AI"
    ]
  },
  {
    "objectID": "index.html#course-summary",
    "href": "index.html#course-summary",
    "title": "Overview",
    "section": "Course Summary",
    "text": "Course Summary\nIn almost every field, there is a need to make predictions based on data to drive decisions. The goal of this course is to provide an introduction to machine learning that is approachable to diverse disciplines and empowers students to become proficient in the foundational concepts and tools. You will learn to (a) structure a machine learning problems and determine which algorithmic tools are appropriate, (b) evaluate the performance of your solution using field-appropriate metrics and practices, and (c) accurately interpret your model output and communicate your results to interdisciplinary audiences. This course is a fast-paced, applied introduction to machine learning that through extensive practice with foundational tools, helps you to develop your knowledge of foundational machine learning concepts, and provides practical experience with those tools to prepare you for practice or future study.",
    "crumbs": [
      "Schedule",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#detailed-description",
    "href": "index.html#detailed-description",
    "title": "Overview",
    "section": "Detailed description",
    "text": "Detailed description\nMachine learning is a collection of useful tools for understanding and making decisions based on data and past experience; it is not a hammer to be applied to every nail, but rather a precision tool to be used when needed. This course will begin with exploring the purpose of machine learning told through a discussion of the types of problems that machine learning can answer: describing, predicting, and strategizing based on data and the tools at our disposal to address these challenges: supervised learning including classification and regression; unsupervised learning including clustering and density estimation; and reinforcement learning. There will be a strong focus on how to formulate a machine learning problem. Central to that formulation will be developing an understanding of how to preprocess data for analysis (e.g. feature extraction/dimensionality reduction, training/validation data sampling), model selection, and performance evaluation with cross validation. The final topic of this course will be a brief overview of state-of-the-art machine learning techniques that are emerging in the field.\nThroughout this course, the focus will be on applying algorithms rather than diving deeply into theory. You will be asked to consider the practical issues of machine learning problem solving: challenges of applying machine learning code packages, striving for parsimony (simplicity of models) and interpretability, and ensuring model assumptions are valid for a given problem and dataset. This course will also stress the importance of team-based collaboration, the value of producing fully reproducible and validated results, and tools to help with both such as version control and code repositories.\nCommunicating your results. Data science solutions are only as impactful as the communicator who shares them: therefore communication of your findings will be a core component of this course. Demonstrating competency in data science means (a) exhibiting a working knowledge of technical concepts including programming, statistics, and mathematics and (b) being able to clearly communicate the problem you were trying to solve or question you were trying to answer, why it matters, and how well your analysis worked. You will have opportunities to practice these skills throughout this course in the context of interpreting and sharing the results of your analyses.",
    "crumbs": [
      "Schedule",
      "Overview"
    ]
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Final Project",
    "section": "",
    "text": "It’s time for you to unleash your creativity in the final project for the course! This is your chance to apply everything you’ve learned so far, from experimental design and model selection to evaluation and optimization. This project gives you the freedom to explore new applications, experiment, and innovate. Creativity, critical thinking, and problem-solving will be key.\nMachine learning tools are not an end in themselves, but yield value when making predictions, quantifying and describing phenomena in the world around us, and in all these ways and more helping us to make decisions that would otherwise be difficult or impossible. For this final project, you will work in teams to (1) identify a problem to solve or a question to answer, (2) apply machine learning techniques to conduct experiments to investigate the application area, (3) rigorously evaluate the performance of your approach, and (4) clearly communicate your findings to a wide audience. The deliverables for this project are:\n\nProject proposal\nFinal written report AND a draft report prior to final submission\nPresentation. During the class exam period, we will have a project showcase and competition.\nGithub repository for your project\nPeer evaluation\n\nOther topics described in this document related to the project include: - Learning objectives - Submission, evaluation, & grading - Project ideas - Frequently asked questions",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Final Project"
    ]
  },
  {
    "objectID": "project.html#summary-and-goals",
    "href": "project.html#summary-and-goals",
    "title": "Final Project",
    "section": "",
    "text": "It’s time for you to unleash your creativity in the final project for the course! This is your chance to apply everything you’ve learned so far, from experimental design and model selection to evaluation and optimization. This project gives you the freedom to explore new applications, experiment, and innovate. Creativity, critical thinking, and problem-solving will be key.\nMachine learning tools are not an end in themselves, but yield value when making predictions, quantifying and describing phenomena in the world around us, and in all these ways and more helping us to make decisions that would otherwise be difficult or impossible. For this final project, you will work in teams to (1) identify a problem to solve or a question to answer, (2) apply machine learning techniques to conduct experiments to investigate the application area, (3) rigorously evaluate the performance of your approach, and (4) clearly communicate your findings to a wide audience. The deliverables for this project are:\n\nProject proposal\nFinal written report AND a draft report prior to final submission\nPresentation. During the class exam period, we will have a project showcase and competition.\nGithub repository for your project\nPeer evaluation\n\nOther topics described in this document related to the project include: - Learning objectives - Submission, evaluation, & grading - Project ideas - Frequently asked questions",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Final Project"
    ]
  },
  {
    "objectID": "project.html#learning-objectives",
    "href": "project.html#learning-objectives",
    "title": "Final Project",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThis is an opportunity to creatively deploy machine learning in an application area of interest to you. A central component of your project must be a machine learning methodology. It does not have to be one that we’ve explicitly discussed in class as you’re welcome to use the project as an opportunity to learn new topics, although there should be a supervised learning component to your project. The objectives of this project are to…\n\nDevelop deeper competency in applying machine learning methods in practical applications\nGain experience in learning more about a topic beyond what was explicitly discussed, but by building on the foundation you have developed throughout the course which enables you to learn about other machine learning concepts\nIncrease your experience with collaborative data science workflows\nExpand your data science portfolio\n\nIn this project you will use what you’ve learned throughout this course and build on that knowledge and experience to apply the paradigms, algorithms, evaluation tools, and interpretation techniques discussed throughout the course. I strongly encourage you to pick a project that is of genuine interest in some way (e.g. the application, the tools, the dataset, etc.). Learning comes from stretching yourself: this requires that you push yourself into some unfamiliar territory and that is often a challenge and leads to desirable difficulty. Through this struggle is how the best learning happens, but it requires perseverance and that is best achieved when you are able to bring intrinsic motivation to that challenge. Find a topic of interest and embrace the challenge!\nFor this project you will identify a problem you wish to solve using machine learning tools. Identify the experiment you would need to run to evaluate how well you solved it as compared to existing approaches in the field including what metrics to use to evaluate performance.\n\nRequirements\n\nThe project must involve supervised machine learning. You may include concepts we were not able to cover in the course. You may include other concepts at well, but there should be a supervised learning component.\nThe project must be able to be completed within the course of this semester and should be scoped correctly: we encourage you to be ambitious, but please visit office hours if you have questions about project scope.\nEvery project should involve learning more about both your application domain and the methods that you’re using. This means reading about both facets. If you’re working on a project involving diagnosis of a disease, you should read enough to understand the disease and how it manifests the symptoms that your data may be noting. You’re expected to develop some domain knowledge related to your problem and demonstrate that in the report.\nAt minimum, your project should have at least one experimental design or study per student on the team. Examples include (but are not limited to): Options for topics to investigate:\n\nCross-domain comparison. What is the impact on generalization performance if we know that the conditions under which we will apply our ML approach are different than the available training data (e.g. if I’m working on a computer vision problem and all my training data are from the U.S., but the goal is to apply the model to regions in Asia)\nModel sensitivity to imbalanced data. What if training data when the training data are not perfectly sampled to be representative of the actual target data. This could be an investigation of the impact and possible solutions to imbalanced datasets.\nActive learning for cases where collecting data is expensive. How little additional data do you need to add to your model and how might it be identified how to sample the new data.\nBias detection and mitigation - could the fairness of algorithms be analyzed across different subsets of the data.\nModel explainability / interpretability.\nModel robustness to noise and/or adversarial attacks\nModel robustness when the application domain will likely be shifted from the training domain\nWhen do ensembles actually help? Investigating model stacking and the conditions under which it is adventageous (performance / computation tradeoff)\nCalibration in classification models - do the confidence scores match the actual probabilities?\n\nYour project should consider the potential ethical implications of your work and describe how that was factored into your work.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Final Project"
    ]
  },
  {
    "objectID": "project.html#proposal",
    "href": "project.html#proposal",
    "title": "Final Project",
    "section": "Proposal",
    "text": "Proposal\nYour team will submit a short project proposal. You will receive feedback that should be used to guide your project development and execution. There are no length requirements on the proposal, but 2 pages should typically be sufficient. Every proposal should have the title of the project and the list of team members at the top of the first page.\nYou can find the project proposal template and instructions here. You are required to use the template for your proposal so that we can provide comments in Google docs. Please read through and discuss the different points mentioned in the template prior to submission.\nAdditionally, content from your proposal may be reused in your draft/final report and so you’re encouraged to invest in it with that in mind.\nIf you are looking for ideas about datasets, etc., please see the Ideas section below. Please stop by office hours if you would like to discuss specific project ideas or for any other help in selecting your project idea.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Final Project"
    ]
  },
  {
    "objectID": "project.html#final-report",
    "href": "project.html#final-report",
    "title": "Final Project",
    "section": "Final Report",
    "text": "Final Report\nThe final project report that you submit will consist of two parts: (1) a draft project report and (2) a final report. The draft project report is your main opportunity to get detailed feedback on your report. While the draft report won’t be graded, we will provide written feedback and suggestions in the form of Google doc comments that we would strongly recommend addressing in your final report.\nPlease find the instructions and template for the final report here.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Final Project"
    ]
  },
  {
    "objectID": "project.html#presentation",
    "href": "project.html#presentation",
    "title": "Final Project",
    "section": "Presentation",
    "text": "Presentation\nYou will also make a 3-minute presentation (strictly enforced) summarizing your project. This presentation should be visually compelling and should not miss the “forest for the trees” – don’t get lost in technical details. Imagine your aunt and uncle watching your presentation – would they know what is going on? Would they find it approachable and engaging? For inspiration for what makes an approachable discussion of a machine learning project, watch videos from the following series:\n\nTwo Minute Papers by Károly Zsolnai-Fehér. Concise 1-4 minute summaries of cutting edge research papers.\n3Blue1Brown by Grant Sanderson. Mathematical concepts conveyed clearly, intuitively, and visually.\n\nBe sure to practice your presentation, ask your friends (especially those who may not be as technically inclined) for feedback. Do they think it was engaging/easy to follow? Ask them their takeaways: did they get the message you were trying to communicate? Address their feedback to help you ensure the quality of your presentation. You must create your presentation as a Google Slides presentation and share and submit the link to your presentation by 5pm the night before the showcase.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Final Project"
    ]
  },
  {
    "objectID": "project.html#github-repository",
    "href": "project.html#github-repository",
    "title": "Final Project",
    "section": "Github Repository",
    "text": "Github Repository\nYour github respository should (a) contain a descriptive README.md file that explains what the repo is for, and how to use the code to reproduce your work (including how to set it up to run), (b) be well commented throughout all files, (c) list all dependencies in a requirements.txt file, (d) inform the user how to get the data and includes all preprocessing code, and (e) actually runs (i.e. we can successfully test it) and does what it says\nAlso include a copy of your final report and a link to your project video from the README.md file.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Final Project"
    ]
  },
  {
    "objectID": "project.html#peer-evaluation",
    "href": "project.html#peer-evaluation",
    "title": "Final Project",
    "section": "Peer Evaluation",
    "text": "Peer Evaluation\nSince this is a team project, you will also receive feedback from your teammates AND reflect on your own performance in a self-evaluation. You will be evaluating your fellow team members on the following criteria:\n\nWas dependable in attending meetings to work on the project\nDid work accurately and completely\nCompleted work on time\nContributed positively to team discussions\nHelped others when needed\nResponded to communications in a timely manner\nTreated other team members respectfully\nDemonstrated a positive attitude about the team and its work\n\nThis evaluation is NOT based directly on the scores that you receive in the feedback, but a satisfactory peer and self-evaluation is assessed based on the level of constructiveness of the feedback you provide. More detailed, constructive feedback is more useful to help your peers better understand their strengths and areas for growth. Doing so respectfully and compassionately is a requirement. Your peers will receive anonymized versions of the feedback that you share.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Final Project"
    ]
  },
  {
    "objectID": "project.html#submission-evaluation-grading",
    "href": "project.html#submission-evaluation-grading",
    "title": "Final Project",
    "section": "Submission, Evaluation, & Grading",
    "text": "Submission, Evaluation, & Grading\nYou should submit each deliverable from your project through Gradescope. You will submit a link to each team deliverable. This should be submitted AS A TEAM not through individual submissions (points will be deducted if this is not followed). The project proposal, and draft final report should be submitted through GradeScope as links to Google Docs (so that we can attach easy-to-repond-to comments) using the templates provided. The link to the presentation slides and github repo should also both be submitted as links via GradeScope. The final project report, however, should be submitted as a PDF document in GradeScope.\nThe grading for this project will be assigned as follows:\n\n\n\n\n\n\n\nComponent\nEvaluation / Feedback Plan\n\n\n\n\nPresentation\n5 points, graded\n\n\nFinal Report\n10 points, graded\n\n\nTeam Proposal\nWritten or verbal feedback will be provided to help guide your project design.**\n\n\nDraft Final Report\nWritten feedback will be provided to help guide your final report writing.**\n\n\nGithub Repository\nRequired for project submission to be considered complete.**\n\n\nPeer Evaluation\nRequired for project submission to be considered complete.**\n\n\nTotal\n15 points\n\n\n\n** No points will be directly assigned. One point will be deducted from your overall final project score for each day late; up to 2 points may be deducted from the overall project score (out of 15 possible points) if the deliverable is unsatisfactory (if it does not represent a serious effort towards the deliverable)",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Final Project"
    ]
  },
  {
    "objectID": "project.html#ideas",
    "href": "project.html#ideas",
    "title": "Final Project",
    "section": "Ideas",
    "text": "Ideas\n\nDesign your own project based on a question or problem. How well can you detect a disease using only data that might be available through sensors on a smart watch? How well do interpretable machine learning techniques perform in predicting recidivism as compared to less interpretable or explainable methodologies?\nReproduce the work of a published study and build on it. Reproducing the results of a journal article can be a great way to dive into advanced materials. The goal for a project like this would be to reproduce the study and build on it in some way: test a new hypothesis, adjust the methodology, try it on other data that may present new and interesting challenges. Reproducing papers can be hard, so you’ll want to choose wisely and make clear what your innovation will be. As a starting place, you can explore Papers with Code which typically have papers where the code and the data are both shared, often making reproducing their work simpler.\nParticipate in an active machine learning competition. Online machine learning competitions are sponsored by organizations with a significantly high interest in a problem that they are investing prize money into finding a solution. Examples of competition platforms include Kaggle, Driven Data, Zindi, AICrowd, etc. If you choose to participate in a competition, it must be an active competition where your team can compete; it cannot be a “sample” competition that is only for learning to use the platform (e.g. the Kaggle Titanic competition, etc.). You will want to learn about the application domain.\nBuild your own tool. Great value can come from making a tool available for use, but building the infrastructure is a challenge. You may want to create a chatbot that creates poetry based on themes that you feed in, or design a search tool that scans satellite data of the Earth for signs of natural disasters. The key here is that your tool will need to be functional and usable by your target audience. For this path, part of your output should be a demonstration of your tool.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Final Project"
    ]
  },
  {
    "objectID": "resources/dcc-documentation.html",
    "href": "resources/dcc-documentation.html",
    "title": "Duke Compute Cluster Guide",
    "section": "",
    "text": "This guide walks you through setting up and running Python and Jupyter Notebook on the DCC, managing environments, and submitting GPU jobs using Slurm (Simple Linux Utility for Resource Management).",
    "crumbs": [
      "Schedule",
      "Resources",
      "Duke Compute Cluster Guide"
    ]
  },
  {
    "objectID": "resources/dcc-documentation.html#one-time-setup-steps-first-time-only",
    "href": "resources/dcc-documentation.html#one-time-setup-steps-first-time-only",
    "title": "Duke Compute Cluster Guide",
    "section": "1 One-Time Setup Steps (First Time Only!)",
    "text": "1 One-Time Setup Steps (First Time Only!)\n\n1.1 Familiarize yourself with the available file systems on the DCC.\n\nOverview: Duke Compute Cluster\n\n\n\n1.2 SSH into the DCC.\nLog into the cluster using your NetID.\nCommand:\nssh {your_netID}@dcc-login.oit.duke.edu\n(Example)\n\n\n\n1.3 Create a symbolic link to the course25 directory.\nThe coursess25 directory has 1 Terabyte of shared storage space for the course. A symbolic link is a shortcut to another file system. When you first login, you will be in your home directory, create your shortcuts there.\nCommand:\nln -s /hpc/group/coursess25 ids_705_storage\n(Example)\n\n\n\n1.4 Download and Install Miniconda.\nExecute the following commands in your terminal and follow the instructions. Log out (close the terminal) and log back in upon successful installation, to being using Miniconda.\nCommands:\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nsh Miniconda3-latest-Linux-x86_64.sh",
    "crumbs": [
      "Schedule",
      "Resources",
      "Duke Compute Cluster Guide"
    ]
  },
  {
    "objectID": "resources/dcc-documentation.html#managing-python-environments",
    "href": "resources/dcc-documentation.html#managing-python-environments",
    "title": "Duke Compute Cluster Guide",
    "section": "2 Managing Python Environments",
    "text": "2 Managing Python Environments\n\n2.1 Create a new environment.\nCommand:\nconda create -n {name_of_environment} python={Python.Version}\n(Example)\n\n\n\n2.2 View available Conda environments.\nCommand:\nconda env list\n(Example)\n\n\n\n2.3 Activate the environment.\nCommand:\nconda activate {name_of_environment}\n(Example)\n\n\n\n2.4 Install Jupyter kernel and test installation.\nCommands:\nconda install ipykernel\n(Example)\n\npython -m ipykernel install --user --name test\n(Example)",
    "crumbs": [
      "Schedule",
      "Resources",
      "Duke Compute Cluster Guide"
    ]
  },
  {
    "objectID": "resources/dcc-documentation.html#accessing-jupyter-lab-via-dcc-ondemand-portal",
    "href": "resources/dcc-documentation.html#accessing-jupyter-lab-via-dcc-ondemand-portal",
    "title": "Duke Compute Cluster Guide",
    "section": "3 Accessing Jupyter Lab via DCC OnDemand Portal",
    "text": "3 Accessing Jupyter Lab via DCC OnDemand Portal\n\n3.1 Login at DCC OnDemand.\n\n\n3.2 Click on Jupyter Lab.\n\n\n\n3.3 Configure session resources:\n\nPartition: courses-gpu (for GPU) or courses (no GPU).\n\nGPUs: 1 (if needed) or 0.\n\nCPUs: Up to 40.\n\nMemory: Up to 208 GB.\n\nAdditional Slurm Parameters:\n--gres=gpu:1\n\n\n\n\n3.4 Launch the session and wait for the status to change to “Running”.\n\n\n3.5 Click the Connect to Jupyter button to access the server.\n\n\n\n3.6 Start your notebook by selecting your preferred kernel.",
    "crumbs": [
      "Schedule",
      "Resources",
      "Duke Compute Cluster Guide"
    ]
  },
  {
    "objectID": "resources/dcc-documentation.html#gpu-demonstration-on-jupyter-lab",
    "href": "resources/dcc-documentation.html#gpu-demonstration-on-jupyter-lab",
    "title": "Duke Compute Cluster Guide",
    "section": "4 GPU Demonstration on Jupyter Lab",
    "text": "4 GPU Demonstration on Jupyter Lab\n\n4.1 Activate the previously created Conda environment.\nIn your terminal, activate the previously created “test” Conda environment. You can access a terminal in this session by clicking the blue plus button and selecting terminal in the now open launcher window (You can also use your ssh terminal).\nCommand:\nconda activate test\n\n\n\n4.2 Install required packages.\nCommands:\nconda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y\nconda install matplotlib\n\n\n\n\n4.3 Open and run the notebook:\nLocation: ids_705_storage/demo/gpu_demo.ipynb.\n\nImportant: Restart the kernel to ensure all installed packages are available.\n\n\n\nExpected Output: Ensure the first line of output says “Using GPU!!”.\n\n\n.\n.",
    "crumbs": [
      "Schedule",
      "Resources",
      "Duke Compute Cluster Guide"
    ]
  },
  {
    "objectID": "resources/dcc-documentation.html#gpu-demonstration-using-slurm-scheduler-preferred-method-to-run-longer-jobs",
    "href": "resources/dcc-documentation.html#gpu-demonstration-using-slurm-scheduler-preferred-method-to-run-longer-jobs",
    "title": "Duke Compute Cluster Guide",
    "section": "5 GPU Demonstration Using Slurm Scheduler (Preferred method to run longer jobs)",
    "text": "5 GPU Demonstration Using Slurm Scheduler (Preferred method to run longer jobs)\nSlurm (Simple Linux Utility for Resource Management) is a powerful workload manager and job scheduler designed for high-performance computing (HPC) clusters. It enables efficient resource allocation by managing compute nodes, CPUs, GPUs, memory, and job priorities across multiple users.\nIn this tutorial, we’ll focus on using Slurm to submit GPU jobs to Duke’s Compute Cluster (DCC). You’ll learn how to create and submit job scripts, specify resources like GPUs, CPUs, and memory.\n\n5.1 (Optional) If you want to run the demo yourself start by navigating to the demo directory.\nCommand:\ncd ids_705_storage/demo/data\n\n\n\n5.2 Create your Python script (demo script: ids_705_storage/demo/data/test.py).\n\n\n\n5.3 Create the Slurm file (demo file: ids_705_storage/demo/test_cuda.slurm).\n\n\n\n5.4 Submit your Slurm job.\nCommand:\nsbatch test_cuda.slurm\n\n\n\n5.5 Monitor job status.\nCommand:\nsqueue -u {your_id}\n\n\n\n5.6 Check job output (Wait for JobID to dissapear from above).\nCommand:\ncat test_cuda_{your_job_id}.out\n\nExpected Output:Ensure the first line of the output says “Using GPU”.\n\n\n.\n.",
    "crumbs": [
      "Schedule",
      "Resources",
      "Duke Compute Cluster Guide"
    ]
  },
  {
    "objectID": "resources/dcc-documentation.html#final-notes",
    "href": "resources/dcc-documentation.html#final-notes",
    "title": "Duke Compute Cluster Guide",
    "section": "6 Final Notes",
    "text": "6 Final Notes\n\nCreate a personal directory under: /hpc/group/coursess25/ids705 and name it after your NetID.\n\nBe mindful of shared resources and job submissions.\n\nQuestions? Contact: rakeen.rouf@duke.edu.\n\nVisit Duke Compute Cluster for more information.\n\nIf resources are busy, set GPUs to 0 and ensure your code works on CPU (see demo code for an example).",
    "crumbs": [
      "Schedule",
      "Resources",
      "Duke Compute Cluster Guide"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Course Logistics",
    "section": "",
    "text": "Class Time and Location\n\n\n\n\nWhen: Tuesday and Thursdays 10:05am - 11:20am\nWhere: Gross Hall 103",
    "crumbs": [
      "Schedule",
      "Syllabus",
      "Course Logistics"
    ]
  },
  {
    "objectID": "syllabus.html#class-tools-and-resources",
    "href": "syllabus.html#class-tools-and-resources",
    "title": "Course Logistics",
    "section": "Class Tools and Resources",
    "text": "Class Tools and Resources\n\nEd Discussions: Announcements, Q&A on course content (assignments, quizzes, grades), ALL course communications\nGradescope: Quizzes, assignments, and project submission & feedback\nSchedule: Schedule of class topics and deliverables\nCanvas: Posted grades",
    "crumbs": [
      "Schedule",
      "Syllabus",
      "Course Logistics"
    ]
  },
  {
    "objectID": "syllabus.html#textbooks",
    "href": "syllabus.html#textbooks",
    "title": "Course Logistics",
    "section": "Textbooks",
    "text": "Textbooks\nA version of each book is available free online:\n\nAn Introduction to Statistical Learning with Python by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani, 2013.\nUnderstanding Deep learning by Simon Prince, 2023.\nPattern Recognition and Machine Learning by Christopher Bishop, 2006.\nDeep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville, 2016.\nReinforcement Learning: An Introduction, by Richard Sutton and Andrew Barto, 2018.",
    "crumbs": [
      "Schedule",
      "Syllabus",
      "Course Logistics"
    ]
  },
  {
    "objectID": "syllabus.html#assignments-grading",
    "href": "syllabus.html#assignments-grading",
    "title": "Course Logistics",
    "section": "Assignments & Grading",
    "text": "Assignments & Grading\nAssignments, projects, & quizzes: Assignments and projects details are posted on the course syllabus. For expectations and instructions on the assignments, see the assignment instructions. Quizzes are found on Gradescope and are due prior to the start of the lecture for which they’re titled (i.e. the Lecture 3 Quiz is due by the start of Lecture 3).\n\nGrading:\n\n60% Assignments (5, each worth 12%)\n25% Quizzes (~23, each worth ~1%)\n15% Final Project\n\n\n\nPrerequisites\nThis course moves quickly, so having a firm grasp on prerequisites is important. The prerequisites are as follows:\n\nProgramming: Fundamentals of Python programming.\nMathematics: Calculus and linear algebra.\nStatistics: Introductory probability and statistics.",
    "crumbs": [
      "Schedule",
      "Syllabus",
      "Course Logistics"
    ]
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Course Logistics",
    "section": "Course Policies",
    "text": "Course Policies\n\nAcademic dishonesty\nAdherence to the Duke Community Standard is expected. To uphold the Duke Community Standard:\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised\nAnyone found in violation of the Standard will be reported to the Office of Student Conduct.\n\n\nClass Attendance\nAttending class is a vital component of the course as it is one of the multiple ways in which you will interact with and learn course material. In person class attendance is therefore expected for this course. For any special circumstances, please reach out to the course instructor.\n\n\nSick absences\nTo keep the university community as safe and healthy as possible, please do not come to class if you have cold symptoms. Please inform me of your absence and plan to complete any missed work. Students who encounter short- and long-term medical issues or instances of personal distress or emergency can seek academic support if needed. Recordings of the class will be available for excused absences.\n\n\nAccommodations and accessibility\nIf you need special accommodations due to physical or learning disabilities, medical needs, religious practices, or other reasons, please inform us as soon as possible so we can work to accommodate those needs.\nIf you are a student with a disability and need accommodations for this class, please register with the Student Disability Access Office (SDAO) and provide them with documentation of your disability. SDAO will work with you to determine what accommodations are appropriate for your situation. Please note that accommodations are not retroactive and disability accommodations cannot be provided until a Faculty Accommodation Letter has been given to your instructor. Please contact SDAO for more information: sdao@duke.edu or .\n\n\nLate Submissions\nAssignments and projects are due in class by the start of class on the date posted. Late deliverables will ONLY be accepted at the discretion of the instructor and according to the following:\n\nCourse projects deliverables will not be accepted after the deadline.\nLate assignment submissions will result in a reduction of 5 points off the grade per day late.\nQuizzes will not be accepted after the deadline since the answers to the quizzes are discussed in and made available after the class in which they’re due. Quizzes are typically posted a week or more in advance, so you are encouraged to start early (you can submit as early as you’d like). Quizzes cannot be made up, but there will be an opportunity later in the semester to make up a quiz to account for any off days.\n\nPlease reach out to the TA’s or instructor as early as possible to request any special accommodations.\n\n\nCollaboration\nThere will be three modes of collaboration ranging from fully-collaborative group projects, to fully-independent work. The three modes are as follows, and will be indicated throughout the course:\n\nMode 1: Team-based Assignment. Collaboration is expected with every member of the team contributing to a single deliverable. Applies to the Project\nMode 2: Individual Assignment – Collaboration Permitted. Students hand in individual work, but they may work with others if they provide citations of the help they received, such as a list of people who assisted/collaborated with them to produce the final product. Duplication or copying is not permissible, even in part, and constitutes a breach of the honor code. Applies to Assignments\nMode 3: Individual Assignment – No Collaboration Permitted. Students hand in individual work that is completed entirely independent of any discussion or help from other students. Clarifying questions to teaching assistants and instructors are both permissible and encouraged. Applies to Quizzes\n\n\n\nRules for recording course content\nStudent recording recordings of lectures must be permitted by the instructor prior to recording and shall be for private study only. Such recordings shall not be distributed to anyone without authorization by the instructor whose lecture has been recorded. However, the instructor may arrange through the Office of Information Technology to make recorded lectures available to students enrolled in the class on such terms and conditions as he or she prescribes. Redistribution of recorded lectures is prohibited. Unauthorized distribution is a cause for disciplinary action by the Judicial Board. The full policy on recoding of lectures falls under the Duke University Policy on Intellectual Property Rights, available here.",
    "crumbs": [
      "Schedule",
      "Syllabus",
      "Course Logistics"
    ]
  },
  {
    "objectID": "syllabus.html#mental-health-and-wellness-resources",
    "href": "syllabus.html#mental-health-and-wellness-resources",
    "title": "Course Logistics",
    "section": "Mental Health and Wellness Resources",
    "text": "Mental Health and Wellness Resources\nStudent mental health and wellness are of primary importance at Duke, and the university offers resources to support students in managing daily stress and self- care.\nIf your mental health concerns or stressful events negatively affect your daily emotional state, academic performance, or ability to participate in your daily activities, many resources are available to help you through difficult times. Duke offers several resources for all students to seek assistance and to nurture daily habits that support overall well-being, some of which are listed below:\n\nDuWell, (919) 681-8421. DuWell provides Moments of Mindfulness (stress management and resilience building) and meditation programming (Koru workshop) to assist students in developing a daily emotional well-being practice. All are welcome and no experience necessary.\nDukeReach. DukeReach provides comprehensive outreach services to identify and support students in managing all aspects of well-being.\nCounseling and Psychological Services (CAPS), (919) 660-1000. CAPS services include individual and group counseling services, psychiatric services, and workshops. CAPS also provides referral to off-campus resources for specialized care. • TimelyCare (formerly known as Blue Devils Care). An online platform that is a convenient, confidential, and free way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling.",
    "crumbs": [
      "Schedule",
      "Syllabus",
      "Course Logistics"
    ]
  },
  {
    "objectID": "notebooks/assignment1.html",
    "href": "notebooks/assignment1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Instructions for all assignments can be found here. Note: this assignment falls under collaboration Mode 2: Individual Assignment – Collaboration Permitted. Please refer to the syllabus for additional information. Please be sure to list the names of any students that you worked with on this assignment. Total points in the assignment add up to 90; an additional 10 points are allocated to professionalism and presentation quality.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 1"
    ]
  },
  {
    "objectID": "notebooks/assignment1.html#instructions",
    "href": "notebooks/assignment1.html#instructions",
    "title": "Assignment 1",
    "section": "",
    "text": "Instructions for all assignments can be found here. Note: this assignment falls under collaboration Mode 2: Individual Assignment – Collaboration Permitted. Please refer to the syllabus for additional information. Please be sure to list the names of any students that you worked with on this assignment. Total points in the assignment add up to 90; an additional 10 points are allocated to professionalism and presentation quality.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 1"
    ]
  },
  {
    "objectID": "notebooks/assignment1.html#learning-objectives",
    "href": "notebooks/assignment1.html#learning-objectives",
    "title": "Assignment 1",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThe purpose of this assignment is to provide a refresher on fundamental concepts that we will use throughout this course and provide an opportunity to develop skills in any of the related skills that may be unfamiliar to you. Through the course of completing this assignment, you will…\n\nRefresh your knowledge of probability theory including properties of random variables, probability density functions, cumulative distribution functions, and key statistics such as mean and variance.\nRevisit common linear algebra and matrix operations and concepts such as matrix multiplication, inner and outer products, inverses, the Hadamard (element-wise) product, eigenvalues and eigenvectors, orthogonality, and symmetry.\nPractice numerical programming, core to machine learning, by applying it to scenarios of probabilistic modeling, linear algebra computations, loading and plotting data, and querying the data to answer relevant questions.\n\nWe will build on these concepts throughout the course, so use this assignment as a catalyst to deepen your knowledge and seek help with anything unfamiliar.\nFor references on the topics in this assignment, please check out the resources page on the course website for online materials such as books and courses to support your learning.\nNote: don’t worry if you don’t understand everything in the references above - some of these books dive into significant minutia of each of these topics.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 1"
    ]
  },
  {
    "objectID": "notebooks/assignment1.html#exercise-1---probabilistic-reasoning",
    "href": "notebooks/assignment1.html#exercise-1---probabilistic-reasoning",
    "title": "Assignment 1",
    "section": "Exercise 1 - Probabilistic Reasoning",
    "text": "Exercise 1 - Probabilistic Reasoning\n1.1. Probabilistic Reasoning I. You are handed three fair dice and roll them sequentially. What’s the probability of the sum of the dice is 10 after you’ve rolled the first die and it shows a 1?\n1.2. Probabilistic Computation I. Simulate the scenario in 1.1 by creating 1 million synthetic rolls of the three dice. Determine what fraction of outcomes that had a “1” for the first die also had a sum of 10 across the three die.\n1.3. Probabilistic Reasoning II. A test for a rare disease has a 95% chance of detecting the disease if a person has it (true positive rate) and a 3% chance of wrongly detecting it if a person does not have it (false positive rate). If 1 in 1,000 people actually have the disease, what is the probability that a randomly chosen person who tests positive actually has the disease?\n1.4. Discrete Probability Theory. A discrete random variable \\(X\\) is distributed as follows (probability mass function):\n\\(P(X = x) = \\begin{cases}\n                0.2 & x = -1 \\\\\n                0.5 & x = 0 \\\\\n                0.3 & x = 1\n            \\end{cases}\\)\nWhat is the expected value, \\(E_X[X]\\) and variance, \\(Var_X(X)\\) of the random variable \\(X\\)?",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 1"
    ]
  },
  {
    "objectID": "notebooks/assignment1.html#exercise-2---probability-distributions-and-modeling",
    "href": "notebooks/assignment1.html#exercise-2---probability-distributions-and-modeling",
    "title": "Assignment 1",
    "section": "Exercise 2 - Probability Distributions and Modeling",
    "text": "Exercise 2 - Probability Distributions and Modeling\nYou’ve been asked to create a model of wait time for customers at Olivander’s Wand Shop. While they strive for the perfect match, there is some cleanup between customers that has been keeping wait times high. They’re open 8 hours a day and the maximum wait time is 8 hours (we won’t assume it’s possible not to be seen, assuming you’re willing to wait). Define the continuous random variable \\(X = \\{\\text{wait time for service as a fraction of 8 hours}\\}\\) . This means that \\(x=1\\) represents a full day’s wait, or 8 hours, \\(x=0.5\\) represents half a day wait or 4 hours. Additionally, the valid values of \\(X\\) are between 0 and 1 (\\(0\\leq x \\leq 1\\)).\nWe’ll begin by analyzing some data of past visits to the shop to understand the customer wait time experience through our data (2.1-2.2). Then, we’ll select a model we hypothesize might fit our situation well and evaluate its properties like mean and variance (2.3-2.7). We’ll also evaluate the quality of the fit of the model as compare to our data (2.8-2.11). Lastly, we’ll explore how this approach can be used to generate insights (2.12).\n\nReviewing our wait time data\n2.1. Load and plot a histogram of your wait time data. The file is wait_times.csv in the data/a1 folder here on Github. I recommend using the simple np.loadtxt() function to accomplish this so you can quickly load it in as a numpy array. Remember, the value 1 represents a full 8 hour work day so you should see your data are all in the range of \\([0,1]\\). Please use 10 bins and limit the bin edges to the range \\([0,1]\\) (no values should be plotted outside that range).\n2.2. Mean, variance, and standard deviation of the data. Compute the mean, variance, and standard deviation of the wait time data. Report the mean and standard deviation in both the original units (in \\([0,1]\\)) and in hours (the variance is unitless).\n\n\nCreating a model for the wait time distribution\nTake a moment to review the distribution of the data. The most common distribution is normal, but this doesn’t seem normally distributed. Neither does it look uniform. The shape actually looks like it may be exponentially distributed, but truncated at 1. It’s not uncommon to have this type of shape in a wait time model, but this introduces a challenge since we can’t just use the standard exponential distribution since an exponential distribution is defined on a domain from 0 to infinity, but our data is defined between 0 and 1. Let’s create a customized distribution as a model for our data and see how well it represents the key statistics of our data.\nIn this section, please note the list of equations and identities at the end of this document as they may be useful for several questions.\n2.3. Probability Density Functions (PDFs). Compute the value of \\(\\alpha\\) that makes \\(f_X(x)\\) a valid probability density function:\n\\(f_X(x) = \\begin{cases}\n                \\alpha e^{-x}  & 0 \\leq x \\leq 1 \\\\\n                0           & \\text{else}\n            \\end{cases}\\)\nProvide this value exactly (with no approximation) and also provide and approximate decimal value with a precision to three decimal places.\n2.4. Cumulative Distribution Functions (CDFs). Compute the cumulative distribution function (CDF) of \\(X\\), \\(F_X(x)\\), where \\(F_X(x)=P(X&lt;x)\\) (here, \\(P(\\cdot)\\) represents the probability of the event within the brackets). Be sure to indicate the value of the CDF for all values of \\(x\\in(-\\infty,\\infty)\\). Express your CDF using the variable \\(\\alpha\\) to provide the precise CDF.\n2.5. Expected Value. Compute the expected value of \\(X\\), \\(E_X[X]\\). Provide this value exactly (no approximations and only in terms of \\(e\\) and \\(\\alpha\\)) and provide a numerical approximation of the expected value to 3 decimal places. Also provide the approximate number of hours waiting (to 3 decimal places).\n2.6. Variance and Standard Deviation. Compute the variance of \\(X\\), \\(Var(X)\\) approximately to 3 significant figures, meaning 3 digits without leading zeros (e.g. 12.3, 0.123, 0.00123, all have 3 significant figures). Using the variance, calculate the standard deviation and express this standard deviation in both the original units and units of hours.\n2.7. Plotting your functions. Create functions to implement your PDF, \\(f_X(x)\\), and CDF, \\(F_X(x)\\), for all possible values of \\(x\\). Using these functions, plot the PDF and CDF on the inverval \\(-0.5 \\leq x \\leq 1.5\\).\n\n\nEvaluating the quality of the model\n2.8. Compare the empirical CDF to the modeled CDF. Plot both of these on the interval \\(0 \\leq x \\leq 1\\). For the empirical CDF of the data from wait_times.csv, you can plot the empirical CDF by sorting the data in ascending order, your \\(x\\) values, and assigning the \\(y\\) value as the cumulative fraction of samples that are smaller than or equal to each \\(x\\) value.\n2.9. Calculate the inverse CDF to enable you to generate synthetic data. Create a numerical simulation of this process. Doing this for a custom PDF is easier than you may think. We typically have access to uniformly distributed samples (through np.random.rand), and we can transform these uniform samples into any distribution we wish. To do this, we can input uniform variates through the inverse of the CDF. If \\(U\\) is a uniformly distributed random variable and \\(F_X(x)\\) is the CDF of the distribution we’re looking to model, then \\(F_X^{-1}(U)\\) will be distributed in the same way as \\(X\\), as shown below in Figure 1.\n\n\n\nSynthetic Data Generation\n\n\nFigure 1. Demonstrating the process of transforming a uniformly distributed random variable into almost any distribution (here we transform into a normal). Here we show the transformation a sample, \\(u^{\\ast}\\), from a uniform distribution to a sample, \\(x^{\\ast}\\) from a normal distribution by applying the inverse of the CDF of \\(X\\) to \\(u^{\\ast}\\), that is \\(x^{\\ast} = F_X^{-1}(u^{\\ast})\\).\nCalculate the inverse of the CDF, \\(F_X^{-1}(y)\\) (We use the variable \\(y\\) as the input into this function to denote that we’re inputting the “output” of the CDF into this inverse CDF).\n2.10. Generate synthetic data using the inverse CDF by transforming uniform samples. Once you have your inverse CDF, code it up and use it to create synthetic samples from the last step. To do so, first generate 10,000 samples from the uniform distribution and then feed those uniform variates through the inverse CDF to generate synthetic variates from our wait time model. Using those samples, compute the mean and standard deviation. Present the mean and standard deviation in a table comparing (a) the empirical values computed from wait_times.csv, (b) your theoretical model values calculated earlier, and (c) your computed values calculated from your synthetic model. How do they compare?\n2.11. Run a statistical test to evaluate the goodness of fit of your model to the empirical data from wait_times.csv. To evaluate the goodness of fit of the model to the data, one tool is the Kolmogorov–Smirnov test, or simply the KS test. The two-sample version of this test evaluates the maximum distance between two CDFs, each calculated from samples of data. In this case, the null hypothesis states that the samples are drawn from the same distribution. We can conclude that the sample data are well-represented by the reference distribution if we do NOT reject the null hypothesis. If we test at the 5% significant level, then we can conclude that data come from the same distribution if the p-value is greater than 0.05 (we fail to reject the null hypothesis).\nCompare the sample of data from wait_times.csv to the synthetic sample from the model distribution and run the KS test. Also compare the sample data to the uniform distributed data you generated before transforming it into the synthetic samples. For the test use scipy.stats.kstest.\n\n\nUsing the model to understand wait times\n2.12. Computing probabilities. Having a way of generating synthetic data can allow us to easily compute probabilities. Compute the probabilities of the following events using the synthetic data samples that you generated.\n\nWait time is more than 6 hours\nWait time is less than 1 hour\nWait time is less than one additional hour given the client has already been waiting for 3 hours (i.e., the probability of the wait time being less than 4 hours if they have already waited for 3 hours)\nWait time is between 3 and 5 hours\nWhat is the 90th percentile of wait times?\nWhat is the 99th percentile of wait times?",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 1"
    ]
  },
  {
    "objectID": "notebooks/assignment1.html#exercise-3---linear-algebra-operations-and-theory",
    "href": "notebooks/assignment1.html#exercise-3---linear-algebra-operations-and-theory",
    "title": "Assignment 1",
    "section": "Exercise 3 - Linear Algebra Operations and Theory",
    "text": "Exercise 3 - Linear Algebra Operations and Theory\n3.1. Matrix manipulations and multiplication. Machine learning involves working with many matrices and understanding what their products represent, so this exercise will provide you with the opportunity to practice those skills.\nLet \\(\\mathbf{A} =  \\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\\), \\(\\mathbf{b} =  \\begin{bmatrix}\n-1  \\\\\n1\n\\end{bmatrix}\\), \\(\\mathbf{c} =  \\begin{bmatrix}\n1  \\\\\n2\n\\end{bmatrix}\\)\nCompute the following by hand or indicate that it cannot be computed. For any cases where an operation is invalid and cannot be computed, explain why it is invalid.\n\n\\(\\mathbf{A}\\mathbf{A}\\)\n\\(\\mathbf{A}\\mathbf{A}^{\\top}\\)\n\\(\\mathbf{A}\\mathbf{b}\\)\n\\(\\mathbf{A}\\mathbf{b}^{\\top}\\)\n\\(\\mathbf{b}\\mathbf{A}\\)\n\\(\\mathbf{b}^{\\top}\\mathbf{A}\\)\n\\(\\mathbf{b}\\mathbf{b}\\)\n\\(\\mathbf{b}^{\\top}\\mathbf{b}\\)\n\\(\\mathbf{b}\\mathbf{b}^{\\top}\\)\n\\(\\mathbf{A}\\circ\\mathbf{A}\\)\n\\(\\mathbf{b}\\circ\\mathbf{c}\\)\n\\(\\mathbf{b}^{\\top}\\mathbf{b}^{\\top}\\)\n\\(\\mathbf{b} + \\mathbf{c}^{\\top}\\)\n\\(\\mathbf{A}^{-1}\\mathbf{b}\\)\n\\(\\mathbf{b}^{{\\top}}\\mathbf{A}\\mathbf{b}\\)\n\\(\\mathbf{b}\\mathbf{A}\\mathbf{b}^{{\\top}}\\)\n\nNote: The element-wise (or Hadamard) product is the product of each element in one matrix with the corresponding element in another matrix, and is represented by the symbol “\\(\\circ\\)”.\n3.2. Matrix manipulations and multiplication using Python. Repeat 3.1, but this time using Python. If you are using a vector, make sure the dimensions of the vector match what you’d expect, for example, matrix \\(\\mathbf{b}\\) is a \\([2 \\times 1]\\) vector. In NumPy, unless you’re specify, you’ll like create a one-dimensional array of length 2 rather than a \\([2 \\times 1]\\) vector if you don’t specify - be careful of this potential pitfall. Refer to NumPy’s tools for handling matrices. There may be circumstances when Python will produce an output, but based on the dimensions of the matrices involved, the linear algebra operation is not possible. Note these cases and explain why they occur. Please provide both the Python code AND the output of that code showing your result. If the output is an error, comment out the code and note that it cannot be computed.\nBe sure to use the right operator for each operation: Matrix multiplication: @; Element-wise multication: *. For this exercise, only use one of those to operators for matrix or vector multiplication.\n3.3. Vector Norms. Norms are the effective lengths of vectors. For example, the Euclidean norm, or \\(L_2\\) norm (denoted as \\(||\\mathbf{\\cdot}||_2\\)), is the most common of several types of norms. The \\(L_2\\) norm can be calculated for a vector \\[\\mathbf{x} =  \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\\] as follows: \\[||\\mathbf{x}||_2 = \\sqrt{\\displaystyle \\sum_{k=1}^n x_k^2} = \\sqrt{\\mathbf{x}^{\\top}\\mathbf{x}}\\]\nWhat is the \\(L_2\\) norm of vectors \\(\\mathbf{d}_1 =  \\begin{bmatrix} 2^{-1/2} \\\\ -2^{-1/2} \\\\ 0 \\end{bmatrix}\\) and \\(\\mathbf{d}_2 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\)?\n3.4. Orthogonality and unit vectors. Orthogonal vectors are frequently used in machine learning in topics such as Principal Components Analysis and feature engineering for creating decorrelated features. Knowing what an orthogonal or orthonomal basis is for a space is an important concept. Find all values of unit vectors, \\(\\mathbf{d}_3\\), that complete an orthonormal basis in a three-dimensional Euclidean space along with the two vectors: \\(\\mathbf{d}_1 =  \\begin{bmatrix} 2^{-1/2} \\\\ -2^{-1/2} \\\\ 0 \\end{bmatrix}\\) and \\(\\mathbf{d}_2 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\).\nFor review, vector \\(\\mathbf{x}_1\\) is orthonormal to vector \\(\\mathbf{x}_2\\) if (a) \\(\\mathbf{x}_1\\) is orthogonal to vector \\(\\mathbf{x}_2\\) AND \\(\\mathbf{x}_1\\) is a unit vector. Orthogonal vectors are perpendicular, which implies that their inner product is zero. A unit vector is of length 1 (meaning its \\(L_2\\) norm is 1).\n3.5. Eigenvectors and eigenvalues. Eigenvectors and eigenvalues are useful for numerous machine learning algorithms, but the concepts take time to solidly grasp. They are used extensively in machine learning including in Principal Components Analysis (PCA) and clustering algorithms. For an intuitive review of these concepts, explore this interactive website at Setosa.io. Also, the series of linear algebra videos by Grant Sanderson of 3Brown1Blue are excellent and can be viewed on youtube here. For these questions, numpy may once again be helpful.\n\nIn Python, calculate the eigenvalues and corresponding eigenvectors of matrix \\(\\mathbf{B}=  \\begin{bmatrix}\n1 & 2 & 3 \\\\\n2 & 4 & 5 \\\\\n3 & 5 & 6\n\\end{bmatrix}\\)\nChoose one of the eigenvector/eigenvalue pairs, \\(\\mathbf{v}\\) and \\(\\lambda\\), and show that \\(\\mathbf{B} \\mathbf{v} = \\lambda \\mathbf{v}\\). This relationship extends to higher orders: \\(\\mathbf{B} \\mathbf{B} \\mathbf{v} = \\lambda^2 \\mathbf{v}\\)\nShow that the eigenvectors are orthogonal to one another (e.g. their inner product is zero - just compute the inner product and show it is approximately 0). This is true for eigenvectors from real, symmetric matrices. In three dimensions or less, this means that the eigenvectors are perpendicular to each other. Typically we use the orthogonal basis of our standard x, y, and z, Cartesian coordinates, which allows us, if we combine them linearly, to represent any point in a 3D space. But any three orthogonal vectors can do the same. This property is used, for example, in PCA to identify the dimensions of greatest variation.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 1"
    ]
  },
  {
    "objectID": "notebooks/assignment1.html#exercise-4---numerical-programming-with-data",
    "href": "notebooks/assignment1.html#exercise-4---numerical-programming-with-data",
    "title": "Assignment 1",
    "section": "Exercise 4 - Numerical Programming with Data",
    "text": "Exercise 4 - Numerical Programming with Data\nLoading data and gathering insights from a real dataset. In data science, we often need to have a sense of the idiosyncrasies of the data, how they relate to the questions we are trying to answer, and to use that information to help us to determine what approach, such as machine learning, we may need to apply to achieve our goal. This exercise provides practice in exploring a dataset and answering question that might arise from applications related to the data.\nYour objective. For this dataset, your goal is to answer the questions below about electricity generation in the United States.\nData. The data for this problem can be found in the data\\a1\\ subfolder in the notebooks folder on github. The filename is egrid2016.xlsx. This dataset is the Environmental Protection Agency’s (EPA) Emissions & Generation Resource Integrated Database (eGRID) containing information about all power plants in the United States, the amount of generation they produce, what fuel they use, the location of the plant, and many more quantities. We’ll be using a subset of those data.\nThe fields we’ll be using include:\n\n\n\nfield\ndescription\n\n\n\n\nSEQPLT16\neGRID2016 Plant file sequence number (the index)\n\n\nPSTATABB\nPlant state abbreviation\n\n\nPNAME\nPlant name\n\n\nLAT\nPlant latitude\n\n\nLON\nPlant longitude\n\n\nPLPRMFL\nPlant primary fuel\n\n\nCAPFAC\nPlant capacity factor\n\n\nNAMEPCAP\nPlant nameplate capacity (Megawatts MW)\n\n\nPLNGENAN\nPlant annual net generation (Megawatt-hours MWh)\n\n\nPLCO2EQA\nPlant annual CO2 equivalent emissions (tons)\n\n\n\nFor more details on the data, you can refer to the eGrid technical documents. For example, you may want to review page 51 and the section “Plant Primary Fuel (PLPRMFL)”, which gives the full names of the fuel types including WND for wind, NG for natural gas, BIT for Bituminous coal, etc.\nThere also are a couple of “gotchas” to watch out for with this dataset:\n\nThe headers are on the second row and you’ll want to ignore the first row (they’re more detailed descriptions of the headers).\nNaN values represent blanks in the data. These will appear regularly in real-world data, so getting experience working with these sorts of missing values will be important.\n\nQuestions to answer:\n4.1. Which power plant generated the most energy in 2016 (measured in MWh)?\n4.2. Which power plant produced the most CO2 emissions (measured in tons)?\n4.3. What is the primary fuel of the plant with the most CO2 emissions?\n4.4. What is the name of the northern-most power plant in the United States?\n4.5. What is the state where the northern-most power plant in the United States is located?\n4.6. Plot a bar plot showing the amount of energy produced by each fuel type across all plants.\n4.7. From the plot in (D), which fuel for generation produces the most energy (MWh) in the United States?\n4.8. Which state has the largest number of hydroelectric plants? In this case, each power plant counts once so regardless of how large the power plant is, we want to determine which state has the most of them. Note the primary fuel for hydroelectric plants is listed as water in the documentation.\n4.9. Which state has generated the most energy (MWh) using coal? You may also want to explore the documentation for the isin() method for pandas. Note: in the eGrid documentation, there are multiple types of coal listed; be sure to factor in each type of coal.\n4.10. Which primary fuel produced the most CO2 emissions in the United States? We would like to compare natural gas, coal, oil, and renewables but the current categories are much more specific than that. As a first step, group the data as shown below, replacing the existing labels with the replacements suggested. For example, BIT and LIG should be replaced with COAL.\n\nCOAL = BIT, LIG, RC, SUB, WC\nOIL = DFO, JF, KER, RFO, WO\nGAS = BFG, COG, LFG, NG, OG, PG, PRG\nRENEW = GEO, SUN, WAT, WDL, WDS, WND\n\nYou may want to create a function that does this replacement prior to running your code. You can check whether or not it was successful by verifying that each of the values that should be replaced has been replaced - check that before moving on with the question.\nYou will want to use ‘PLCO2EQA’ to answer this question as it’s the quantity of emissions each plant generates.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 1"
    ]
  },
  {
    "objectID": "notebooks/assignment1.html#appendix-definitions-and-identities",
    "href": "notebooks/assignment1.html#appendix-definitions-and-identities",
    "title": "Assignment 1",
    "section": "Appendix: Definitions and identities",
    "text": "Appendix: Definitions and identities\nThe symbology in this assignment conforms to the following convention:\n\n\n\n\n\n\n\n\nSymbol Example\nMeaning\nPossible variations\n\n\n\n\n\\(X\\)\nA random variable\nUpper case, non-bolded letter\n\n\n\\(\\bar{X}\\)\nThe complement of a random variable\nUpper case, non-bolded letter with bar\n\n\n\\(\\mathbf{x}\\)\nVector (\\(N \\times 1\\))\nLower case, bolded letters/symbols\n\n\n\\(\\mathbf{X}\\)\nMatrix (\\(N \\times M\\))\nUpper case, bolded letters/symbols\n\n\n\\(P(\\cdot)\\)\nProbability of the event within the parenthesis\nParenthesis may include one event or more events\n\n\n\\(A \\cap B\\)\nIntersection of \\(A\\) and \\(B\\), that is the case of events \\(A\\) and \\(B\\) occurring simultaneously\nTwo random variables represented by upper case unbolded letters\n\n\n\nBelow is a list of potentially helpful identities and equations for reference.\n\n\n\n\n\n\n\nIdentities and equations\nDescription\n\n\n\n\n\\(E_X[X] = \\displaystyle \\int_{-\\infty}^{\\infty} x f_X(x) dx\\)\nExpected value of continuous random variable \\(X\\)\n\n\n\\(Var_X(X) = E_X[X^2]-E_X[X]^2\\)\nVariance of random variable \\(X\\)\n\n\n\\(\\sigma_X(X) = \\sqrt{Var_X(X)}\\)\nStandard deviation of \\(X\\) as a function of variance\n\n\n\\(\\displaystyle P( X \\vert Y)= \\frac{P(X \\cap Y)}{P(Y)}\\)\nConditional probability of event \\(X\\) given event \\(Y\\) has occurred\n\n\n\\(\\displaystyle P(Y \\vert X)= \\frac{P(X \\vert Y)P(Y)}{P(X)}\\)\nBayes’ Rule\n\n\n\\(\\displaystyle F_X(x) = \\int_{-\\infty}^{x} f_X(x) dx\\)\nCDF as a function of PDF\n\n\n\\(\\displaystyle f_X(x) = \\frac{dF_X(x)}{dx}\\)\nPDF as a function of CDF\n\n\n\\(P(X \\leq x) = F_X(x)\\)\nProbabilistic definition of the CDF\n\n\n\\(P(a &lt; X \\leq b) = F_X(b) - F_X(a)\\)\nProbability the \\(X\\) lies between \\(a\\) and \\(b\\)\n\n\n\\(P(A) + P(\\bar{A}) = 1\\)\nThe sum of the probability of an event and its complement is 1\n\n\n\\(\\displaystyle P(Y) = P(Y \\vert X)P(X) + P(Y  \\vert \\bar{X})P(\\bar{X})\\)\nLaw of Total Probability\n\n\n\\(\\displaystyle \\int e^{-x} dx = -e^{-x}\\)\nIndefinite integral\n\n\n\\(\\displaystyle \\int x e^{-x} dx = -e^{-x}(x+1)\\)\nIndefinite integral\n\n\n\\(\\displaystyle \\int x^2 e^{-x} dx = -e^{-x}(x^2+2x+2)\\)\nIndefinite integral\n\n\n\\(\\displaystyle \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}^{-1} = \\frac{1}{ad-cb} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\\)\n\\(2 \\times 2\\) matrix inversion formula",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 1"
    ]
  },
  {
    "objectID": "notebooks/assignment3.html",
    "href": "notebooks/assignment3.html",
    "title": "Assignment 3",
    "section": "",
    "text": "Instructions for all assignments can be found here. Note: this assignment falls under collaboration Mode 2: Individual Assignment – Collaboration Permitted. Please refer to the syllabus for additional information. Please be sure to list the names of any students that you worked with on this assignment. Total points in the assignment add up to 90; an additional 10 points are allocated to professionalism and presentation quality.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 3"
    ]
  },
  {
    "objectID": "notebooks/assignment3.html#instructions",
    "href": "notebooks/assignment3.html#instructions",
    "title": "Assignment 3",
    "section": "",
    "text": "Instructions for all assignments can be found here. Note: this assignment falls under collaboration Mode 2: Individual Assignment – Collaboration Permitted. Please refer to the syllabus for additional information. Please be sure to list the names of any students that you worked with on this assignment. Total points in the assignment add up to 90; an additional 10 points are allocated to professionalism and presentation quality.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 3"
    ]
  },
  {
    "objectID": "notebooks/assignment3.html#learning-objectives",
    "href": "notebooks/assignment3.html#learning-objectives",
    "title": "Assignment 3",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThis assignment will provide structured practice to help enable you to…\n\nUnderstand the primary workflow in machine learning: (1) identifying a hypothesis function set of models, (2) determining a loss/cost/error/objective function to minimize, and (3) minimizing that function through gradient descent\nUnderstand the inner workings of logistic regression and how linear models for classification can be developed.\nGain practice in implementing machine learning algorithms from the most basic building blocks to understand the math and programming behind them to achieve practical proficiency with the techniques\nImplement batch gradient descent and become familiar with how that technique is used and its dependence on the choice of learning rate\nEvaluate supervised learning algorithm performance through ROC curves and using cross validation\nApply regularization to linear models to improve model generalization performance",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 3"
    ]
  },
  {
    "objectID": "notebooks/assignment3.html#exercise-1---classification-using-logistic-regression-build-it-from-the-ground-up",
    "href": "notebooks/assignment3.html#exercise-1---classification-using-logistic-regression-build-it-from-the-ground-up",
    "title": "Assignment 3",
    "section": "Exercise 1 - Classification using logistic regression: build it from the ground up",
    "text": "Exercise 1 - Classification using logistic regression: build it from the ground up\n[60 points]\nThis exercise will walk you through the full life-cycle of a supervised machine learning classification problem. Classification problem consists of two features/predictors (e.g. petal width and petal length) and your goal is to predict one of two possible classes (class 0 or class 1). You will build, train, and evaluate the performance of a logistic regression classifier on the data provided. Before you begin any modeling, you’ll load and explore your data in Part I to familiarize yourself with it - and check for any missing or erroneous data. Then, in Part II, we will review an appropriate hypothesis set of functions to fit to the data: in this case, logistic regression. In Part III, we will derive an appropriate cost function for the data (spoiler alert: it’s cross-entropy) as well as the gradient descent update equation that will allow you to optimize that cost function to identify the parameters that minimize the cost for the training data. In Part IV, all the pieces come together and you will implement your logistic regression model class including methods for fitting the data using gradient descent. Using that model you’ll test it out and plot learning curves to verify the model learns as you train it and to identify and appropriate learning rate hyperparameter. Lastly, in Part V you will apply the model you designed, implemented, and verified to your actual data and evaluate and visualize its generalization performance as compared to a KNN algorithm. When complete, you will have accomplished learning objectives 1-5 above!\n\nA. Load, prepare, and plot your data\n\n\n\n\n\n\nNote\n\n\n\nData for this exercise can be downloaded here\n\n\nYou are given some data for which you are tasked with constructing a classifier. The first step when facing any machine learning project: look at your data!\n1.1 Load the data.\n\nIn the data folder in the same directory of this notebook, you’ll find the data in A3_Q1_data.csv. This file contains the binary class labels, \\(y\\), and the features \\(x_1\\) and \\(x_2\\).\nDivide your data into a training and testing set where the test set accounts for 30 percent of the data and the training set the remaining 70 percent.\n\nPlot the training data by class.\nComment on the data: do the data appear separable? May logistic regression be a good choice for these data? Why or why not?\n\n1.2 Do the data require any preprocessing due to missing values, scale differences (e.g. different ranges of values), etc.? If so, how did you handle these issues?\nNext, we walk through our key steps for model fitting: choose a hypothesis set of models to train (in this case, logistic regression); identify a cost function to measure the model fit to our training data; optimize model parameters to minimize cost (in this case using gradient descent). Once we’ve completed model fitting, we will evaluate the performance of our model and compare performance to another approach (a KNN classifier).\n\n\nB. Stating the hypothesis set of models to evaluate (we’ll use logistic regression)\nGiven that our data consists of two features, our logistic regression problem will be applied to a two-dimensional feature space. Recall that our logistic regression model is:\n\\[f(\\mathbf{x}_i,\\mathbf{w})=\\sigma(\\mathbf{w}^{\\top} \\mathbf{x}_i)\\]\nwhere the sigmoid function is defined as \\(\\sigma(x) = \\dfrac{e^x}{1+e^{x}}= \\dfrac{1}{1+e^{-x}}\\). Also, since this is a two-dimensional problem, we define \\(\\mathbf{w}^{\\top} \\mathbf{x}_i = w_0 x_{i,0} + w_1 x_{i,1} + w_2 x_{i,2}\\) and here, \\(\\mathbf{x}_i=[x_{i,0}, x_{i,1}, x_{i,2}]^{\\top}\\), and \\(x_{i,0} \\triangleq 1\\)\nRemember from class that we interpret our logistic regression classifier output (or confidence score) as the conditional probability that the target variable for a given sample \\(y_i\\) is from class “1”, given the observed features, \\(\\mathbf{x}_i\\). For one sample, \\((y_i, \\mathbf{x}_i)\\), this is given as:\n\\[P(Y=1|X=\\mathbf{x}_i) = f(\\mathbf{x}_i,\\mathbf{w})=\\sigma(\\mathbf{w}^{\\top} \\mathbf{x}_i)\\]\nIn the context of maximizing the likelihood of our parameters given the data, we define this to be the likelihood function \\(L(\\mathbf{w}|y_i,\\mathbf{x}_i)\\), corresponding to one sample observation from the training dataset.\nAside: the careful reader will recognize this expression looks different from when we talk about the likelihood of our data given the true class label, typically expressed as \\(P(x|y)\\), or the posterior probability of a class label given our data, typically expressed as \\(P(y|x)\\). In the context of training a logistic regression model, the likelihood we are interested in is the likelihood function of our logistic regression parameters, \\(\\mathbf{w}\\). It’s our goal to use this to choose the parameters to maximize the likelihood function.\nNo output is required for this section - just read and use this information in the later sections.\n\n\nC. Find the cost function that we can use to choose the model parameters, \\(\\mathbf{w}\\), that best fit the training data.\n1.3 What is the likelihood function that corresponds to all the \\(N\\) samples in our training dataset that we will wish to maximize? Unlike the likelihood function written above which gives the likelihood function for a single training data pair \\((y_i, \\mathbf{x}_i)\\), this question asks for the likelihood function for the entire training dataset \\(\\{(y_1, \\mathbf{x}_1), (y_2, \\mathbf{x}_2), ..., (y_N, \\mathbf{x}_N)\\}\\).\n1.4 Since a logarithm is a monotonic function, maximizing the \\(f(x)\\) is equivalent to maximizing \\(\\ln [f(x)]\\). Express the likelihood from the last question as a cost function of the model parameters, \\(C(\\mathbf{w})\\); that is the negative of the logarithm of the likelihood. Express this cost as an average cost per sample (i.e. divide your final value by \\(N\\)), and use this quantity going forward as the cost function to optimize.\n1.5 Calculate the gradient of the cost function with respect to the model parameters \\(\\nabla_{\\mathbf{w}}C(\\mathbf{w})\\). Express this in terms of the partial derivatives of the cost function with respect to each of the parameters, e.g. \\(\\nabla_{\\mathbf{w}}C(\\mathbf{w}) = \\left[\\dfrac{\\partial C}{\\partial w_0}, \\dfrac{\\partial C}{\\partial w_1}, \\dfrac{\\partial C}{\\partial w_2}\\right]\\).\nTo simplify notation, please use \\(\\mathbf{w}^{\\top}\\mathbf{x}\\) instead of writing out \\(w_0 x_{i,0} + w_1 x_{i,1} + w_2 x_{i,2}\\) when it appears each time (where \\(x_{i,0} = 1\\) for all \\(i\\)). You are also welcome to use \\(\\sigma()\\) to represent the sigmoid function. Lastly, this will be a function the features, \\(x_{i,j}\\) (with the first index in the subscript representing the observation and the second the feature; targets, \\(y_i\\); and the logistic regression model parameters, \\(w_j\\).\n1.6 Write out the gradient descent update equation. This should clearly express how to update each weight from one step in gradient descent \\(w_j^{(k)}\\) to the next \\(w_j^{(k+1)}\\). There should be one equation for each model logistic regression model parameter (or you can represent it in vectorized form). Assume that \\(\\eta\\) represents the learning rate.\n\n\nD. Implement gradient descent and your logistic regression algorithm\n1.7 Implement your logistic regression model.\n\nYou are provided with a template, below, for a class with key methods to help with your model development. It is modeled on the Scikit-Learn convention. For this, you only need to create a version of logistic regression for the case of two feature variables (i.e. two predictors).\nCreate a method called sigmoid that calculates the sigmoid function\nCreate a method called cost that computes the cost function \\(C(\\mathbf{w})\\) for a given dataset and corresponding class labels. This should be the average cost (make sure your total cost is divided by your number of samples in the dataset).\nCreate a method called gradient_descent to run one step of gradient descent on your training data. We’ll refer to this as “batch” gradient descent since it takes into account the gradient based on all our data at each iteration of the algorithm.\nCreate a method called fit that fits the model to the data (i.e. sets the model parameters to minimize cost) using your gradient_descent method. In doing this we’ll need to make some assumptions about the following:\n\nWeight initialization. What should you initialize the model parameters to? For this, randomly initialize the weights to a different values between 0 and 1.\nLearning rate. How slow/fast should the algorithm step towards the minimum? This you will vary in a later part of this problem.\nStopping criteria. When should the algorithm be finished searching for the optimum? There are two stopping criteria: small changes in the gradient descent step size and a maximum number of iterations. The first is whether there was a sufficiently small change in the gradient; this is evaluated as whether the magnitude of the step that the gradient descent algorithm takes changes by less than \\(10^{-6}\\) between iterations. Since we have a weight vector, we can compute the change in the weight by evaluating the \\(L_2\\) norm (Euclidean norm) of the change in the vector between iterations. From our gradient descent update equation we know that mathematically this is \\(||-\\eta\\nabla_{\\mathbf{w}}C(\\mathbf{w})||\\). The second criterion is met if a maximum number of iterations has been reach (5,000 in this case, to prevent infinite loops from poor choices of learning rates).\nDesign your approach so that at each step in the gradient descent algorithm you evaluate the cost function for both the training and the test data for each new value for the model weights. You should be able to plot cost vs gradient descent iteration for both the training and the test data. This will allow you to plot “learning curves” that can be informative for how the model training process is proceeding.\n\nCreate a method called predict_proba that predicts confidence scores (that can be thresholded into the predictions of the predict method.\nCreate a method called predict that makes predictions based on the trained model, selecting the most probable class, given the data, as the prediction, that is class that yields the larger \\(P(y|\\mathbf{x})\\).\n(Optional, but recommended) Create a method called learning_curve that produces the cost function values that correspond to each step from a previously run gradient descent operation.\n(Optional, but recommended) Create a method called prepare_x which appends a column of ones as the first feature of the dataset \\(\\mathbf{X}\\) to account for the bias term (\\(x_{i,1}=1\\)).\n\nThis structure is strongly encouraged; however, you’re welcome to adjust this to your needs (adding helper methods, modifying parameters, etc.).\n\n# Logistic regression class\nclass Logistic_regression:\n    # Class constructor\n    def __init__(self):\n        self.w = None     # logistic regression weights\n        self.saved_w = [] # Since this is a small problem, we can save the weights\n                          #  at each iteration of gradient descent to build our \n                          #  learning curves\n        # returns nothing\n        pass\n    \n    # Method for calculating the sigmoid function of w^T X for an input set of weights\n    def sigmoid(self, X, w):\n        # returns the value of the sigmoid\n        pass\n    \n    # Cost function for an input set of weights\n    def cost(self, X, y, w):\n        # returns the average cross entropy cost\n        pass\n    \n    # Update the weights in an iteration of gradient descent\n    def gradient_descent(self, X, y, lr):\n        # returns a scalar of the magnitude of the Euclidean norm \n        #  of the change in the weights during one gradient descent step\n        pass\n    \n    # Fit the logistic regression model to the data through gradient descent\n    def fit(self, X, y, w_init, lr, delta_thresh=1e-6, max_iter=5000, verbose=False):\n        # Note the verbose flag enables you to print out the weights at each iteration \n        #  (optional - but may help with one of the questions)\n        \n        # returns nothing\n        pass\n    \n    # Use the trained model to predict the confidence scores (prob of positive class in this case)\n    def predict_proba(self, X):\n        # returns the confidence score for the each sample\n        pass\n    \n    # Use the trained model to make binary predictions\n    def predict(self, X, thresh=0.5):\n        # returns a binary prediction for each sample\n        pass\n    \n    # Stores the learning curves from saved weights from gradient descent\n    def learning_curve(self, X, y):\n        # returns the value of the cost function from each step in gradient descent\n        #  from the last model fitting process\n        pass\n    \n    # Appends a column of ones as the first feature to account for the bias term\n    def prepare_x(self, X):\n        # returns the X with a new feature of all ones (a column that is the new column 0)\n        pass\n\n1.8 Choose a learning rate and fit your model. Learning curves are a plot of metrics of model performance evaluated through the process of model training to provide insights about how model training is proceeding. Show the learning curves for the gradient descent process for learning rates of \\(\\{10^{-0}, 10^{-2}, 10^{-4}\\}\\). For each learning rate plot the learning curves by plotting both the training and test data average cost as a function of each iteration of gradient descent. You should run the model fitting process until it completes (up to 5,000 iterations of gradient descent). All of the 6 resulting curves (train and test average cost for each learning rate) should be plotted on the same set of axes to enable direct comparison. Note: make sure you’re using average cost per sample, not the total cost.\n\nTry running this process for a really big learning rate for this problem: \\(10^2\\). Look at the weights that the fitting process generates over the first 50 iterations and how they change. Either print these first 50 iterations as console output or plot them. What happens? How does the output compare to that corresponding to a learning rate of \\(10^0\\) and why?\nWhat is the impact that the different values of learning have on the speed of the process and the results?\nOf the options explored, what learning rate do you prefer and why?\nUse your chosen learning rate for the remainder of this problem.\n\n\n\nE. Evaluate your model performance through cross validation\n1.9 Test the performance of your trained classifier using K-folds cross validation resampling technique. The scikit-learn package StratifiedKFolds may be helpful.\n\nTrain your logistic regression model and a K-Nearest Neighbor classification model with \\(k=7\\) nearest neighbors.\nUsing the trained models, make four plots: two for logistic regression and two for KNN. For each model have one plot showing the training data used for fitting the model, and the other showing the test data. On each plot, include the decision boundary resulting from your trained classifier.\nProduce a Receiver Operating Characteristic curve (ROC curve) that represents the performance from cross validated performance evaluation for each classifier (your logistic regression model and the KNN model, with \\(k=7\\) nearest neighbors). For the cross validation, use \\(k=10\\) folds.\n\nPlot these curves on the same set of axes to compare them. You should not plot one curve for each fold of k-folds; instead, you should plot one ROC curve for Logistic Regression and one for KNN (each should incorporate all 10 folds of validation). Also, don’t forget to plot the “chance” line.\nOn the ROC curve plot, also include the chance diagonal for reference (this represents the performance of the worst possible classifier). This is represented as a line from \\((0,0)\\) to \\((1,1)\\).\nCalculate the Area Under the Curve for each model and include this measure in the legend of the ROC plot.\n\nComment on the following:\n\nWhat is the purpose of using cross validation for this problem?\nHow do the models compare in terms of performance (both ROC curves and decision boundaries) and which model (logistic regression or KNN) would you select to use on previously unseen data for this problem and why?",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 3"
    ]
  },
  {
    "objectID": "notebooks/assignment3.html#exercise-2---digits-classification",
    "href": "notebooks/assignment3.html#exercise-2---digits-classification",
    "title": "Assignment 3",
    "section": "Exercise 2 - Digits classification",
    "text": "Exercise 2 - Digits classification\nAn exploration of regularization, imbalanced classes, ROC and PR curves\n[30 points]\nThe goal of this exercise is to apply your supervised learning skills on a very different dataset: in this case, image data; MNIST: a collection of images of handwritten digits. Your goal is to train a classifier that is able to distinguish the number “3” from all possible numbers and to do so as accurately as possible. You will first explore your data (this should always be your starting point to gain domain knowledge about the problem.). Since the feature space in this problem is 784-dimensional, overfitting is possible. To avoid overfitting you will investigate the impact of regularization on generalization performance (test accuracy) and compare regularized and unregularized logistic regression model test error against other classification techniques such as naive Bayes and random forests and draw conclusions about the best-performing model.\nStart by loading your dataset from the MNIST dataset of handwritten digits, using the code provided below. MNIST has a training set of 60,000 examples, and a test set of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image.\nYour goal is to classify whether or not an example digit is a 3. Your binary classifier should predict \\(y=1\\) if the digit is a 3, and \\(y=0\\) otherwise. Create your dataset by transforming your labels into a binary format (3’s are class 1, and all other digits are class 0).\n2.1 Plot 10 examples of each class (i.e. class \\(y=0\\), which are not 3’s and class \\(y=1\\) which are 3’s), from the training dataset.\nNote that the data are composed of samples of length 784. These represent 28 x 28 images, but have been reshaped for storage convenience. To plot digit examples, you’ll need to reshape the data to be 28 x 28 (which can be done with numpy reshape).\n2.2 How many examples are present in each class? Show a plot of samples by class (bar plot). What fraction of samples are positive? What issues might this cause?\n2.3 Identify the value of the regularization parameter that optimizes model performance on out-of-sample data. Using a logistic regression classifier, apply lasso regularization and retrain the model and evaluate its performance on the test set over a range of values on the regularization coefficient. You can implement this using the LogisticRegression module and activating the ‘l1’ penalty; the parameter \\(C\\) is the inverse of the regularization strength. Vary the value of C logarithmically from \\(10^{-4}\\) to \\(10^4\\) (and make your x-axes logarithmic in scale) and evaluate it at least 20 different values of C. As you vary the regularization coefficient, Plot the following four quantities (this should result in 4 separate plots)…\n\nThe number of model parameters that are estimated to be nonzero (in the logistic regression model, one attribute is coef_, which gives you access to the model parameters for a trained model)\nThe cross entropy loss (which can be evaluated with the Scikit Learn log_loss function)\nArea under the ROC curve (AUC)\nThe \\(F_1\\)-score (assuming a threshold of 0.5 on the predicted confidence scores, that is, scores above 0.5 are predicted as Class 1, otherwise Class 0). Scikit Learn also has a f1_score function which may be useful. -Which value of C seems best for this problem? Please select the closest power of 10. You will use this in the next part of this exercise.\n\n2.4 Train and test a (1) logistic regression classifier with minimal regularization (using the Scikit Learn package, set penalty=‘l1’, C=1e100 to approximate this), (2) a logistic regression classifier with the best value of the regularization parameter from the last section, (3) a Gradient Boosting classifier, and (4) a Random Forest (RF) classifier (using default parameters for the RF classifier).\n\nCompare your classifiers’ performance using ROC and Precision Recall (PR) curves. For the ROC curves, all your curves should be plotted on the same set of axes so that you can directly compare them. Please do the same wih the PR curves.\nPlot the line that represents randomly guessing the class (50% of the time a “3”, 50% not a “3”). You SHOULD NOT actually create random guesses. Instead, you should think through the theory behind how ROC and PR curves work and plot the appropriate lines. It’s a good practice to include these in ROC and PR curve plots as a reference point.\nFor PR curves, an excellent resource on how to correctly plot them can be found here (ignore the section on “non-linear interpolation between two points”). This describes how a random classifier is represented in PR curves and demonstrates that it should provide a lower bound on performance.\nWhen training your logistic regression model, it’s recommended that you use solver=“liblinear”; otherwise, your results may not converge.\nDescribe the performance of the classifiers you compared. Did the regularization of the logistic regression model make much difference here? Which classifier you would select for application to unseen data.\n\n\n# Load the MNIST Data\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\n\n# Set this to True to download the data for the first time and False after the first time \n#   so that you just load the data locally instead\ndownload_data = True\n\nif download_data:\n    # Load data from https://www.openml.org/d/554\n    X, y = fetch_openml('mnist_784', return_X_y=True, as_frame=False)\n    \n    # Adjust the labels to be '1' if y==3, and '0' otherwise\n    y[y!='3'] = 0\n    y[y=='3'] = 1\n    y = y.astype('int')\n    \n    # Divide the data into a training and test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/7, random_state=88)\n    \n    file = open('tmpdata', 'wb')\n    pickle.dump((X_train, X_test, y_train, y_test), file)\n    file.close()\nelse:\n    file = open('tmpdata', 'rb')\n    X_train, X_test, y_train, y_test = pickle.load(file)\n    file.close()",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 3"
    ]
  },
  {
    "objectID": "notebooks/assignment5.html",
    "href": "notebooks/assignment5.html",
    "title": "Assignment 5",
    "section": "",
    "text": "Instructions for all assignments can be found here. Note: this assignment falls under collaboration Mode 2: Individual Assignment – Collaboration Permitted. Please refer to the syllabus for additional information. Please be sure to list the names of any students that you worked with on this assignment. Total points in the assignment add up to 90; an additional 10 points are allocated to professionalism and presentation quality.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 5"
    ]
  },
  {
    "objectID": "notebooks/assignment5.html#instructions",
    "href": "notebooks/assignment5.html#instructions",
    "title": "Assignment 5",
    "section": "",
    "text": "Instructions for all assignments can be found here. Note: this assignment falls under collaboration Mode 2: Individual Assignment – Collaboration Permitted. Please refer to the syllabus for additional information. Please be sure to list the names of any students that you worked with on this assignment. Total points in the assignment add up to 90; an additional 10 points are allocated to professionalism and presentation quality.",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 5"
    ]
  },
  {
    "objectID": "notebooks/assignment5.html#learning-objectives",
    "href": "notebooks/assignment5.html#learning-objectives",
    "title": "Assignment 5",
    "section": "Learning objectives",
    "text": "Learning objectives\nThrough completing this assignment you will be able to…\n\nApply the full supervised machine learning pipeline of preprocessing, model selection, model performance evaluation and comparison, and model application to a real-world scale dataset\nApply clustering techniques to a variety of datasets with diverse distributional properties, gaining an understanding of their strengths and weaknesses and how to tune model parameters\nApply PCA and t-SNE for performing dimensionality reduction and data visualization",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 5"
    ]
  },
  {
    "objectID": "notebooks/assignment5.html#exercise-1---kaggle-classification-competition",
    "href": "notebooks/assignment5.html#exercise-1---kaggle-classification-competition",
    "title": "Assignment 5",
    "section": "Exercise 1 - Kaggle Classification Competition",
    "text": "Exercise 1 - Kaggle Classification Competition\n[40 points]\nYou’ve learned a great deal about supervised learning and now it’s time to bring together all that you’ve learned. You will be competing in a Kaggle Competition along with the rest of the class! Your goal is to predict hotel reservation cancellations based on a number of potentially related factors such as lead time on the booking, time of year, type of room, special requests made, number of children, etc. While you will be asked to take certain steps along the way to your submission, you’re encouraged to try creative solutions to this problem and your choices are wide open for you to make your decisions on how to best make the predictions.\n\n\n\n\n\n\nImportant note\n\n\n\nFollow the link posted on Ed to register for the competition. You can view the public leaderboard anytime at the Kaggle website (see the Ed post).\n\n\nThe Data. The dataset is provided as a5_q1.pkl which is a pickle file format, which allows you to load the data directly using the code below; the data can be downloaded from the Kaggle competition website (see Ed Discussions for the link). A data dictionary for the project can be found here and the original paper that describes the dataset can be found here. When you load the data, 5 matrices are provided X_train_original, y_train, and X_test_original, which are the original, unprocessed features and labels for the training set and the test features (the test labels are not provided - that’s what you’re predicting). Additionally, X_train_ohe and X_test_ohe are provided which are one-hot-encoded (OHE) versions of the data. The OHE versions OHE processed every categorical variable. This is provided for convenience if you find it helpful, but you’re welcome to reprocess the original data other ways if your prefer.\nScoring. You will need to achieve a minimum acceptable level of performance to demonstrate proficiency with using these supervised learning techniques. Beyond that, it’s an open competition and scoring in the top three places of the private leaderboard will result in 3, 2, and 1 bonus points in this assignment, respectively (and the pride of the class!). Note: the Kaggle leaderboard has a public and private component. The public component is viewable throughout the competition, but the private leaderboard is revealed at the end. When you make a submission, you immediately see your submission on the public leaderboard, but that only represents scoring on a fraction of the total collection of test data, the rest remains hidden until the end of the competition to prevent overfitting to the test data through repeated submissions. You will be be allowed to hand-select two eligible submissions for private score, or by default your best two public scoring submissions will be selected for private scoring.\n\nRequirements:\n1.1. Explore your data. Review and understand your data. Look at it; read up on what the features represent; think through the application domain; visualize statistics from the paper data to understand any key relationships. There is no output required for this question, but you are encouraged to explore the data personally before going further.\n1.2. Preprocess your data. Preprocess your data so it’s ready for use for classification and describe what you did and why you did it. Preprocessing may include: normalizing data, handling missing or erroneous values, separating out a validation dataset, preparing categorical variables through one-hot-encoding, etc. To make one step in this process easier, you’re provided with a one-hot-encoded version of the data already.\n\nComment on each type of preprocessing that you apply and both how and why you apply it.\n\n1.3. Select, train, and compare models. Fit at least 5 models to the data. Some of these can be experiments with different hyperparameter-tuned versions of the same model, although all 5 should not be the same type of model. There are no constraints on the types of models, but you’re encouraged to explore examples we’ve discussed in class including:\n\nLogistic regression\nK-nearest neighbors\nRandom Forests\nNeural networks\nSupport Vector Machines\nEnsembles of models (e.g. model bagging, boosting, or stacking). Scikit-learn offers a number of tools for assisting with this including those for bagging, boosting, and stacking. You’re also welcome to explore options beyond the sklean universe; for example, some of you may have heard of XGBoost which is a very fast implementation of gradient boosted decision trees that also allows for parallelization.\n\nWhen selecting models, be aware that some models may take far longer than others to train. Monitor your output and plan your time accordingly.\nAssess the classification performance AND computational efficiency of the models you selected:\n\nPlot the ROC curves and PR curves for your models in two plots: one of ROC curves and one of PR curves. For each of these two plots, compare the performance of the models you selected above and trained on the training data, evaluating them on the validation data. Be sure to plot the line representing random guessing on each plot. You should plot all of the model’s ROC curves on a single plot and the PR curves on a single plot. One of the models should also be your BEST performing submission on the Kaggle public leaderboard (see below). In the legends of each, include the area under the curve for each model (limit to 3 significant figures). For the ROC curve, this is the AUC; for the PR curve, this is the average precision (AP).\nAs you train and validate each model time how long it takes to train and validate in each case and create a plot that shows both the training and prediction time for each model included in the ROC and PR curves.\nDescribe:\n\nYour process of model selection and hyperparameter tuning\nWhich model performed best and your process for identifying/selecting it\n\n\n1.4. Apply your model “in practice”. Make at least 5 submissions of different model results to the competition (more submissions are encouraged and you can submit up to 5 per day!). These do not need to be the same that you report on above, but you should select your most competitive models.\n\nProduce submissions by applying your model on the test data.\nBe sure to RETRAIN YOUR MODEL ON ALL LABELED TRAINING AND VALIDATION DATA before making your predictions on the test data for submission. This will help to maximize your performance on the test data.\nIn order to get full credit on this problem you must achieve an AUC on the Kaggle public leaderboard above the “Benchmark” score on the public leaderboard.\n\n\n\nGuidance\n\nPreprocessing. You may need to preprocess the data for some of these models to perform well (scaling inputs or reducing dimensionality). Some of this preprocessing may differ from model to model to achieve the best performance. A helpful tool for creating such preprocessing and model fitting pipelines is the sklearn pipeline module which lets you group a series of processing steps together.\nHyperparameters. Hyperparameters may need to be tuned for some of the model you use. You may want to perform hyperparameter tuning for some of the models. If you experiment with different hyperparameters that include many model runs, you may want to apply them to a small subsample of your overall data before running it on the larger training set to be time efficient (if you do, just make sure to ensure your selected subset is representative of the rest of your data).\nValidation data. You’re encouraged to create your own validation dataset for comparing model performance; without this, there’s a significant likelihood of overfitting to the data. A common choice of the split is 80% training, 20% validation. Before you make your final predictions on the test data, be sure to retrain your model on the entire dataset.\nTraining time. This is a larger dataset than you’ve worked with previously in this class, so training times may be higher that what you’ve experienced in the past. Plan ahead and get your model pipeline working early so you can experiment with the models you use for this problem and have time to let them run.\n\n\n\nStarter code\nBelow is some code for (1) loading the data and (2) once you have predictions in the form of confidence scores for those classifiers, to produce submission files for Kaggle.\n\nimport pandas as pd\nimport numpy as np\nimport pickle\n\n################################\n# Load the data\n################################\ndata = pd.read_pickle(\"./data/a5_q1.pkl\")\n\ny_train = data['y_train']\nX_train_original = data['X_train'] # Original dataset\nX_train_ohe = data['X_train_ohe']  # One-hot-encoded dataset\n\nX_test_original = data['X_test']\nX_test_ohe = data['X_test_ohe']\n\n################################\n# Produce submission\n################################\n\ndef create_submission(confidence_scores, save_path):\n    '''Creates an output file of submissions for Kaggle\n    \n    Parameters\n    ----------\n    confidence_scores : list or numpy array\n        Confidence scores (from predict_proba methods from classifiers) or\n        binary predictions (only recommended in cases when predict_proba is \n        not available)\n    save_path : string\n        File path for where to save the submission file.\n    \n    Example:\n    create_submission(my_confidence_scores, './data/submission.csv')\n\n    '''\n    import pandas as pd\n\n    submission = pd.DataFrame({\"score\":confidence_scores})\n    submission.to_csv(save_path, index_label=\"id\")",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 5"
    ]
  },
  {
    "objectID": "notebooks/assignment5.html#exercise-2---clustering",
    "href": "notebooks/assignment5.html#exercise-2---clustering",
    "title": "Assignment 5",
    "section": "Exercise 2 - Clustering",
    "text": "Exercise 2 - Clustering\n[25 points]\nClustering can be used to reveal structure between samples of data and assign group membership to similar groups of samples. This exercise will provide you with experience applying clustering algorithms and comparing these techniques on various datasets to experience the pros and cons of these approaches when the structure of the data being clustered varies. For this exercise, we’ll explore clustering in two dimensions to make the results more tangible, but in practice these approaches can be applied to any number of dimensions.\nNote: For each set of plots across the five datasets, please create subplots within a single figure (for example, when applying DBSCAN - please show the clusters resulting from DBSCAN as a single figure with one subplot for each dataset). This will make comparison easier.\n2.1. Run K-means and choose the number of clusters. Five datasets are provided for you below and the code to load them below.\n\nScatterplot each dataset\nFor each dataset run the k-means algorithm for values of \\(k\\) ranging from 1 to 10 and for each plot the “elbow curve” where you plot dissimilarity in each case. Here, you can measure dissimilarity using the within-cluster sum-of-squares, which in sklean is known as “inertia” and can be accessed through the inertia_ attribute of a fit KMeans class instance.\nFor each dataset, where is the elbow in the curve of within-cluster sum-of-squares and why? Is the elbow always clearly visible? When it’s not clear, you will have to use your judgment in terms of selecting a reasonable number of clusters for the data. There are also other metrics you can use to explore to measure the quality of cluster fit (but do not have to for this assignment) including the silhouette score, the Calinski-Harabasz index, and the Davies-Bouldin, to name a few within sklearn alone. However, assessing the quality of fit without “preferred” cluster assignments to compare against (that is, in a truly unsupervised manner) is challenging because measuring cluster fit quality is typically poorly-defined and doesn’t generalize across all types of inter- and intra-cluster variation.\nPlot your clustered data (different color for each cluster assignment) for your best \\(k\\)-means fit determined from both the elbow curve and your judgment for each dataset and your inspection of the dataset.\n\n2.2. Apply DBSCAN. Vary the eps and min_samples parameters to get as close as you can to having the same number of clusters as your choices with K-means. The same code plots as gray/black any points that were not assigned to clusters.\n2.3. Apply Spectral Clustering. Select the same number of clusters as selected by k-means.\n2.4. Comment on the strengths and weaknesses of each approach. In particular, mention:\n\nWhich technique worked “best” and “worst” (as defined by matching how human intuition would cluster the data) on each dataset?\nHow much effort was required to get good clustering for each method (how much parameter tuning needed to be done)?\n\n\n\n\n\n\n\nImportant note\n\n\n\nFor these clustering plots in this question, do NOT include legends indicating cluster assignment; instead, just make sure the cluster assignments are clear from the plot (e.g. different colors for each cluster)\n\n\nCode is provided below for loading the datasets and for making plots with the clusters as distinct colors\n\n################################\n# Load the data\n################################\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs, make_moons\n\n# Create / load the datasets:\nn_samples = 1500\nX0, _ = make_blobs(n_samples=n_samples, centers=2, n_features=2, random_state=0)\nX1, _ = make_blobs(n_samples=n_samples, centers=5, n_features=2, random_state=0)\n\nrandom_state = 170\nX, y = make_blobs(n_samples=n_samples, random_state=random_state, cluster_std=1.3)\ntransformation = [[0.6, -0.6], [-0.2, 0.8]]\nX2 = np.dot(X, transformation)\nX3, _ = make_blobs(n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state)\nX4, _ = make_moons(n_samples=n_samples, noise=.12)\n\nX = [X0, X1, X2, X3, X4]\n# The datasets are X[i], where i ranges from 0 to 4\n\n\n################################\n# Code to plot clusters\n################################\ndef plot_cluster(ax, data, cluster_assignments):\n    '''Plot two-dimensional data clusters\n    \n    Parameters\n    ----------\n    ax : matplotlib axis\n        Axis to plot on\n    data : list or numpy array of size [N x 2] \n        Clustered data\n    cluster_assignments : list or numpy array [N]\n        Cluster assignments for each point in data\n\n    '''\n    clusters = np.unique(cluster_assignments)\n    n_clusters = len(clusters)\n    for ca in clusters:\n        kwargs = {}\n        if ca == -1:\n            # if samples are not assigned to a cluster (have a cluster assignment of -1, color them gray)\n            kwargs = {'color':'gray'}\n            n_clusters = n_clusters - 1\n        ax.scatter(data[cluster_assignments==ca, 0], data[cluster_assignments==ca, 1],s=5,alpha=0.5, **kwargs)\n        ax.set_xlabel('feature 1')\n        ax.set_ylabel('feature 2')\n        ax.set_title(f'No. Clusters = {n_clusters}')\n        ax.axis('equal')",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 5"
    ]
  },
  {
    "objectID": "notebooks/assignment5.html#exercise-3---dimensionality-reduction-and-visualization-of-digits-with-pca-and-t-sne",
    "href": "notebooks/assignment5.html#exercise-3---dimensionality-reduction-and-visualization-of-digits-with-pca-and-t-sne",
    "title": "Assignment 5",
    "section": "Exercise 3 - Dimensionality reduction and visualization of digits with PCA and t-SNE",
    "text": "Exercise 3 - Dimensionality reduction and visualization of digits with PCA and t-SNE\n[25 points]\n3.1. Reduce the dimensionality of the data with PCA for data visualization. Load the scikit-learn digits dataset (code provided to do this below). Consider whether any preprocessing may need to be applied (do the data need to be normalized?). Apply PCA and reduce the data (with the associated cluster labels 0-9) into a 2-dimensional space. Plot the data with labels in this two dimensional space (labels can be colors, shapes, or using the actual numbers to represent the data - definitely include a legend in your plot).\n3.2. Create a plot showing the cumulative fraction of variance explained as you incorporate from \\(1\\) through all \\(D\\) principal components of the data (where \\(D\\) is the dimensionality of the data).\n\nWhat fraction of variance in the data is UNEXPLAINED by the first two principal components of the data?\nBriefly comment on how this may impact how well-clustered the data are. You can use the explained_variance_ attribute of the PCA module in scikit-learn to assist with this question\n\n3.3. Reduce the dimensionality of the data with t-SNE for data visualization. T-distributed stochastic neighborhood embedding (t-SNE) is a nonlinear dimensionality reduction technique that is particularly adept at embedding the data into lower 2 or 3 dimensional spaces. Apply t-SNE using the scikit-learn implementation to the digits dataset and plot it in 2-dimensions (with associated cluster labels 0-9). You may need to adjust the parameters to get acceptable performance. You can read more about how to use t-SNE effectively here.\n3.4. Briefy compare/contrast the performance of these two techniques.\n\nWhich seemed to cluster the data best and why?\nNotice that while t-SNE has a fit method and a fit_transform method, these methods are actually identical, and there is no transform method. Why is this? What implications does this imply for using this method?\n\nNote: Remember that you typically will not have labels available in most problems.\nCode is provided for loading the data below.\n\n################################\n# Load the data\n################################\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n# load dataset\ndigits = datasets.load_digits()\nn_sample = digits.target.shape[0]\nn_feature = digits.images.shape[1] * digits.images.shape[2]\nX_digits = np.zeros((n_sample, n_feature))\nfor i in range(n_sample):\n    X_digits[i, :] = digits.images[i, :, :].flatten()\ny_digits = digits.target",
    "crumbs": [
      "Schedule",
      "Assignments",
      "Assignment 5"
    ]
  },
  {
    "objectID": "shared/assignment0.html",
    "href": "shared/assignment0.html",
    "title": "Assignment 0 - Sample Assignment",
    "section": "",
    "text": "Calculate the following:\n1.1. The first derivative of \\(f(t) = t^3\\).\n1.2. The second derivative of \\(f(t) = t^3\\).\n1.3. Numerically evaluate the second derivative of \\(f(t) = t^3\\) for \\(t=3\\).\n1.4. If \\(t\\) represents time (seconds) and \\(f\\) represents distance of a ball over time (in meters), then what does the second derivative at \\(t=3\\) mean?"
  },
  {
    "objectID": "shared/assignment0.html#exercise-1---derivatives",
    "href": "shared/assignment0.html#exercise-1---derivatives",
    "title": "Assignment 0 - Sample Assignment",
    "section": "",
    "text": "Calculate the following:\n1.1. The first derivative of \\(f(t) = t^3\\).\n1.2. The second derivative of \\(f(t) = t^3\\).\n1.3. Numerically evaluate the second derivative of \\(f(t) = t^3\\) for \\(t=3\\).\n1.4. If \\(t\\) represents time (seconds) and \\(f\\) represents distance of a ball over time (in meters), then what does the second derivative at \\(t=3\\) mean?"
  },
  {
    "objectID": "shared/assignment0.html#exercise-2---plotting",
    "href": "shared/assignment0.html#exercise-2---plotting",
    "title": "Assignment 0 - Sample Assignment",
    "section": "Exercise 2 - Plotting",
    "text": "Exercise 2 - Plotting\n2.1. Create a plot of the first derivative of \\(f(t)\\) for the values \\(-2 \\leq t \\leq 1\\).\n2.2. What does the first derivative of \\(f(t)\\) for \\(t=2\\) mean in this case?"
  }
]